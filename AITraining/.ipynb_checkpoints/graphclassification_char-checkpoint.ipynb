{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa09a59e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T07:38:20.515169Z",
     "start_time": "2021-09-07T07:38:20.498592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(6, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.utils as ut\n",
    "np.random.seed(42)\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features,hidden_channels,num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(num_node_features=6,hidden_channels=64,num_classes=10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb13a1ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T07:38:31.883931Z",
     "start_time": "2021-09-07T07:38:31.859524Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lableWord={0:'A',1:'B',2:'C',3:'D',4:'E',5:'F',6:'G',7:'H',8:'I',9:'J',10:'K',11:'L',12:'M',13:'N',14:'O',15:'P',16:'Q',17:'R',18:'S',19:'T',20:'U',21:'V',22:'W',23:'X',24:'Y',25:'Z'}\n",
    "\n",
    "num_edge_features=1\n",
    "label_dic=dict(zip(lableWord.values(), lableWord.keys()))\n",
    "def read_flex_data(raw_dir):\n",
    "    data=[]\n",
    "    for tempfile in os.listdir(raw_dir):\n",
    "        filename=os.path.join(raw_dir,tempfile)\n",
    "        tempdata=np.loadtxt(filename,delimiter=',')\n",
    "        label=label_dic[tempfile.split(\".\")[0]]\n",
    "        print(\"filename: {} label {} \".format(tempfile,label))\n",
    "        label=np.full((len(tempdata),1),label,dtype=np.int)\n",
    "        tempdata=np.column_stack((tempdata,label))\n",
    "        if len(data)==0:\n",
    "            data=tempdata\n",
    "        else:\n",
    "            data=np.row_stack((data,tempdata))\n",
    "    return data\n",
    "\n",
    "def standardization(data):\n",
    "    mu = np.mean(data, axis=0)\n",
    "    sigma = np.std(data, axis=0)\n",
    "    return (data - mu) / sigma\n",
    "def graphfeatureHandle(record):\n",
    "    #todo\n",
    "    meanvalue=np.mean(record,axis=0)\n",
    "    data=np.append(record,meanvalue)\n",
    "    data=standardization(data)\n",
    "    adj=[]\n",
    "    temp=[]\n",
    "    for i in range(0,len(data)):\n",
    "        for j in range(0,len(data)):\n",
    "            if data[i]-data[j]>0:\n",
    "                    temp.append(data[i]-data[j])   #只有正直，负值用0表示\n",
    "            else:\n",
    "                temp.append(0)\n",
    "                \n",
    "    data=np.array(temp)\n",
    "    data=np.reshape(data,(6,6))    #直接data.reshape() 不起作用\n",
    "    return data\n",
    "def handleToData(data_list):\n",
    "    listData=[]\n",
    "    for i in range(len(data_list)):\n",
    "        adj=graphfeatureHandle(data_list[i][:-1])\n",
    "        #print(\"adj\",adj,\"\\n np.nonzero(adj)\",np.nonzero(adj))\n",
    "        source_nodes, target_nodes = np.nonzero(adj)\n",
    "        source_nodes = source_nodes.reshape((1, -1))\n",
    "        target_nodes = target_nodes.reshape((1, -1))\n",
    "        #print(target_nodes.shape,type(target_nodes),\"value:\",target_nodes,\"shape:\",target_nodes.shape)\n",
    "        #print(source_nodes.shape,type(source_nodes),\"value:\",source_nodes)\n",
    "        edge_index = torch.tensor(np.concatenate((source_nodes, target_nodes), axis=0), dtype=torch.long) # edge_index should be long type\n",
    "        #print(\"edge_index\",edge_index,\"shape:\",edge_index.shape)  #torch.Size([2, 14])\n",
    "\n",
    "\n",
    "        #edge_weight = adj\n",
    "        edge_weight=[]\n",
    "        for i in range(len(source_nodes[0])):\n",
    "            edge_weight.append(adj[source_nodes[0][i]][target_nodes[0][i]])\n",
    "        #print(\"edge_weight:\",edge_weight)\n",
    "        edge_weight=np.asarray(edge_weight)\n",
    "        edge_weight = torch.tensor(edge_weight.reshape((-1, num_edge_features)), dtype=torch.float) # edge_index should be float\n",
    "        type\n",
    "\n",
    "        x = torch.tensor([[i] for i in range(26)], dtype=torch.int) \n",
    "\n",
    "        # y should be long type, graph label should not be a 0-dimesion tensor\n",
    "        # use [graph_label[i]] ranther than graph_label[i]\n",
    "        y = torch.tensor([data_list[i][-1]], dtype=torch.int) \n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=y, edge_attr=edge_weight)\n",
    "        #print(data)\n",
    "        listData.append(data)\n",
    "        #break\n",
    "    return listData\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22af0a4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T07:40:15.793294Z",
     "start_time": "2021-09-07T07:40:13.563900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "filename: T.txt label 19 \n",
      "filename: U.txt label 20 \n",
      "filename: Q.txt label 16 \n",
      "filename: R.txt label 17 \n",
      "filename: L.txt label 11 \n",
      "filename: D.txt label 3 \n",
      "filename: E.txt label 4 \n",
      "filename: J.txt label 9 \n",
      "filename: V.txt label 21 \n",
      "filename: X.txt label 23 \n",
      "filename: Y.txt label 24 \n",
      "filename: W.txt label 22 \n",
      "filename: I.txt label 8 \n",
      "filename: Z.txt label 25 \n",
      "filename: F.txt label 5 \n",
      "filename: P.txt label 15 \n",
      "filename: C.txt label 2 \n",
      "filename: N.txt label 13 \n",
      "filename: S.txt label 18 \n",
      "filename: A.txt label 0 \n",
      "filename: O.txt label 14 \n",
      "filename: M.txt label 12 \n",
      "filename: H.txt label 7 \n",
      "filename: B.txt label 1 \n",
      "filename: G.txt label 6 \n",
      "filename: K.txt label 10 \n",
      "save file\n",
      "Done!\n",
      "Data(edge_attr=[15, 1], edge_index=[2, 15], x=[26, 1], y=[1])\n"
     ]
    }
   ],
   "source": [
    "class GraphFlexSensorInMem(InMemoryDataset):\n",
    "    \"\"\"\n",
    "    Graph classification \n",
    "    \"\"\"\n",
    "    def __init__(self, root,transform=None, pre_transform=None):\n",
    "        super(GraphFlexSensorInMem, self).__init__(root,transform, pre_transform)\n",
    "#         classtype='d'\n",
    "#         if classtype[0]=='d':\n",
    "#             self.label_dic=dict(zip(lableDigit.values(), lableDigit.keys()))\n",
    "#         else:\n",
    "#             self.label_dic=dict(zip(lableWord.values(), lableWord.keys()))\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "        \n",
    "    def process(self):\n",
    "        data=read_flex_data('/home/iot/jupyter/root_dir/liudongdong/data/flexData/chars/char1')\n",
    "        data_list=handleToData(data)\n",
    "            \n",
    "        data, slices = self.collate(data_list) # Here used to be [data] for one graph\n",
    "        print(\"save file\")\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "        \n",
    "\n",
    "\n",
    "test_dataset = GraphFlexSensorInMem(root='./dataset/char/train')\n",
    "print(test_dataset[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c76b67b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T07:24:57.822615Z",
     "start_time": "2021-09-07T07:24:56.495835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_attr=[15, 1], edge_index=[2, 15], x=[26], y=[1])\n",
      "Data(edge_attr=[15, 1], edge_index=[2, 15], x=[26], y=[1])\n",
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "Batch(batch=[1664], edge_attr=[948, 1], edge_index=[2, 948], ptr=[65], x=[1664], y=[64])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "Batch(batch=[1664], edge_attr=[943, 1], edge_index=[2, 943], ptr=[65], x=[1664], y=[64])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "Batch(batch=[1664], edge_attr=[946, 1], edge_index=[2, 946], ptr=[65], x=[1664], y=[64])\n",
      "\n",
      "Step 4:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "Batch(batch=[1664], edge_attr=[952, 1], edge_index=[2, 952], ptr=[65], x=[1664], y=[64])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "train_dataset = GraphFlexSensorInMem(root='./dataset/char/train')\n",
    "test_dataset = GraphFlexSensorInMem(root='./dataset/char/test')\n",
    "torch.manual_seed(12345)\n",
    "train_dataset=train_dataset.shuffle()\n",
    "test_dataset = test_dataset.shuffle()\n",
    "print(train_dataset[0])  #Data(edge_attr=[15, 1], edge_index=[2, 15], x=[26], y=[1])\n",
    "print(test_dataset[1]) \n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "for step, data in enumerate(test_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data) #Batch(batch=[1169], edge_attr=[2592, 4], edge_index=[2, 2592], x=[1169, 7], y=[64])\n",
    "    print()\n",
    "    if step>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23764cbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T07:34:35.293692Z",
     "start_time": "2021-09-07T07:34:34.984303Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got -2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cb3eabd1ddb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m#train_acc = test(train_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-cb3eabd1ddb0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Iterate in batches over the training dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m          \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Perform a single forward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m          \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute the loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m          \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Derive gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dataglove/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-42d73fc492d2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# 1. Obtain node embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dataglove/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dataglove/lib/python3.7/site-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                     edge_index, edge_weight = gcn_norm(  # yapf: disable\n\u001b[0;32m--> 161\u001b[0;31m                         \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                         self.improved, self.add_self_loops)\n\u001b[1;32m    163\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got -2)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#GCN(\n",
    "#  (conv1): GCNConv(7, 64)\n",
    "#  (conv2): GCNConv(64, 64)\n",
    "#  (conv3): GCNConv(64, 64)\n",
    "#  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
    "#)\n",
    "\n",
    "# num_node_features,hidden_channels,num_classes\n",
    "model = GCN(num_node_features=6,hidden_channels=64,num_classes=9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    train()\n",
    "    #train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d8fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本文件旨在为所有数据集提供一个统一的数据处理手段，待处理的文件需要有以下四列构成：FromID，ToID，weight，snapshotID。输出为一个tg中由data\n",
    "# 组成的dataset。\n",
    "import os\n",
    "import torch\n",
    "import torch_geometric as tg\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# window_size记录每一个data由多少张snapshot构成\n",
    "def load_edges(file_path, edge_window_size=1, to_undirected=True, reform_values=False, do_coalesce=True):\n",
    "    # 从文件中读取数据，主要构建出edge_index, edge_attr, graph_idx, num_nodes四个值（整个数据集的，并对其做一点变换（变为无向图，重组织边权重之类））\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = f.read().split('\\n')[1:-1]\n",
    "        data = [[x for x in line.split(',')] for line in data]\n",
    "\n",
    "        edge_index = [[int(line[0]), int(line[1])] for line in data]\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "        edge_index = edge_index - edge_index.min()\n",
    "\n",
    "        # 对edge_index重排序，令id连续\n",
    "        nodes, edge_index = edge_index.unique(return_inverse=True)\n",
    "\n",
    "        edge_index = edge_index.t().contiguous()\n",
    "        num_nodes = nodes.shape[0]\n",
    "\n",
    "        edge_attr = [float(line[2]) for line in data]\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.long)\n",
    "\n",
    "        # 重置边权重\n",
    "        pos_indices = edge_attr > 0\n",
    "        neg_indices = edge_attr <= 0\n",
    "        edge_attr[pos_indices] = 1\n",
    "        edge_attr[neg_indices] = -1\n",
    "\n",
    "        graph_idx = [int(line[3]) for line in data]\n",
    "        graph_idx = torch.tensor(graph_idx, dtype=torch.long)\n",
    "\n",
    "    max_time = graph_idx.max().item()\n",
    "    min_time = graph_idx.min().item()\n",
    "\n",
    "    if to_undirected:\n",
    "        edge_index = torch.cat((edge_index, edge_index[[1, 0], :]), dim=-1)\n",
    "        edge_attr = torch.cat((edge_attr, edge_attr), dim=-1)\n",
    "        graph_idx = torch.cat((graph_idx, graph_idx), dim=-1)\n",
    "\n",
    "    # 这里的处理，针对edge_weight的值有正有负的情形，将每一个snapshot上的多次出现的同一边的权重加和作为其边标签，将出现次数（不论正负）作为边权重\n",
    "    if reform_values:\n",
    "\n",
    "        neg_mask = edge_attr == -1\n",
    "        neg_sp_indices = torch.cat((edge_index[:, neg_mask], graph_idx[neg_mask].view(1, -1)), dim=0)\n",
    "        neg_sp_values = edge_attr[neg_mask]\n",
    "        neg_sp_edges = torch.sparse.Tensor(neg_sp_indices,\n",
    "                                           neg_sp_values,\n",
    "                                           torch.Size([num_nodes,\n",
    "                                                       num_nodes,\n",
    "                                                       max_time+1])).coalesce()\n",
    "\n",
    "        pos_mask = edge_attr == 1\n",
    "        pos_sp_indices = torch.cat((edge_index[:, pos_mask], graph_idx[pos_mask].view(1, -1)), dim=0)\n",
    "        pos_sp_values = edge_attr[pos_mask]\n",
    "        pos_sp_edges = torch.sparse.Tensor(pos_sp_indices,\n",
    "                                           pos_sp_values,\n",
    "                                           torch.Size([num_nodes,\n",
    "                                                       num_nodes,\n",
    "                                                       max_time+1])).coalesce()\n",
    "\n",
    "        pos_sp_edges *= 1000\n",
    "        sp_edges = (pos_sp_edges - neg_sp_edges).coalesce()\n",
    "        vals = sp_edges._values()\n",
    "        neg_vals = vals % 1000\n",
    "        pos_vals = vals // 1000\n",
    "\n",
    "        edge_attr = pos_vals + neg_vals\n",
    "        edge_indices = sp_edges._indices()\n",
    "        edge_index = edge_indices[:2]\n",
    "        graph_idx = edge_indices[2]\n",
    "\n",
    "    # 开始构造dataset\n",
    "    data_list = []\n",
    "    for i in range(graph_idx.max().item() + 1):\n",
    "        mask = (graph_idx > (i - edge_window_size)) & (graph_idx <= i)\n",
    "        data = Data()\n",
    "        data.edge_index = edge_index[:, mask]\n",
    "        data.edge_attr = edge_attr[mask]\n",
    "        data.num_nodes = num_nodes\n",
    "        data_list.append(data)\n",
    "\n",
    "    if do_coalesce:\n",
    "        for data in data_list:\n",
    "            data.coalesce()\n",
    "\n",
    "    return data_list, num_nodes, max_time, min_time\n",
    "\n",
    "\n",
    "class DynamicDataset(Dataset):\n",
    "    def __init__(self, args):\n",
    "        super(DynamicDataset, self).__init__()\n",
    "\n",
    "        file_path = os.path.join(args.folder, args.edges_file)\n",
    "\n",
    "        self.data_list, self.num_nodes, self.max_time, self.min_time = load_edges(file_path)\n",
    "        # # max_time是所有的graph idx中的最大值,而不是总数(因为idx从0开始计数)\n",
    "        # self.max_time = len(self.data_list) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
