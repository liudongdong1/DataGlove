{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "148f649f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:15:34.534441Z",
     "start_time": "2021-10-15T13:15:32.493370Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path, PosixPath\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions.multinomial import Multinomial\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9eb44a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:16:16.137749Z",
     "start_time": "2021-10-15T13:16:15.789006Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Dataloader\n",
    "class TextDS(Dataset):\n",
    "    def __init__(self, text_file: PosixPath, seq_length: int):\n",
    "        #defined by user\n",
    "        self.path = Path('.')/text_file\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        #text data\n",
    "        self.text = None\n",
    "        self.vocab = None \n",
    "        self.char2idx = None\n",
    "        self.idx2char = None\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        #datasets\n",
    "        self.samples = []\n",
    "        self.input_dataset = None\n",
    "        self.target_dataset = None\n",
    "        \n",
    "        #initialization of dataset\n",
    "        self.init_dataset()\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "    def init_dataset(self):\n",
    "        #text loading and cleaning\n",
    "        text = '' \n",
    "        utf8_apostrophe = b'\\xe2\\x80\\x99'.decode(\"utf8\") \n",
    "        string_apostrophe = \"'\" \n",
    "        with open(self.path,encoding=\"utf8\") as f: \n",
    "            for line in f: \n",
    "                line = re.sub(utf8_apostrophe, string_apostrophe, line) \n",
    "                line = re.sub(\"‘\", \"\", line) \n",
    "                text += self.subst(line.strip('\\n').lower()) \n",
    "\n",
    "        text = re.sub('speech [0-9]', '', text) \n",
    "        self.text = text[1:]\n",
    "        \n",
    "        \n",
    "        #vocab and dataset\n",
    "        self.vocab = sorted(set(self.text)) \n",
    "        self.char2idx = {u:i for i, u in enumerate(self.vocab)} \n",
    "        self.idx2char = {i:u for i, u in enumerate(self.vocab)}\n",
    "        \n",
    "        #creating datasets:\n",
    "        text_int = np.array([self.char2idx[c] for c in self.text])\n",
    "        examples_per_epoch = len(text_int)//(self.seq_length - 1)\n",
    "\n",
    "        input_dataset = torch.from_numpy(text_int[:-1])\n",
    "        target_dataset = torch.from_numpy(text_int[1:])\n",
    "\n",
    "        self.input_dataset = torch.chunk(input_dataset,examples_per_epoch)[:-1]\n",
    "        self.target_dataset = torch.chunk(target_dataset,examples_per_epoch)[:-1]\n",
    "\n",
    "        for i in range(len(self.input_dataset)):\n",
    "            self.samples.append((self.input_dataset[i],self.target_dataset[i]))\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.samples[idx]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def to_int(self,char):\n",
    "        #text to integers:\n",
    "        return self.char2idx[char] \n",
    "\n",
    "    \n",
    "    def to_char(self,integer):\n",
    "        #integers to text\n",
    "        return self.idx2char[integer] \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def subst(self,phrase): \n",
    "        #takes care of short forms \n",
    "        phrase = re.sub(r\"won't\", \"will not\", phrase) \n",
    "        phrase = re.sub(r\"can't\", \"cannot\", phrase) \n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase) \n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase) \n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase) \n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase) \n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase) \n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase) \n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase) \n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase) \n",
    "        phrase = re.sub(r\"\\'cause\", \" because\", phrase) \n",
    "        phrase = re.sub(r\"\\‘cause\", \" because\", phrase) \n",
    "\n",
    "        #takes care of unecessary signs for text generation    \n",
    "        phrase = re.sub(r\"é\", \"e\", phrase) \n",
    "        phrase = re.sub(r\"=\", \"\", phrase) \n",
    "        phrase = re.sub(r\"_\", \"\", phrase) \n",
    "        phrase = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", phrase) \n",
    "        phrase = re.sub(r'“', '', phrase) \n",
    "        phrase = re.sub(r'”', '', phrase) \n",
    "        phrase = re.sub(r'\"', '', phrase)  \n",
    "        phrase = re.sub(r'/', '', phrase) \n",
    "        phrase = re.sub(r\"@realdonaldtrump.\", \"\", phrase) \n",
    "        phrase = re.sub(r\"q&a\", \"\", phrase) \n",
    "        phrase = re.sub(r\"…\", \",\", phrase) \n",
    "        phrase = re.sub(r\"\\.{3}\", \",\", phrase) \n",
    "        phrase = re.sub(r\"—\", \",\", phrase) \n",
    "        phrase = re.sub(r\"–\", \",\", phrase) \n",
    "        phrase = re.sub(r\"%\", \"\", phrase)\n",
    "\n",
    "        phrase = re.sub(r\"--\", \" \", phrase)  \n",
    "        phrase = re.sub(r\"-\", \"\", phrase) \n",
    "        #To ease punctuation: \n",
    "        phrase = re.sub(r\";\", \",\", phrase) \n",
    "        phrase = re.sub(r\"!\", \".\", phrase)  \n",
    "\n",
    "\n",
    "        return phrase\n",
    "\n",
    "\n",
    "#Loading datasets  \n",
    "batch_size = 64\n",
    "dataset_train = TextDS('/home/iot/jupyter/root_dir/liudongdong/src/ai/charPredict/Character_preditction-Pytorch/trump_train.txt', 150)\n",
    "dataloader_train  = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_test = TextDS('/home/iot/jupyter/root_dir/liudongdong/src/ai/charPredict/Character_preditction-Pytorch/trump_val.txt', 150)\n",
    "dataloader_test  = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9097bbe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:16:28.395150Z",
     "start_time": "2021-10-15T13:16:28.320612Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size,output_size,batch_size):\n",
    "        super(Model, self).__init__()\n",
    "        #Parameters for weights:\n",
    "        self.input = input_size\n",
    "        self.hidden = hidden_size\n",
    "        self.output = output_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        ###Weights###\n",
    "        #Input weights:\n",
    "        self.W_input = nn.Parameter(torch.Tensor(self.input,self.hidden))\n",
    "        self.R_input = nn.Parameter(torch.Tensor(self.hidden,self.hidden))\n",
    "        self.b_input = nn.Parameter(torch.Tensor(self.hidden))\n",
    "        \n",
    "        #Output weights:\n",
    "        self.W_output = nn.Parameter(torch.Tensor(self.input,self.hidden))\n",
    "        self.R_output = nn.Parameter(torch.Tensor(self.hidden,self.hidden))\n",
    "        self.b_output = nn.Parameter(torch.Tensor(self.hidden))\n",
    "        \n",
    "        #Forget weights:\n",
    "        self.W_forget = nn.Parameter(torch.Tensor(self.input,self.hidden))\n",
    "        self.R_forget = nn.Parameter(torch.Tensor(self.hidden,self.hidden))\n",
    "        self.b_forget = nn.Parameter(torch.Tensor(self.hidden))\n",
    "        \n",
    "        #New Cell wights:\n",
    "        self.W_cell = nn.Parameter(torch.Tensor(self.input,self.hidden))\n",
    "        self.R_cell = nn.Parameter(torch.Tensor(self.hidden,self.hidden))\n",
    "        self.b_cell = nn.Parameter(torch.Tensor(self.hidden))\n",
    "        \n",
    "        #Linear weights for prediction:\n",
    "        self.W_pred = nn.Parameter(torch.Tensor(self.hidden,self.output))\n",
    "        self.b_pred = nn.Parameter(torch.Tensor(self.output))\n",
    "        \n",
    "        #Embedding:\n",
    "        self.emb = nn.Embedding(num_embeddings = self.output, embedding_dim =  self.input)\n",
    "        \n",
    "        #Softmax:\n",
    "        self.Softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Weights initialization:\n",
    "        self.weight_init()\n",
    "        \n",
    "        \n",
    "    def weight_init(self):\n",
    "        for p in self.parameters():\n",
    "            if len(p.shape)>=2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            else:\n",
    "                nn.init.zeros_(p.data)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        logits_batch = []\n",
    "        prediction_batch = []\n",
    "        \n",
    "        #cell staste and hidden state:\n",
    "        self.c_t = torch.zeros(x.shape[0],self.hidden)\n",
    "        self.h_t = torch.zeros(x.shape[0],self.hidden)\n",
    "        \n",
    "        embeddings = self.emb(x)\n",
    "        \n",
    "        for i in range(embeddings.shape[1]): \n",
    "            x_t = embeddings[:,i,:]\n",
    "            \n",
    "            input_gate = torch.sigmoid(x_t @ self.W_input + self.h_t @ self.R_input + self.b_input)\n",
    "            output_gate = torch.sigmoid(x_t @ self.W_output + self.h_t @ self.R_output + self.b_output)\n",
    "            forget_gate = torch.sigmoid(x_t @ self.W_forget + self.h_t @ self.R_forget + self.b_forget)\n",
    "            cell_gate = torch.tanh(x_t @ self.W_cell + self.h_t @ self.R_cell + self.b_cell)\n",
    "            \n",
    "            self.c_t = forget_gate * self.c_t + input_gate * cell_gate\n",
    "            self.h_t = output_gate * torch.tanh(self.c_t)\n",
    "            \n",
    "            logits = self.h_t @ self.W_pred + self.b_pred\n",
    "            probs = self.Softmax(self.h_t @ self.W_pred + self.b_pred)\n",
    "            \n",
    "            pred = torch.argmax(probs,dim = 1)\n",
    "            pred = pred.unsqueeze(1)\n",
    "            \n",
    "            prediction_batch.append(pred)\n",
    "            logits_batch.append(logits)\n",
    "            \n",
    "        \n",
    "        prediction_batch = torch.stack(prediction_batch,dim = 1)\n",
    "        prediction_batch = prediction_batch.squeeze(2)\n",
    "\n",
    "        logits_batch = torch.stack(logits_batch,dim = 1)\n",
    "        logits_batch = torch.transpose(logits_batch,1,2)\n",
    "\n",
    "        return logits_batch,prediction_batch\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \n",
    "        embeddings = self.emb(x)\n",
    "        for i in range(embeddings.shape[1]): \n",
    "\n",
    "            x_t = embeddings[:,i,:]\n",
    "            \n",
    "            input_gate = torch.sigmoid(x_t @ self.W_input + self.h_t @ self.R_input + self.b_input)\n",
    "            output_gate = torch.sigmoid(x_t @ self.W_output + self.h_t @ self.R_output + self.b_output)\n",
    "            forget_gate = torch.sigmoid(x_t @ self.W_forget + self.h_t @ self.R_forget + self.b_forget)\n",
    "            cell_gate = torch.tanh(x_t @ self.W_cell + self.h_t @ self.R_cell + self.b_cell)\n",
    "            \n",
    "            self.c_t = forget_gate * self.c_t + input_gate * cell_gate\n",
    "            self.h_t = output_gate * torch.tanh(self.c_t)\n",
    "            \n",
    "            if i == embeddings.shape[1] - 1:\n",
    "                probs = self.Softmax(self.h_t @ self.W_pred + self.b_pred)\n",
    "                return probs\n",
    "        \n",
    "        \n",
    "    def reset_hidden(self,batch_size):\n",
    "        self.c_t = torch.zeros(batch_size,self.hidden)\n",
    "        self.h_t = torch.zeros(batch_size,self.hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "423d2d32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:16:37.554056Z",
     "start_time": "2021-10-15T13:16:37.538793Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Model Training and Validation\n",
    "\n",
    "def train(model,train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_overall_train = 0.0\n",
    "    loss_plot = 0.0\n",
    "    \n",
    "    #training:\n",
    "    for i, (inputs_train,targets_train) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits_train,pred_train = model.forward(inputs_train)\n",
    "        loss_train = criterion(logits_train, targets_train)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_overall_train += loss_train.item()\n",
    "        loss_plot += loss_train.item()\n",
    "        accuracy_train = torch.sum(pred_train == targets_train)/(targets_train.shape[0] * targets_train.shape[1])\n",
    "        \n",
    "        if i % 10 == 9:\n",
    "            print('[%d, %5d] Loss Train: %.3f' %(epoch + 1, i + 1, loss_overall_train / 10))\n",
    "            print('[%d, %5d] Accuracy Train: %.2f' % (epoch + 1, i + 1,accuracy_train * 100) + '%')\n",
    "            print('----------------------------') \n",
    "            loss_overall_train = 0.0\n",
    "    \n",
    "    return loss_plot / (i + 1)\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    loss_overall_test = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs_test,targets_test) in enumerate(test_loader):\n",
    "\n",
    "            logits_test,pred_test = model.forward(inputs_test)\n",
    "            loss_test = criterion(logits_test, targets_test)\n",
    "\n",
    "            loss_overall_test += loss_test.item()\n",
    "            accuracy_test = torch.sum(pred_test == targets_test)/(targets_test.shape[0] * targets_test.shape[1])\n",
    "\n",
    "            \n",
    "    print('[%d] Loss Test: %.3f' %(epoch + 1, loss_overall_test / (i + 1)))\n",
    "    print('[%d] Accuracy Test: %.2f' % (epoch + 1,accuracy_test * 100) + '%')\n",
    "    print('EPOCH [%d] is completed!'% (epoch + 1))\n",
    "    print('----------------------------')\n",
    "    return loss_overall_test / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbf10a97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:47:37.175142Z",
     "start_time": "2021-10-15T13:16:47.604274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] Loss Train: 3.579\n",
      "[1,    10] Accuracy Train: 25.41%\n",
      "----------------------------\n",
      "[1,    20] Loss Train: 2.562\n",
      "[1,    20] Accuracy Train: 30.83%\n",
      "----------------------------\n",
      "[1,    30] Loss Train: 2.312\n",
      "[1,    30] Accuracy Train: 35.93%\n",
      "----------------------------\n",
      "[1,    40] Loss Train: 2.173\n",
      "[1,    40] Accuracy Train: 39.20%\n",
      "----------------------------\n",
      "[1,    50] Loss Train: 2.035\n",
      "[1,    50] Accuracy Train: 43.02%\n",
      "----------------------------\n",
      "[1,    60] Loss Train: 1.912\n",
      "[1,    60] Accuracy Train: 45.09%\n",
      "----------------------------\n",
      "[1,    70] Loss Train: 1.788\n",
      "[1,    70] Accuracy Train: 48.33%\n",
      "----------------------------\n",
      "[1,    80] Loss Train: 1.693\n",
      "[1,    80] Accuracy Train: 50.81%\n",
      "----------------------------\n",
      "[1,    90] Loss Train: 1.612\n",
      "[1,    90] Accuracy Train: 54.04%\n",
      "----------------------------\n",
      "[1] Loss Test: 1.709\n",
      "[1] Accuracy Test: 50.22%\n",
      "EPOCH [1] is completed!\n",
      "----------------------------\n",
      "[2,    10] Loss Train: 1.536\n",
      "[2,    10] Accuracy Train: 53.96%\n",
      "----------------------------\n",
      "[2,    20] Loss Train: 1.480\n",
      "[2,    20] Accuracy Train: 56.53%\n",
      "----------------------------\n",
      "[2,    30] Loss Train: 1.439\n",
      "[2,    30] Accuracy Train: 57.48%\n",
      "----------------------------\n",
      "[2,    40] Loss Train: 1.390\n",
      "[2,    40] Accuracy Train: 59.82%\n",
      "----------------------------\n",
      "[2,    50] Loss Train: 1.366\n",
      "[2,    50] Accuracy Train: 59.98%\n",
      "----------------------------\n",
      "[2,    60] Loss Train: 1.345\n",
      "[2,    60] Accuracy Train: 60.36%\n",
      "----------------------------\n",
      "[2,    70] Loss Train: 1.286\n",
      "[2,    70] Accuracy Train: 62.51%\n",
      "----------------------------\n",
      "[2,    80] Loss Train: 1.278\n",
      "[2,    80] Accuracy Train: 61.11%\n",
      "----------------------------\n",
      "[2,    90] Loss Train: 1.281\n",
      "[2,    90] Accuracy Train: 61.90%\n",
      "----------------------------\n",
      "[2] Loss Test: 1.404\n",
      "[2] Accuracy Test: 57.24%\n",
      "EPOCH [2] is completed!\n",
      "----------------------------\n",
      "[3,    10] Loss Train: 1.215\n",
      "[3,    10] Accuracy Train: 64.47%\n",
      "----------------------------\n",
      "[3,    20] Loss Train: 1.207\n",
      "[3,    20] Accuracy Train: 62.40%\n",
      "----------------------------\n",
      "[3,    30] Loss Train: 1.207\n",
      "[3,    30] Accuracy Train: 63.10%\n",
      "----------------------------\n",
      "[3,    40] Loss Train: 1.194\n",
      "[3,    40] Accuracy Train: 64.07%\n",
      "----------------------------\n",
      "[3,    50] Loss Train: 1.197\n",
      "[3,    50] Accuracy Train: 64.10%\n",
      "----------------------------\n",
      "[3,    60] Loss Train: 1.187\n",
      "[3,    60] Accuracy Train: 64.40%\n",
      "----------------------------\n",
      "[3,    70] Loss Train: 1.184\n",
      "[3,    70] Accuracy Train: 65.36%\n",
      "----------------------------\n",
      "[3,    80] Loss Train: 1.180\n",
      "[3,    80] Accuracy Train: 64.01%\n",
      "----------------------------\n",
      "[3,    90] Loss Train: 1.163\n",
      "[3,    90] Accuracy Train: 64.44%\n",
      "----------------------------\n",
      "[3] Loss Test: 1.297\n",
      "[3] Accuracy Test: 60.91%\n",
      "EPOCH [3] is completed!\n",
      "----------------------------\n",
      "[4,    10] Loss Train: 1.130\n",
      "[4,    10] Accuracy Train: 65.58%\n",
      "----------------------------\n",
      "[4,    20] Loss Train: 1.123\n",
      "[4,    20] Accuracy Train: 65.65%\n",
      "----------------------------\n",
      "[4,    30] Loss Train: 1.129\n",
      "[4,    30] Accuracy Train: 64.86%\n",
      "----------------------------\n",
      "[4,    40] Loss Train: 1.110\n",
      "[4,    40] Accuracy Train: 66.84%\n",
      "----------------------------\n",
      "[4,    50] Loss Train: 1.111\n",
      "[4,    50] Accuracy Train: 66.47%\n",
      "----------------------------\n",
      "[4,    60] Loss Train: 1.098\n",
      "[4,    60] Accuracy Train: 64.73%\n",
      "----------------------------\n",
      "[4,    70] Loss Train: 1.101\n",
      "[4,    70] Accuracy Train: 66.49%\n",
      "----------------------------\n",
      "[4,    80] Loss Train: 1.101\n",
      "[4,    80] Accuracy Train: 65.77%\n",
      "----------------------------\n",
      "[4,    90] Loss Train: 1.100\n",
      "[4,    90] Accuracy Train: 66.09%\n",
      "----------------------------\n",
      "[4] Loss Test: 1.248\n",
      "[4] Accuracy Test: 64.33%\n",
      "EPOCH [4] is completed!\n",
      "----------------------------\n",
      "[5,    10] Loss Train: 1.067\n",
      "[5,    10] Accuracy Train: 67.64%\n",
      "----------------------------\n",
      "[5,    20] Loss Train: 1.048\n",
      "[5,    20] Accuracy Train: 67.61%\n",
      "----------------------------\n",
      "[5,    30] Loss Train: 1.072\n",
      "[5,    30] Accuracy Train: 66.98%\n",
      "----------------------------\n",
      "[5,    40] Loss Train: 1.059\n",
      "[5,    40] Accuracy Train: 67.17%\n",
      "----------------------------\n",
      "[5,    50] Loss Train: 1.060\n",
      "[5,    50] Accuracy Train: 67.90%\n",
      "----------------------------\n",
      "[5,    60] Loss Train: 1.061\n",
      "[5,    60] Accuracy Train: 67.85%\n",
      "----------------------------\n",
      "[5,    70] Loss Train: 1.058\n",
      "[5,    70] Accuracy Train: 67.15%\n",
      "----------------------------\n",
      "[5,    80] Loss Train: 1.069\n",
      "[5,    80] Accuracy Train: 67.09%\n",
      "----------------------------\n",
      "[5,    90] Loss Train: 1.059\n",
      "[5,    90] Accuracy Train: 68.47%\n",
      "----------------------------\n",
      "[5] Loss Test: 1.209\n",
      "[5] Accuracy Test: 65.18%\n",
      "EPOCH [5] is completed!\n",
      "----------------------------\n",
      "[6,    10] Loss Train: 1.014\n",
      "[6,    10] Accuracy Train: 68.65%\n",
      "----------------------------\n",
      "[6,    20] Loss Train: 1.025\n",
      "[6,    20] Accuracy Train: 68.80%\n",
      "----------------------------\n",
      "[6,    30] Loss Train: 1.024\n",
      "[6,    30] Accuracy Train: 67.98%\n",
      "----------------------------\n",
      "[6,    40] Loss Train: 1.033\n",
      "[6,    40] Accuracy Train: 68.02%\n",
      "----------------------------\n",
      "[6,    50] Loss Train: 1.019\n",
      "[6,    50] Accuracy Train: 67.65%\n",
      "----------------------------\n",
      "[6,    60] Loss Train: 1.015\n",
      "[6,    60] Accuracy Train: 67.84%\n",
      "----------------------------\n",
      "[6,    70] Loss Train: 1.021\n",
      "[6,    70] Accuracy Train: 67.20%\n",
      "----------------------------\n",
      "[6,    80] Loss Train: 1.038\n",
      "[6,    80] Accuracy Train: 68.06%\n",
      "----------------------------\n",
      "[6,    90] Loss Train: 1.034\n",
      "[6,    90] Accuracy Train: 69.20%\n",
      "----------------------------\n",
      "[6] Loss Test: 1.206\n",
      "[6] Accuracy Test: 64.33%\n",
      "EPOCH [6] is completed!\n",
      "----------------------------\n",
      "[7,    10] Loss Train: 0.977\n",
      "[7,    10] Accuracy Train: 68.33%\n",
      "----------------------------\n",
      "[7,    20] Loss Train: 0.990\n",
      "[7,    20] Accuracy Train: 69.40%\n",
      "----------------------------\n",
      "[7,    30] Loss Train: 0.991\n",
      "[7,    30] Accuracy Train: 68.72%\n",
      "----------------------------\n",
      "[7,    40] Loss Train: 1.002\n",
      "[7,    40] Accuracy Train: 68.20%\n",
      "----------------------------\n",
      "[7,    50] Loss Train: 0.999\n",
      "[7,    50] Accuracy Train: 68.22%\n",
      "----------------------------\n",
      "[7,    60] Loss Train: 0.996\n",
      "[7,    60] Accuracy Train: 69.53%\n",
      "----------------------------\n",
      "[7,    70] Loss Train: 1.006\n",
      "[7,    70] Accuracy Train: 69.57%\n",
      "----------------------------\n",
      "[7,    80] Loss Train: 1.010\n",
      "[7,    80] Accuracy Train: 69.16%\n",
      "----------------------------\n",
      "[7,    90] Loss Train: 1.001\n",
      "[7,    90] Accuracy Train: 68.68%\n",
      "----------------------------\n",
      "[7] Loss Test: 1.206\n",
      "[7] Accuracy Test: 63.71%\n",
      "EPOCH [7] is completed!\n",
      "----------------------------\n",
      "[8,    10] Loss Train: 0.950\n",
      "[8,    10] Accuracy Train: 69.77%\n",
      "----------------------------\n",
      "[8,    20] Loss Train: 0.952\n",
      "[8,    20] Accuracy Train: 70.68%\n",
      "----------------------------\n",
      "[8,    30] Loss Train: 0.965\n",
      "[8,    30] Accuracy Train: 69.92%\n",
      "----------------------------\n",
      "[8,    40] Loss Train: 0.971\n",
      "[8,    40] Accuracy Train: 69.44%\n",
      "----------------------------\n",
      "[8,    50] Loss Train: 0.983\n",
      "[8,    50] Accuracy Train: 69.75%\n",
      "----------------------------\n",
      "[8,    60] Loss Train: 0.978\n",
      "[8,    60] Accuracy Train: 69.18%\n",
      "----------------------------\n",
      "[8,    70] Loss Train: 0.986\n",
      "[8,    70] Accuracy Train: 67.97%\n",
      "----------------------------\n",
      "[8,    80] Loss Train: 0.988\n",
      "[8,    80] Accuracy Train: 69.57%\n",
      "----------------------------\n",
      "[8,    90] Loss Train: 0.985\n",
      "[8,    90] Accuracy Train: 68.86%\n",
      "----------------------------\n",
      "[8] Loss Test: 1.199\n",
      "[8] Accuracy Test: 64.00%\n",
      "EPOCH [8] is completed!\n",
      "----------------------------\n",
      "[9,    10] Loss Train: 0.944\n",
      "[9,    10] Accuracy Train: 71.45%\n",
      "----------------------------\n",
      "[9,    20] Loss Train: 0.939\n",
      "[9,    20] Accuracy Train: 70.71%\n",
      "----------------------------\n",
      "[9,    30] Loss Train: 0.950\n",
      "[9,    30] Accuracy Train: 69.97%\n",
      "----------------------------\n",
      "[9,    40] Loss Train: 0.944\n",
      "[9,    40] Accuracy Train: 71.54%\n",
      "----------------------------\n",
      "[9,    50] Loss Train: 0.963\n",
      "[9,    50] Accuracy Train: 70.38%\n",
      "----------------------------\n",
      "[9,    60] Loss Train: 0.954\n",
      "[9,    60] Accuracy Train: 68.82%\n",
      "----------------------------\n",
      "[9,    70] Loss Train: 0.962\n",
      "[9,    70] Accuracy Train: 70.12%\n",
      "----------------------------\n",
      "[9,    80] Loss Train: 0.960\n",
      "[9,    80] Accuracy Train: 70.46%\n",
      "----------------------------\n",
      "[9,    90] Loss Train: 0.973\n",
      "[9,    90] Accuracy Train: 70.23%\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9] Loss Test: 1.215\n",
      "[9] Accuracy Test: 62.58%\n",
      "EPOCH [9] is completed!\n",
      "----------------------------\n",
      "[10,    10] Loss Train: 0.930\n",
      "[10,    10] Accuracy Train: 71.33%\n",
      "----------------------------\n",
      "[10,    20] Loss Train: 0.923\n",
      "[10,    20] Accuracy Train: 71.24%\n",
      "----------------------------\n",
      "[10,    30] Loss Train: 0.926\n",
      "[10,    30] Accuracy Train: 70.69%\n",
      "----------------------------\n",
      "[10,    40] Loss Train: 0.937\n",
      "[10,    40] Accuracy Train: 70.33%\n",
      "----------------------------\n",
      "[10,    50] Loss Train: 0.946\n",
      "[10,    50] Accuracy Train: 70.59%\n",
      "----------------------------\n",
      "[10,    60] Loss Train: 0.945\n",
      "[10,    60] Accuracy Train: 71.33%\n",
      "----------------------------\n",
      "[10,    70] Loss Train: 0.945\n",
      "[10,    70] Accuracy Train: 70.18%\n",
      "----------------------------\n",
      "[10,    80] Loss Train: 0.952\n",
      "[10,    80] Accuracy Train: 70.03%\n",
      "----------------------------\n",
      "[10,    90] Loss Train: 0.951\n",
      "[10,    90] Accuracy Train: 70.60%\n",
      "----------------------------\n",
      "[10] Loss Test: 1.182\n",
      "[10] Accuracy Test: 65.00%\n",
      "EPOCH [10] is completed!\n",
      "----------------------------\n",
      "[11,    10] Loss Train: 0.908\n",
      "[11,    10] Accuracy Train: 70.92%\n",
      "----------------------------\n",
      "[11,    20] Loss Train: 0.922\n",
      "[11,    20] Accuracy Train: 70.76%\n",
      "----------------------------\n",
      "[11,    30] Loss Train: 0.912\n",
      "[11,    30] Accuracy Train: 71.04%\n",
      "----------------------------\n",
      "[11,    40] Loss Train: 0.930\n",
      "[11,    40] Accuracy Train: 70.99%\n",
      "----------------------------\n",
      "[11,    50] Loss Train: 0.922\n",
      "[11,    50] Accuracy Train: 70.28%\n",
      "----------------------------\n",
      "[11,    60] Loss Train: 0.935\n",
      "[11,    60] Accuracy Train: 70.23%\n",
      "----------------------------\n",
      "[11,    70] Loss Train: 0.932\n",
      "[11,    70] Accuracy Train: 71.62%\n",
      "----------------------------\n",
      "[11,    80] Loss Train: 0.946\n",
      "[11,    80] Accuracy Train: 70.91%\n",
      "----------------------------\n",
      "[11,    90] Loss Train: 0.936\n",
      "[11,    90] Accuracy Train: 70.69%\n",
      "----------------------------\n",
      "[11] Loss Test: 1.193\n",
      "[11] Accuracy Test: 64.47%\n",
      "EPOCH [11] is completed!\n",
      "----------------------------\n",
      "[12,    10] Loss Train: 0.889\n",
      "[12,    10] Accuracy Train: 71.27%\n",
      "----------------------------\n",
      "[12,    20] Loss Train: 0.910\n",
      "[12,    20] Accuracy Train: 71.73%\n",
      "----------------------------\n",
      "[12,    30] Loss Train: 0.906\n",
      "[12,    30] Accuracy Train: 71.30%\n",
      "----------------------------\n",
      "[12,    40] Loss Train: 0.908\n",
      "[12,    40] Accuracy Train: 71.45%\n",
      "----------------------------\n",
      "[12,    50] Loss Train: 0.914\n",
      "[12,    50] Accuracy Train: 70.83%\n",
      "----------------------------\n",
      "[12,    60] Loss Train: 0.924\n",
      "[12,    60] Accuracy Train: 70.74%\n",
      "----------------------------\n",
      "[12,    70] Loss Train: 0.927\n",
      "[12,    70] Accuracy Train: 70.23%\n",
      "----------------------------\n",
      "[12,    80] Loss Train: 0.943\n",
      "[12,    80] Accuracy Train: 69.40%\n",
      "----------------------------\n",
      "[12,    90] Loss Train: 0.939\n",
      "[12,    90] Accuracy Train: 70.55%\n",
      "----------------------------\n",
      "[12] Loss Test: 1.188\n",
      "[12] Accuracy Test: 63.82%\n",
      "EPOCH [12] is completed!\n",
      "----------------------------\n",
      "[13,    10] Loss Train: 0.891\n",
      "[13,    10] Accuracy Train: 72.20%\n",
      "----------------------------\n",
      "[13,    20] Loss Train: 0.882\n",
      "[13,    20] Accuracy Train: 72.15%\n",
      "----------------------------\n",
      "[13,    30] Loss Train: 0.898\n",
      "[13,    30] Accuracy Train: 72.57%\n",
      "----------------------------\n",
      "[13,    40] Loss Train: 0.906\n",
      "[13,    40] Accuracy Train: 71.06%\n",
      "----------------------------\n",
      "[13,    50] Loss Train: 0.921\n",
      "[13,    50] Accuracy Train: 69.99%\n",
      "----------------------------\n",
      "[13,    60] Loss Train: 0.922\n",
      "[13,    60] Accuracy Train: 70.96%\n",
      "----------------------------\n",
      "[13,    70] Loss Train: 0.921\n",
      "[13,    70] Accuracy Train: 71.88%\n",
      "----------------------------\n",
      "[13,    80] Loss Train: 0.931\n",
      "[13,    80] Accuracy Train: 71.00%\n",
      "----------------------------\n",
      "[13,    90] Loss Train: 0.934\n",
      "[13,    90] Accuracy Train: 70.23%\n",
      "----------------------------\n",
      "[13] Loss Test: 1.204\n",
      "[13] Accuracy Test: 63.53%\n",
      "EPOCH [13] is completed!\n",
      "----------------------------\n",
      "[14,    10] Loss Train: 0.891\n",
      "[14,    10] Accuracy Train: 72.44%\n",
      "----------------------------\n",
      "[14,    20] Loss Train: 0.875\n",
      "[14,    20] Accuracy Train: 71.96%\n",
      "----------------------------\n",
      "[14,    30] Loss Train: 0.900\n",
      "[14,    30] Accuracy Train: 72.39%\n",
      "----------------------------\n",
      "[14,    40] Loss Train: 0.905\n",
      "[14,    40] Accuracy Train: 71.25%\n",
      "----------------------------\n",
      "[14,    50] Loss Train: 0.911\n",
      "[14,    50] Accuracy Train: 70.73%\n",
      "----------------------------\n",
      "[14,    60] Loss Train: 0.911\n",
      "[14,    60] Accuracy Train: 71.46%\n",
      "----------------------------\n",
      "[14,    70] Loss Train: 0.919\n",
      "[14,    70] Accuracy Train: 71.09%\n",
      "----------------------------\n",
      "[14,    80] Loss Train: 0.922\n",
      "[14,    80] Accuracy Train: 70.80%\n",
      "----------------------------\n",
      "[14,    90] Loss Train: 0.936\n",
      "[14,    90] Accuracy Train: 71.30%\n",
      "----------------------------\n",
      "[14] Loss Test: 1.209\n",
      "[14] Accuracy Test: 64.31%\n",
      "EPOCH [14] is completed!\n",
      "----------------------------\n",
      "[15,    10] Loss Train: 0.881\n",
      "[15,    10] Accuracy Train: 72.34%\n",
      "----------------------------\n",
      "[15,    20] Loss Train: 0.893\n",
      "[15,    20] Accuracy Train: 71.21%\n",
      "----------------------------\n",
      "[15,    30] Loss Train: 0.903\n",
      "[15,    30] Accuracy Train: 71.07%\n",
      "----------------------------\n",
      "[15,    40] Loss Train: 0.903\n",
      "[15,    40] Accuracy Train: 70.51%\n",
      "----------------------------\n",
      "[15,    50] Loss Train: 0.903\n",
      "[15,    50] Accuracy Train: 72.08%\n",
      "----------------------------\n",
      "[15,    60] Loss Train: 0.912\n",
      "[15,    60] Accuracy Train: 71.21%\n",
      "----------------------------\n",
      "[15,    70] Loss Train: 0.924\n",
      "[15,    70] Accuracy Train: 70.20%\n",
      "----------------------------\n",
      "[15,    80] Loss Train: 0.932\n",
      "[15,    80] Accuracy Train: 70.48%\n",
      "----------------------------\n",
      "[15,    90] Loss Train: 0.931\n",
      "[15,    90] Accuracy Train: 71.19%\n",
      "----------------------------\n",
      "[15] Loss Test: 1.202\n",
      "[15] Accuracy Test: 65.42%\n",
      "EPOCH [15] is completed!\n",
      "----------------------------\n",
      "[16,    10] Loss Train: 0.883\n",
      "[16,    10] Accuracy Train: 71.95%\n",
      "----------------------------\n",
      "[16,    20] Loss Train: 0.899\n",
      "[16,    20] Accuracy Train: 72.28%\n",
      "----------------------------\n",
      "[16,    30] Loss Train: 0.902\n",
      "[16,    30] Accuracy Train: 71.94%\n",
      "----------------------------\n",
      "[16,    40] Loss Train: 0.903\n",
      "[16,    40] Accuracy Train: 71.55%\n",
      "----------------------------\n",
      "[16,    50] Loss Train: 0.915\n",
      "[16,    50] Accuracy Train: 70.93%\n",
      "----------------------------\n",
      "[16,    60] Loss Train: 0.915\n",
      "[16,    60] Accuracy Train: 71.15%\n",
      "----------------------------\n",
      "[16,    70] Loss Train: 0.928\n",
      "[16,    70] Accuracy Train: 69.66%\n",
      "----------------------------\n",
      "[16,    80] Loss Train: 0.928\n",
      "[16,    80] Accuracy Train: 71.79%\n",
      "----------------------------\n",
      "[16,    90] Loss Train: 0.934\n",
      "[16,    90] Accuracy Train: 69.92%\n",
      "----------------------------\n",
      "[16] Loss Test: 1.229\n",
      "[16] Accuracy Test: 63.87%\n",
      "EPOCH [16] is completed!\n",
      "----------------------------\n",
      "[17,    10] Loss Train: 0.890\n",
      "[17,    10] Accuracy Train: 71.62%\n",
      "----------------------------\n",
      "[17,    20] Loss Train: 0.894\n",
      "[17,    20] Accuracy Train: 71.24%\n",
      "----------------------------\n",
      "[17,    30] Loss Train: 0.903\n",
      "[17,    30] Accuracy Train: 70.94%\n",
      "----------------------------\n",
      "[17,    40] Loss Train: 0.915\n",
      "[17,    40] Accuracy Train: 71.97%\n",
      "----------------------------\n",
      "[17,    50] Loss Train: 0.927\n",
      "[17,    50] Accuracy Train: 70.28%\n",
      "----------------------------\n",
      "[17,    60] Loss Train: 0.931\n",
      "[17,    60] Accuracy Train: 70.81%\n",
      "----------------------------\n",
      "[17,    70] Loss Train: 0.928\n",
      "[17,    70] Accuracy Train: 70.76%\n",
      "----------------------------\n",
      "[17,    80] Loss Train: 0.935\n",
      "[17,    80] Accuracy Train: 70.98%\n",
      "----------------------------\n",
      "[17,    90] Loss Train: 0.948\n",
      "[17,    90] Accuracy Train: 70.39%\n",
      "----------------------------\n",
      "[17] Loss Test: 1.241\n",
      "[17] Accuracy Test: 61.18%\n",
      "EPOCH [17] is completed!\n",
      "----------------------------\n",
      "[18,    10] Loss Train: 0.896\n",
      "[18,    10] Accuracy Train: 72.02%\n",
      "----------------------------\n",
      "[18,    20] Loss Train: 0.905\n",
      "[18,    20] Accuracy Train: 70.84%\n",
      "----------------------------\n",
      "[18,    30] Loss Train: 0.894\n",
      "[18,    30] Accuracy Train: 71.24%\n",
      "----------------------------\n",
      "[18,    40] Loss Train: 0.924\n",
      "[18,    40] Accuracy Train: 71.09%\n",
      "----------------------------\n",
      "[18,    50] Loss Train: 0.928\n",
      "[18,    50] Accuracy Train: 70.90%\n",
      "----------------------------\n",
      "[18,    60] Loss Train: 0.925\n",
      "[18,    60] Accuracy Train: 70.70%\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18,    70] Loss Train: 0.940\n",
      "[18,    70] Accuracy Train: 70.83%\n",
      "----------------------------\n",
      "[18,    80] Loss Train: 0.946\n",
      "[18,    80] Accuracy Train: 70.09%\n",
      "----------------------------\n",
      "[18,    90] Loss Train: 0.962\n",
      "[18,    90] Accuracy Train: 69.71%\n",
      "----------------------------\n",
      "[18] Loss Test: 1.251\n",
      "[18] Accuracy Test: 61.67%\n",
      "EPOCH [18] is completed!\n",
      "----------------------------\n",
      "[19,    10] Loss Train: 0.899\n",
      "[19,    10] Accuracy Train: 71.15%\n",
      "----------------------------\n",
      "[19,    20] Loss Train: 0.915\n",
      "[19,    20] Accuracy Train: 69.60%\n",
      "----------------------------\n",
      "[19,    30] Loss Train: 0.924\n",
      "[19,    30] Accuracy Train: 70.92%\n",
      "----------------------------\n",
      "[19,    40] Loss Train: 0.923\n",
      "[19,    40] Accuracy Train: 70.91%\n",
      "----------------------------\n",
      "[19,    50] Loss Train: 0.944\n",
      "[19,    50] Accuracy Train: 69.55%\n",
      "----------------------------\n",
      "[19,    60] Loss Train: 0.954\n",
      "[19,    60] Accuracy Train: 70.19%\n",
      "----------------------------\n",
      "[19,    70] Loss Train: 0.948\n",
      "[19,    70] Accuracy Train: 69.72%\n",
      "----------------------------\n",
      "[19,    80] Loss Train: 0.960\n",
      "[19,    80] Accuracy Train: 69.39%\n",
      "----------------------------\n",
      "[19,    90] Loss Train: 0.958\n",
      "[19,    90] Accuracy Train: 70.06%\n",
      "----------------------------\n",
      "[19] Loss Test: 1.251\n",
      "[19] Accuracy Test: 61.98%\n",
      "EPOCH [19] is completed!\n",
      "----------------------------\n",
      "[20,    10] Loss Train: 0.916\n",
      "[20,    10] Accuracy Train: 69.74%\n",
      "----------------------------\n",
      "[20,    20] Loss Train: 0.914\n",
      "[20,    20] Accuracy Train: 72.08%\n",
      "----------------------------\n",
      "[20,    30] Loss Train: 0.940\n",
      "[20,    30] Accuracy Train: 70.30%\n",
      "----------------------------\n",
      "[20,    40] Loss Train: 0.939\n",
      "[20,    40] Accuracy Train: 71.02%\n",
      "----------------------------\n",
      "[20,    50] Loss Train: 0.964\n",
      "[20,    50] Accuracy Train: 69.48%\n",
      "----------------------------\n",
      "[20,    60] Loss Train: 0.970\n",
      "[20,    60] Accuracy Train: 69.47%\n",
      "----------------------------\n",
      "[20,    70] Loss Train: 0.963\n",
      "[20,    70] Accuracy Train: 70.06%\n",
      "----------------------------\n",
      "[20,    80] Loss Train: 0.967\n",
      "[20,    80] Accuracy Train: 69.39%\n",
      "----------------------------\n",
      "[20,    90] Loss Train: 0.973\n",
      "[20,    90] Accuracy Train: 69.38%\n",
      "----------------------------\n",
      "[20] Loss Test: 1.260\n",
      "[20] Accuracy Test: 62.36%\n",
      "EPOCH [20] is completed!\n",
      "----------------------------\n",
      "[21,    10] Loss Train: 0.939\n",
      "[21,    10] Accuracy Train: 70.64%\n",
      "----------------------------\n",
      "[21,    20] Loss Train: 0.956\n",
      "[21,    20] Accuracy Train: 70.03%\n",
      "----------------------------\n",
      "[21,    30] Loss Train: 0.957\n",
      "[21,    30] Accuracy Train: 70.05%\n",
      "----------------------------\n",
      "[21,    40] Loss Train: 0.975\n",
      "[21,    40] Accuracy Train: 69.33%\n",
      "----------------------------\n",
      "[21,    50] Loss Train: 0.982\n",
      "[21,    50] Accuracy Train: 69.75%\n",
      "----------------------------\n",
      "[21,    60] Loss Train: 0.988\n",
      "[21,    60] Accuracy Train: 68.34%\n",
      "----------------------------\n",
      "[21,    70] Loss Train: 0.975\n",
      "[21,    70] Accuracy Train: 69.65%\n",
      "----------------------------\n",
      "[21,    80] Loss Train: 0.992\n",
      "[21,    80] Accuracy Train: 68.15%\n",
      "----------------------------\n",
      "[21,    90] Loss Train: 0.994\n",
      "[21,    90] Accuracy Train: 69.42%\n",
      "----------------------------\n",
      "[21] Loss Test: 1.267\n",
      "[21] Accuracy Test: 62.07%\n",
      "EPOCH [21] is completed!\n",
      "----------------------------\n",
      "[22,    10] Loss Train: 0.953\n",
      "[22,    10] Accuracy Train: 71.10%\n",
      "----------------------------\n",
      "[22,    20] Loss Train: 0.968\n",
      "[22,    20] Accuracy Train: 69.52%\n",
      "----------------------------\n",
      "[22,    30] Loss Train: 0.974\n",
      "[22,    30] Accuracy Train: 69.62%\n",
      "----------------------------\n",
      "[22,    40] Loss Train: 0.976\n",
      "[22,    40] Accuracy Train: 68.73%\n",
      "----------------------------\n",
      "[22,    50] Loss Train: 0.986\n",
      "[22,    50] Accuracy Train: 68.81%\n",
      "----------------------------\n",
      "[22,    60] Loss Train: 0.984\n",
      "[22,    60] Accuracy Train: 69.48%\n",
      "----------------------------\n",
      "[22,    70] Loss Train: 1.007\n",
      "[22,    70] Accuracy Train: 69.31%\n",
      "----------------------------\n",
      "[22,    80] Loss Train: 1.022\n",
      "[22,    80] Accuracy Train: 67.89%\n",
      "----------------------------\n",
      "[22,    90] Loss Train: 1.013\n",
      "[22,    90] Accuracy Train: 68.16%\n",
      "----------------------------\n",
      "[22] Loss Test: 1.274\n",
      "[22] Accuracy Test: 62.78%\n",
      "EPOCH [22] is completed!\n",
      "----------------------------\n",
      "[23,    10] Loss Train: 0.993\n",
      "[23,    10] Accuracy Train: 68.03%\n",
      "----------------------------\n",
      "[23,    20] Loss Train: 0.986\n",
      "[23,    20] Accuracy Train: 68.64%\n",
      "----------------------------\n",
      "[23,    30] Loss Train: 0.993\n",
      "[23,    30] Accuracy Train: 68.80%\n",
      "----------------------------\n",
      "[23,    40] Loss Train: 0.995\n",
      "[23,    40] Accuracy Train: 68.40%\n",
      "----------------------------\n",
      "[23,    50] Loss Train: 1.005\n",
      "[23,    50] Accuracy Train: 68.71%\n",
      "----------------------------\n",
      "[23,    60] Loss Train: 1.009\n",
      "[23,    60] Accuracy Train: 68.54%\n",
      "----------------------------\n",
      "[23,    70] Loss Train: 1.031\n",
      "[23,    70] Accuracy Train: 67.04%\n",
      "----------------------------\n",
      "[23,    80] Loss Train: 1.030\n",
      "[23,    80] Accuracy Train: 68.05%\n",
      "----------------------------\n",
      "[23,    90] Loss Train: 1.052\n",
      "[23,    90] Accuracy Train: 67.06%\n",
      "----------------------------\n",
      "[23] Loss Test: 1.288\n",
      "[23] Accuracy Test: 63.49%\n",
      "EPOCH [23] is completed!\n",
      "----------------------------\n",
      "[24,    10] Loss Train: 1.017\n",
      "[24,    10] Accuracy Train: 66.77%\n",
      "----------------------------\n",
      "[24,    20] Loss Train: 1.031\n",
      "[24,    20] Accuracy Train: 67.04%\n",
      "----------------------------\n",
      "[24,    30] Loss Train: 1.018\n",
      "[24,    30] Accuracy Train: 67.50%\n",
      "----------------------------\n",
      "[24,    40] Loss Train: 1.036\n",
      "[24,    40] Accuracy Train: 66.60%\n",
      "----------------------------\n",
      "[24,    50] Loss Train: 1.032\n",
      "[24,    50] Accuracy Train: 66.86%\n",
      "----------------------------\n",
      "[24,    60] Loss Train: 1.045\n",
      "[24,    60] Accuracy Train: 67.53%\n",
      "----------------------------\n",
      "[24,    70] Loss Train: 1.049\n",
      "[24,    70] Accuracy Train: 67.73%\n",
      "----------------------------\n",
      "[24,    80] Loss Train: 1.062\n",
      "[24,    80] Accuracy Train: 66.88%\n",
      "----------------------------\n",
      "[24,    90] Loss Train: 1.072\n",
      "[24,    90] Accuracy Train: 66.18%\n",
      "----------------------------\n",
      "[24] Loss Test: 1.306\n",
      "[24] Accuracy Test: 62.58%\n",
      "EPOCH [24] is completed!\n",
      "----------------------------\n",
      "[25,    10] Loss Train: 1.037\n",
      "[25,    10] Accuracy Train: 67.21%\n",
      "----------------------------\n",
      "[25,    20] Loss Train: 1.038\n",
      "[25,    20] Accuracy Train: 67.78%\n",
      "----------------------------\n",
      "[25,    30] Loss Train: 1.060\n",
      "[25,    30] Accuracy Train: 67.50%\n",
      "----------------------------\n",
      "[25,    40] Loss Train: 1.046\n",
      "[25,    40] Accuracy Train: 67.53%\n",
      "----------------------------\n",
      "[25,    50] Loss Train: 1.077\n",
      "[25,    50] Accuracy Train: 66.48%\n",
      "----------------------------\n",
      "[25,    60] Loss Train: 1.068\n",
      "[25,    60] Accuracy Train: 66.78%\n",
      "----------------------------\n",
      "[25,    70] Loss Train: 1.091\n",
      "[25,    70] Accuracy Train: 66.06%\n",
      "----------------------------\n",
      "[25,    80] Loss Train: 1.110\n",
      "[25,    80] Accuracy Train: 65.50%\n",
      "----------------------------\n",
      "[25,    90] Loss Train: 1.123\n",
      "[25,    90] Accuracy Train: 64.86%\n",
      "----------------------------\n",
      "[25] Loss Test: 1.341\n",
      "[25] Accuracy Test: 60.27%\n",
      "EPOCH [25] is completed!\n",
      "----------------------------\n",
      "[26,    10] Loss Train: 1.085\n",
      "[26,    10] Accuracy Train: 66.39%\n",
      "----------------------------\n",
      "[26,    20] Loss Train: 1.100\n",
      "[26,    20] Accuracy Train: 64.80%\n",
      "----------------------------\n",
      "[26,    30] Loss Train: 1.107\n",
      "[26,    30] Accuracy Train: 64.83%\n",
      "----------------------------\n",
      "[26,    40] Loss Train: 1.127\n",
      "[26,    40] Accuracy Train: 66.54%\n",
      "----------------------------\n",
      "[26,    50] Loss Train: 1.123\n",
      "[26,    50] Accuracy Train: 65.48%\n",
      "----------------------------\n",
      "[26,    60] Loss Train: 1.109\n",
      "[26,    60] Accuracy Train: 65.68%\n",
      "----------------------------\n",
      "[26,    70] Loss Train: 1.128\n",
      "[26,    70] Accuracy Train: 65.75%\n",
      "----------------------------\n",
      "[26,    80] Loss Train: 1.127\n",
      "[26,    80] Accuracy Train: 65.65%\n",
      "----------------------------\n",
      "[26,    90] Loss Train: 1.148\n",
      "[26,    90] Accuracy Train: 64.89%\n",
      "----------------------------\n",
      "[26] Loss Test: 1.352\n",
      "[26] Accuracy Test: 58.80%\n",
      "EPOCH [26] is completed!\n",
      "----------------------------\n",
      "[27,    10] Loss Train: 1.112\n",
      "[27,    10] Accuracy Train: 65.58%\n",
      "----------------------------\n",
      "[27,    20] Loss Train: 1.132\n",
      "[27,    20] Accuracy Train: 64.59%\n",
      "----------------------------\n",
      "[27,    30] Loss Train: 1.137\n",
      "[27,    30] Accuracy Train: 64.46%\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27,    40] Loss Train: 1.124\n",
      "[27,    40] Accuracy Train: 65.51%\n",
      "----------------------------\n",
      "[27,    50] Loss Train: 1.155\n",
      "[27,    50] Accuracy Train: 63.95%\n",
      "----------------------------\n",
      "[27,    60] Loss Train: 1.198\n",
      "[27,    60] Accuracy Train: 63.22%\n",
      "----------------------------\n",
      "[27,    70] Loss Train: 1.198\n",
      "[27,    70] Accuracy Train: 63.70%\n",
      "----------------------------\n",
      "[27,    80] Loss Train: 1.203\n",
      "[27,    80] Accuracy Train: 63.43%\n",
      "----------------------------\n",
      "[27,    90] Loss Train: 1.202\n",
      "[27,    90] Accuracy Train: 62.76%\n",
      "----------------------------\n",
      "[27] Loss Test: 1.392\n",
      "[27] Accuracy Test: 60.18%\n",
      "EPOCH [27] is completed!\n",
      "----------------------------\n",
      "[28,    10] Loss Train: 1.166\n",
      "[28,    10] Accuracy Train: 65.82%\n",
      "----------------------------\n",
      "[28,    20] Loss Train: 1.182\n",
      "[28,    20] Accuracy Train: 64.81%\n",
      "----------------------------\n",
      "[28,    30] Loss Train: 1.168\n",
      "[28,    30] Accuracy Train: 64.48%\n",
      "----------------------------\n",
      "[28,    40] Loss Train: 1.172\n",
      "[28,    40] Accuracy Train: 64.10%\n",
      "----------------------------\n",
      "[28,    50] Loss Train: 1.165\n",
      "[28,    50] Accuracy Train: 65.33%\n",
      "----------------------------\n",
      "[28,    60] Loss Train: 1.130\n",
      "[28,    60] Accuracy Train: 66.40%\n",
      "----------------------------\n",
      "[28,    70] Loss Train: 1.151\n",
      "[28,    70] Accuracy Train: 64.27%\n",
      "----------------------------\n",
      "[28,    80] Loss Train: 1.152\n",
      "[28,    80] Accuracy Train: 64.82%\n",
      "----------------------------\n",
      "[28,    90] Loss Train: 1.151\n",
      "[28,    90] Accuracy Train: 64.24%\n",
      "----------------------------\n",
      "[28] Loss Test: 1.312\n",
      "[28] Accuracy Test: 62.53%\n",
      "EPOCH [28] is completed!\n",
      "----------------------------\n",
      "[29,    10] Loss Train: 1.104\n",
      "[29,    10] Accuracy Train: 66.55%\n",
      "----------------------------\n",
      "[29,    20] Loss Train: 1.107\n",
      "[29,    20] Accuracy Train: 65.90%\n",
      "----------------------------\n",
      "[29,    30] Loss Train: 1.114\n",
      "[29,    30] Accuracy Train: 65.62%\n",
      "----------------------------\n",
      "[29,    40] Loss Train: 1.112\n",
      "[29,    40] Accuracy Train: 66.67%\n",
      "----------------------------\n",
      "[29,    50] Loss Train: 1.125\n",
      "[29,    50] Accuracy Train: 65.36%\n",
      "----------------------------\n",
      "[29,    60] Loss Train: 1.112\n",
      "[29,    60] Accuracy Train: 65.57%\n",
      "----------------------------\n",
      "[29,    70] Loss Train: 1.129\n",
      "[29,    70] Accuracy Train: 63.75%\n",
      "----------------------------\n",
      "[29,    80] Loss Train: 1.139\n",
      "[29,    80] Accuracy Train: 63.99%\n",
      "----------------------------\n",
      "[29,    90] Loss Train: 1.147\n",
      "[29,    90] Accuracy Train: 64.47%\n",
      "----------------------------\n",
      "[29] Loss Test: 1.325\n",
      "[29] Accuracy Test: 63.00%\n",
      "EPOCH [29] is completed!\n",
      "----------------------------\n",
      "[30,    10] Loss Train: 1.126\n",
      "[30,    10] Accuracy Train: 64.96%\n",
      "----------------------------\n",
      "[30,    20] Loss Train: 1.113\n",
      "[30,    20] Accuracy Train: 66.27%\n",
      "----------------------------\n",
      "[30,    30] Loss Train: 1.105\n",
      "[30,    30] Accuracy Train: 65.70%\n",
      "----------------------------\n",
      "[30,    40] Loss Train: 1.119\n",
      "[30,    40] Accuracy Train: 65.82%\n",
      "----------------------------\n",
      "[30,    50] Loss Train: 1.116\n",
      "[30,    50] Accuracy Train: 64.18%\n",
      "----------------------------\n",
      "[30,    60] Loss Train: 1.116\n",
      "[30,    60] Accuracy Train: 65.66%\n",
      "----------------------------\n",
      "[30,    70] Loss Train: 1.121\n",
      "[30,    70] Accuracy Train: 65.61%\n",
      "----------------------------\n",
      "[30,    80] Loss Train: 1.149\n",
      "[30,    80] Accuracy Train: 63.89%\n",
      "----------------------------\n",
      "[30,    90] Loss Train: 1.173\n",
      "[30,    90] Accuracy Train: 61.82%\n",
      "----------------------------\n",
      "[30] Loss Test: 1.440\n",
      "[30] Accuracy Test: 57.44%\n",
      "EPOCH [30] is completed!\n",
      "----------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = Model(input_size = 256,hidden_size = 512,output_size = len(dataset_train.vocab),batch_size = batch_size)\n",
    "\n",
    "EPOCH = 30\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    loss_train.append(train(model,dataloader_train,optimizer,epoch))\n",
    "    loss_test.append(test(model,dataloader_test))\n",
    "    \n",
    "\n",
    "\n",
    "#Visualization\n",
    "plt.plot(loss_train)\n",
    "plt.plot(loss_test)\n",
    "plt.rcParams[\"figure.figsize\"]=50,50\n",
    "plt.rcParams.update({'font.size': 35})\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.xticks(range(1, EPOCH + 1))\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c5d5e45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T14:44:00.946688Z",
     "start_time": "2021-10-15T14:44:00.820672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apprentice and the pres\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Text generation\n",
    "\n",
    "def generate_text(model, input_string ,num_char = 50,top_k = 1):\n",
    "    num_string = np.array([dataset_test.to_int(x) for x in input_string.lower()])\n",
    "    tensor_string = torch.from_numpy(num_string).unsqueeze(0)\n",
    "    model.eval()\n",
    "    generated_text = []\n",
    "    \n",
    "    model.reset_hidden(1)\n",
    "        \n",
    "    for i in range(num_char):\n",
    "        probs = model.predict(tensor_string)\n",
    "        probs = probs.squeeze(0)\n",
    "        \n",
    "        mult_distr = Multinomial(total_count = 50,probs = probs)\n",
    "        mult_distr_sample = mult_distr.sample()\n",
    "        \n",
    "        indices = torch.topk(mult_distr_sample,k = top_k).indices #selects top_k classes with highest probability\n",
    "        rd = np.random.randint(low = 0,high = top_k, size = 1)#select random class from top probabilities\n",
    "        idx = indices[rd].unsqueeze(1)\n",
    "        \n",
    "        tensor_string = torch.cat((tensor_string,idx),dim = 1)        \n",
    "        generated_text.append(dataset_test.to_char(indices[rd].numpy()[0]))\n",
    "        \n",
    "    return (input_string + ''.join(generated_text))\n",
    "\n",
    "print(generate_text(model=model,input_string=\"app\",num_char=1,top_k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c1414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dataglove] *",
   "language": "python",
   "name": "conda-env-dataglove-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
