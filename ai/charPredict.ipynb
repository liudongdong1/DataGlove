{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "007f81e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T02:35:05.004771Z",
     "start_time": "2021-06-02T02:35:04.275027Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6baf5af7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T02:35:06.552946Z",
     "start_time": "2021-06-02T02:35:06.226262Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/home/iot/jupyter/root_dir/liudongdong/dataset/charprediction/train.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ede937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T02:35:18.266457Z",
     "start_time": "2021-06-02T02:35:18.259443Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "808bf6b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T02:35:19.838908Z",
     "start_time": "2021-06-02T02:35:19.827989Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    ## TODO: Get the number of batches we can make\n",
    "    n_batches = (len(arr))//(batch_size*seq_length)\n",
    "    \n",
    "    ## TODO: Keep only enough characters to make full batches\n",
    "    arr = arr[:(n_batches*batch_size*seq_length)]\n",
    "    \n",
    "    ## TODO: Reshape into batch_size rows\n",
    "    size=(batch_size,-1)\n",
    "    arr = arr.reshape(size)  #(batch, columns)  后续数据直接在 columns 遍历\n",
    "    \n",
    "    ## TODO: Iterate over the batches using a window of size seq_length\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "          y[:, :-1],y[:, -1]=x[:,1:], arr[:,n+seq_length]\n",
    "        except IndexError:\n",
    "          y[:, :-1],y[:, -1]=x[:,1:], arr[:,0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a84b56f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T02:35:22.149137Z",
     "start_time": "2021-06-02T02:35:22.108702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "acdf79b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T03:12:58.659965Z",
     "start_time": "2021-06-02T03:12:58.644133Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the layers of the model\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)   #注意这里\n",
    "        \n",
    "        self.dropout=nn.Dropout(drop_prob)\n",
    "\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        #x=rearrange(x,'b s d-> s b d')\n",
    "        r_output,hidden=self.lstm(x,hidden)\n",
    "        #r_output=rearrange(r_output,'s b d-> b s d')\n",
    "        out=self.dropout(r_output)\n",
    "        #print(\"self.dropout\",out.shape)\n",
    "        out=out.contiguous().view(-1,self.n_hidden)\n",
    "        #print(\"self.contiguous\",out.shape)\n",
    "        out=self.fc(out)\n",
    "# self.dropout torch.Size([128, 100, 512])\n",
    "# self.contiguous torch.Size([12800, 512])\n",
    "# output, torch.Size([12800, 94])\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c8bafc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T03:12:59.292408Z",
     "start_time": "2021-06-02T03:12:59.255779Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(94, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=94, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## TODO: set you model hyperparameters\n",
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "00a3f981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T03:39:44.036378Z",
     "start_time": "2021-06-02T03:39:44.010973Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "#     scheduler=ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            #print(\"inputs.shape,targets.shape\",x.shape,y.shape,[int2char[ch] for ch in x[0]])\n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            #print(\"inputs.shape,targets.shape\",inputs.shape,targets.shape)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            #print(\"output,\",output.shape)\n",
    "    \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                accuracy_train=0\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    #print(output.shape)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                    pre_train=F.softmax(np.reshape(output.cpu().detach(),(batch_size,seq_length,-1)),dim=2)\n",
    "                    #print(\"inputs.shape,targets.shape\",pre_train[0],targets.shape)\n",
    "                    pre_train=torch.argmax(pre_train,dim = 2)\n",
    "                    #print(\"inputs.shape,targets.shape\",pre_train[0],targets.shape)\n",
    "        \n",
    "                    \n",
    "                    #print(\"inputs.shape,targets.shape\",pre_train.shape,targets.shape)\n",
    "                    targets_train=targets.cpu().detach()\n",
    "                    accuracy_train = torch.sum(pre_train == targets_train)/(targets_train.shape[0] * targets_train.shape[1])\n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f} acc:{}\".format(np.mean(val_losses),accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1da1e914",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T04:08:43.732544Z",
     "start_time": "2021-06-02T03:40:09.516739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100... Step: 10... Loss: 1.5137... Val Loss: 1.4837 acc:0.5511718988418579\n",
      "Epoch: 1/100... Step: 20... Loss: 1.5372... Val Loss: 1.4763 acc:0.5548437237739563\n",
      "Epoch: 1/100... Step: 30... Loss: 1.4863... Val Loss: 1.4723 acc:0.5546093583106995\n",
      "Epoch: 1/100... Step: 40... Loss: 1.4652... Val Loss: 1.4681 acc:0.5569531321525574\n",
      "Epoch: 1/100... Step: 50... Loss: 1.5087... Val Loss: 1.4653 acc:0.5575781464576721\n",
      "Epoch: 1/100... Step: 60... Loss: 1.4948... Val Loss: 1.4610 acc:0.5596093535423279\n",
      "Epoch: 1/100... Step: 70... Loss: 1.4860... Val Loss: 1.4586 acc:0.5614843964576721\n",
      "Epoch: 1/100... Step: 80... Loss: 1.4970... Val Loss: 1.4597 acc:0.5596874952316284\n",
      "Epoch: 1/100... Step: 90... Loss: 1.4899... Val Loss: 1.4542 acc:0.5625781416893005\n",
      "Epoch: 1/100... Step: 100... Loss: 1.4754... Val Loss: 1.4546 acc:0.5637500286102295\n",
      "Epoch: 1/100... Step: 110... Loss: 1.4868... Val Loss: 1.4503 acc:0.5625\n",
      "Epoch: 1/100... Step: 120... Loss: 1.4911... Val Loss: 1.4478 acc:0.5608593821525574\n",
      "Epoch: 1/100... Step: 130... Loss: 1.4742... Val Loss: 1.4469 acc:0.5628125071525574\n",
      "Epoch: 1/100... Step: 140... Loss: 1.5159... Val Loss: 1.4426 acc:0.5653906464576721\n",
      "Epoch: 1/100... Step: 150... Loss: 1.5196... Val Loss: 1.4423 acc:0.5634375214576721\n",
      "Epoch: 1/100... Step: 160... Loss: 1.5018... Val Loss: 1.4379 acc:0.5642969012260437\n",
      "Epoch: 1/100... Step: 170... Loss: 1.4891... Val Loss: 1.4368 acc:0.5655468702316284\n",
      "Epoch: 1/100... Step: 180... Loss: 1.4857... Val Loss: 1.4367 acc:0.5663281083106995\n",
      "Epoch: 1/100... Step: 190... Loss: 1.5167... Val Loss: 1.4321 acc:0.5667187571525574\n",
      "Epoch: 1/100... Step: 200... Loss: 1.5225... Val Loss: 1.4300 acc:0.5674999952316284\n",
      "Epoch: 2/100... Step: 210... Loss: 1.4581... Val Loss: 1.4289 acc:0.5699999928474426\n",
      "Epoch: 2/100... Step: 220... Loss: 1.4842... Val Loss: 1.4271 acc:0.567187488079071\n",
      "Epoch: 2/100... Step: 230... Loss: 1.4408... Val Loss: 1.4234 acc:0.5684375166893005\n",
      "Epoch: 2/100... Step: 240... Loss: 1.4186... Val Loss: 1.4237 acc:0.5700781345367432\n",
      "Epoch: 2/100... Step: 250... Loss: 1.4668... Val Loss: 1.4204 acc:0.5693749785423279\n",
      "Epoch: 2/100... Step: 260... Loss: 1.4490... Val Loss: 1.4175 acc:0.569531261920929\n",
      "Epoch: 2/100... Step: 270... Loss: 1.4453... Val Loss: 1.4173 acc:0.5701562762260437\n",
      "Epoch: 2/100... Step: 280... Loss: 1.4507... Val Loss: 1.4147 acc:0.5699999928474426\n",
      "Epoch: 2/100... Step: 290... Loss: 1.4417... Val Loss: 1.4121 acc:0.5724999904632568\n",
      "Epoch: 2/100... Step: 300... Loss: 1.4294... Val Loss: 1.4116 acc:0.5726562738418579\n",
      "Epoch: 2/100... Step: 310... Loss: 1.4412... Val Loss: 1.4110 acc:0.571093738079071\n",
      "Epoch: 2/100... Step: 320... Loss: 1.4412... Val Loss: 1.4090 acc:0.5746874809265137\n",
      "Epoch: 2/100... Step: 330... Loss: 1.4272... Val Loss: 1.4074 acc:0.5746093988418579\n",
      "Epoch: 2/100... Step: 340... Loss: 1.4697... Val Loss: 1.4050 acc:0.5760937333106995\n",
      "Epoch: 2/100... Step: 350... Loss: 1.4747... Val Loss: 1.4041 acc:0.5735937356948853\n",
      "Epoch: 2/100... Step: 360... Loss: 1.4500... Val Loss: 1.4026 acc:0.575390636920929\n",
      "Epoch: 2/100... Step: 370... Loss: 1.4442... Val Loss: 1.4028 acc:0.573437511920929\n",
      "Epoch: 2/100... Step: 380... Loss: 1.4396... Val Loss: 1.3998 acc:0.5764062404632568\n",
      "Epoch: 2/100... Step: 390... Loss: 1.4735... Val Loss: 1.3965 acc:0.5754687786102295\n",
      "Epoch: 2/100... Step: 400... Loss: 1.4865... Val Loss: 1.3940 acc:0.5766406059265137\n",
      "Epoch: 3/100... Step: 410... Loss: 1.4180... Val Loss: 1.3936 acc:0.5778906345367432\n",
      "Epoch: 3/100... Step: 420... Loss: 1.4431... Val Loss: 1.3905 acc:0.5774999856948853\n",
      "Epoch: 3/100... Step: 430... Loss: 1.4052... Val Loss: 1.3910 acc:0.5751562714576721\n",
      "Epoch: 3/100... Step: 440... Loss: 1.3700... Val Loss: 1.3887 acc:0.5768749713897705\n",
      "Epoch: 3/100... Step: 450... Loss: 1.4182... Val Loss: 1.3901 acc:0.5794531106948853\n",
      "Epoch: 3/100... Step: 460... Loss: 1.4118... Val Loss: 1.3854 acc:0.576953113079071\n",
      "Epoch: 3/100... Step: 470... Loss: 1.4069... Val Loss: 1.3844 acc:0.578125\n",
      "Epoch: 3/100... Step: 480... Loss: 1.4093... Val Loss: 1.3827 acc:0.5782812237739563\n",
      "Epoch: 3/100... Step: 490... Loss: 1.4065... Val Loss: 1.3798 acc:0.5832812786102295\n",
      "Epoch: 3/100... Step: 500... Loss: 1.3814... Val Loss: 1.3832 acc:0.5772656202316284\n",
      "Epoch: 3/100... Step: 510... Loss: 1.4065... Val Loss: 1.3785 acc:0.5775781273841858\n",
      "Epoch: 3/100... Step: 520... Loss: 1.4010... Val Loss: 1.3777 acc:0.5789843797683716\n",
      "Epoch: 3/100... Step: 530... Loss: 1.3940... Val Loss: 1.3771 acc:0.5801562666893005\n",
      "Epoch: 3/100... Step: 540... Loss: 1.4315... Val Loss: 1.3749 acc:0.58203125\n",
      "Epoch: 3/100... Step: 550... Loss: 1.4318... Val Loss: 1.3747 acc:0.5810937285423279\n",
      "Epoch: 3/100... Step: 560... Loss: 1.4214... Val Loss: 1.3740 acc:0.5807812213897705\n",
      "Epoch: 3/100... Step: 570... Loss: 1.4085... Val Loss: 1.3728 acc:0.5807812213897705\n",
      "Epoch: 3/100... Step: 580... Loss: 1.4061... Val Loss: 1.3743 acc:0.5827343463897705\n",
      "Epoch: 3/100... Step: 590... Loss: 1.4319... Val Loss: 1.3688 acc:0.5830468535423279\n",
      "Epoch: 3/100... Step: 600... Loss: 1.4459... Val Loss: 1.3668 acc:0.5859375\n",
      "Epoch: 4/100... Step: 610... Loss: 1.3973... Val Loss: 1.3656 acc:0.5853906273841858\n",
      "Epoch: 4/100... Step: 620... Loss: 1.4078... Val Loss: 1.3647 acc:0.5849218964576721\n",
      "Epoch: 4/100... Step: 630... Loss: 1.3730... Val Loss: 1.3626 acc:0.5824999809265137\n",
      "Epoch: 4/100... Step: 640... Loss: 1.3507... Val Loss: 1.3619 acc:0.5845312476158142\n",
      "Epoch: 4/100... Step: 650... Loss: 1.3818... Val Loss: 1.3633 acc:0.5865625143051147\n",
      "Epoch: 4/100... Step: 660... Loss: 1.3796... Val Loss: 1.3603 acc:0.5850781202316284\n",
      "Epoch: 4/100... Step: 670... Loss: 1.3887... Val Loss: 1.3619 acc:0.5844531059265137\n",
      "Epoch: 4/100... Step: 680... Loss: 1.3817... Val Loss: 1.3567 acc:0.587890625\n",
      "Epoch: 4/100... Step: 690... Loss: 1.3688... Val Loss: 1.3587 acc:0.5894531011581421\n",
      "Epoch: 4/100... Step: 700... Loss: 1.3523... Val Loss: 1.3554 acc:0.5866405963897705\n",
      "Epoch: 4/100... Step: 710... Loss: 1.3717... Val Loss: 1.3553 acc:0.5861718654632568\n",
      "Epoch: 4/100... Step: 720... Loss: 1.3701... Val Loss: 1.3541 acc:0.5895312428474426\n",
      "Epoch: 4/100... Step: 730... Loss: 1.3607... Val Loss: 1.3523 acc:0.5871875286102295\n",
      "Epoch: 4/100... Step: 740... Loss: 1.3938... Val Loss: 1.3505 acc:0.5872656106948853\n",
      "Epoch: 4/100... Step: 750... Loss: 1.4062... Val Loss: 1.3505 acc:0.5881249904632568\n",
      "Epoch: 4/100... Step: 760... Loss: 1.3796... Val Loss: 1.3508 acc:0.5883593559265137\n",
      "Epoch: 4/100... Step: 770... Loss: 1.3769... Val Loss: 1.3501 acc:0.5879687666893005\n",
      "Epoch: 4/100... Step: 780... Loss: 1.3759... Val Loss: 1.3526 acc:0.5892968773841858\n",
      "Epoch: 4/100... Step: 790... Loss: 1.4035... Val Loss: 1.3489 acc:0.5888281464576721\n",
      "Epoch: 4/100... Step: 800... Loss: 1.4164... Val Loss: 1.3437 acc:0.5914062261581421\n",
      "Epoch: 5/100... Step: 810... Loss: 1.3635... Val Loss: 1.3477 acc:0.5909374952316284\n",
      "Epoch: 5/100... Step: 820... Loss: 1.3762... Val Loss: 1.3407 acc:0.5907812714576721\n",
      "Epoch: 5/100... Step: 830... Loss: 1.3427... Val Loss: 1.3434 acc:0.5900781154632568\n",
      "Epoch: 5/100... Step: 840... Loss: 1.3237... Val Loss: 1.3416 acc:0.5910937786102295\n",
      "Epoch: 5/100... Step: 850... Loss: 1.3479... Val Loss: 1.3412 acc:0.5927343964576721\n",
      "Epoch: 5/100... Step: 860... Loss: 1.3612... Val Loss: 1.3402 acc:0.5879687666893005\n",
      "Epoch: 5/100... Step: 870... Loss: 1.3501... Val Loss: 1.3396 acc:0.5911718606948853\n",
      "Epoch: 5/100... Step: 880... Loss: 1.3619... Val Loss: 1.3374 acc:0.5939062237739563\n",
      "Epoch: 5/100... Step: 890... Loss: 1.3340... Val Loss: 1.3373 acc:0.5932812690734863\n",
      "Epoch: 5/100... Step: 900... Loss: 1.3301... Val Loss: 1.3376 acc:0.5921093821525574\n",
      "Epoch: 5/100... Step: 910... Loss: 1.3481... Val Loss: 1.3358 acc:0.5920312404632568\n",
      "Epoch: 5/100... Step: 920... Loss: 1.3450... Val Loss: 1.3347 acc:0.5936718583106995\n",
      "Epoch: 5/100... Step: 930... Loss: 1.3334... Val Loss: 1.3325 acc:0.5946875214576721\n",
      "Epoch: 5/100... Step: 940... Loss: 1.3686... Val Loss: 1.3317 acc:0.5942968726158142\n",
      "Epoch: 5/100... Step: 950... Loss: 1.3738... Val Loss: 1.3314 acc:0.592968761920929\n",
      "Epoch: 5/100... Step: 960... Loss: 1.3609... Val Loss: 1.3322 acc:0.5956249833106995\n",
      "Epoch: 5/100... Step: 970... Loss: 1.3567... Val Loss: 1.3365 acc:0.5916406512260437\n",
      "Epoch: 5/100... Step: 980... Loss: 1.3463... Val Loss: 1.3330 acc:0.5951562523841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100... Step: 990... Loss: 1.3812... Val Loss: 1.3289 acc:0.5948437452316284\n",
      "Epoch: 5/100... Step: 1000... Loss: 1.4033... Val Loss: 1.3273 acc:0.5956249833106995\n",
      "Epoch: 6/100... Step: 1010... Loss: 1.3398... Val Loss: 1.3288 acc:0.5960937738418579\n",
      "Epoch: 6/100... Step: 1020... Loss: 1.3531... Val Loss: 1.3252 acc:0.5955469012260437\n",
      "Epoch: 6/100... Step: 1030... Loss: 1.3147... Val Loss: 1.3259 acc:0.5941406488418579\n",
      "Epoch: 6/100... Step: 1040... Loss: 1.2861... Val Loss: 1.3262 acc:0.596484363079071\n",
      "Epoch: 6/100... Step: 1050... Loss: 1.3324... Val Loss: 1.3235 acc:0.5992968678474426\n",
      "Epoch: 6/100... Step: 1060... Loss: 1.3257... Val Loss: 1.3219 acc:0.5950781106948853\n",
      "Epoch: 6/100... Step: 1070... Loss: 1.3229... Val Loss: 1.3228 acc:0.594921886920929\n",
      "Epoch: 6/100... Step: 1080... Loss: 1.3293... Val Loss: 1.3202 acc:0.599609375\n",
      "Epoch: 6/100... Step: 1090... Loss: 1.3138... Val Loss: 1.3213 acc:0.5986718535423279\n",
      "Epoch: 6/100... Step: 1100... Loss: 1.3092... Val Loss: 1.3214 acc:0.5958593487739563\n",
      "Epoch: 6/100... Step: 1110... Loss: 1.3229... Val Loss: 1.3195 acc:0.5958593487739563\n",
      "Epoch: 6/100... Step: 1120... Loss: 1.3226... Val Loss: 1.3204 acc:0.5985937714576721\n",
      "Epoch: 6/100... Step: 1130... Loss: 1.3134... Val Loss: 1.3194 acc:0.599609375\n",
      "Epoch: 6/100... Step: 1140... Loss: 1.3559... Val Loss: 1.3171 acc:0.5993750095367432\n",
      "Epoch: 6/100... Step: 1150... Loss: 1.3450... Val Loss: 1.3166 acc:0.5981249809265137\n",
      "Epoch: 6/100... Step: 1160... Loss: 1.3289... Val Loss: 1.3158 acc:0.5989062786102295\n",
      "Epoch: 6/100... Step: 1170... Loss: 1.3283... Val Loss: 1.3173 acc:0.5971875190734863\n",
      "Epoch: 6/100... Step: 1180... Loss: 1.3281... Val Loss: 1.3163 acc:0.5992187261581421\n",
      "Epoch: 6/100... Step: 1190... Loss: 1.3555... Val Loss: 1.3147 acc:0.598828136920929\n",
      "Epoch: 6/100... Step: 1200... Loss: 1.3837... Val Loss: 1.3122 acc:0.5987499952316284\n",
      "Epoch: 7/100... Step: 1210... Loss: 1.3255... Val Loss: 1.3168 acc:0.5985937714576721\n",
      "Epoch: 7/100... Step: 1220... Loss: 1.3351... Val Loss: 1.3089 acc:0.6000000238418579\n",
      "Epoch: 7/100... Step: 1230... Loss: 1.3015... Val Loss: 1.3120 acc:0.5993750095367432\n",
      "Epoch: 7/100... Step: 1240... Loss: 1.2745... Val Loss: 1.3136 acc:0.5997655987739563\n",
      "Epoch: 7/100... Step: 1250... Loss: 1.3019... Val Loss: 1.3101 acc:0.6015625\n",
      "Epoch: 7/100... Step: 1260... Loss: 1.3066... Val Loss: 1.3090 acc:0.5986718535423279\n",
      "Epoch: 7/100... Step: 1270... Loss: 1.3006... Val Loss: 1.3089 acc:0.6001562476158142\n",
      "Epoch: 7/100... Step: 1280... Loss: 1.3136... Val Loss: 1.3087 acc:0.6017968654632568\n",
      "Epoch: 7/100... Step: 1290... Loss: 1.2979... Val Loss: 1.3075 acc:0.6011718511581421\n",
      "Epoch: 7/100... Step: 1300... Loss: 1.2909... Val Loss: 1.3072 acc:0.5993750095367432\n",
      "Epoch: 7/100... Step: 1310... Loss: 1.3069... Val Loss: 1.3061 acc:0.6000000238418579\n",
      "Epoch: 7/100... Step: 1320... Loss: 1.3030... Val Loss: 1.3066 acc:0.6014062762260437\n",
      "Epoch: 7/100... Step: 1330... Loss: 1.2962... Val Loss: 1.3060 acc:0.6008594036102295\n",
      "Epoch: 7/100... Step: 1340... Loss: 1.3286... Val Loss: 1.3044 acc:0.6025781035423279\n",
      "Epoch: 7/100... Step: 1350... Loss: 1.3283... Val Loss: 1.3039 acc:0.6011718511581421\n",
      "Epoch: 7/100... Step: 1360... Loss: 1.3159... Val Loss: 1.3037 acc:0.6017968654632568\n",
      "Epoch: 7/100... Step: 1370... Loss: 1.3055... Val Loss: 1.3040 acc:0.6017187237739563\n",
      "Epoch: 7/100... Step: 1380... Loss: 1.3006... Val Loss: 1.3032 acc:0.6021875143051147\n",
      "Epoch: 7/100... Step: 1390... Loss: 1.3400... Val Loss: 1.3010 acc:0.6017968654632568\n",
      "Epoch: 7/100... Step: 1400... Loss: 1.3671... Val Loss: 1.2993 acc:0.6037499904632568\n",
      "Epoch: 8/100... Step: 1410... Loss: 1.3076... Val Loss: 1.3002 acc:0.6055468916893005\n",
      "Epoch: 8/100... Step: 1420... Loss: 1.3165... Val Loss: 1.2976 acc:0.6043750047683716\n",
      "Epoch: 8/100... Step: 1430... Loss: 1.2781... Val Loss: 1.2984 acc:0.6044531464576721\n",
      "Epoch: 8/100... Step: 1440... Loss: 1.2645... Val Loss: 1.2991 acc:0.6039843559265137\n",
      "Epoch: 8/100... Step: 1450... Loss: 1.2883... Val Loss: 1.3005 acc:0.60546875\n",
      "Epoch: 8/100... Step: 1460... Loss: 1.3027... Val Loss: 1.2981 acc:0.6041406393051147\n",
      "Epoch: 8/100... Step: 1470... Loss: 1.2797... Val Loss: 1.2946 acc:0.6055468916893005\n",
      "Epoch: 8/100... Step: 1480... Loss: 1.3013... Val Loss: 1.2966 acc:0.6067187786102295\n",
      "Epoch: 8/100... Step: 1490... Loss: 1.2751... Val Loss: 1.2933 acc:0.6060156226158142\n",
      "Epoch: 8/100... Step: 1500... Loss: 1.2682... Val Loss: 1.2941 acc:0.604687511920929\n",
      "Epoch: 8/100... Step: 1510... Loss: 1.2932... Val Loss: 1.2926 acc:0.604687511920929\n",
      "Epoch: 8/100... Step: 1520... Loss: 1.2906... Val Loss: 1.2928 acc:0.6071875095367432\n",
      "Epoch: 8/100... Step: 1530... Loss: 1.2754... Val Loss: 1.2935 acc:0.6053906083106995\n",
      "Epoch: 8/100... Step: 1540... Loss: 1.3129... Val Loss: 1.2952 acc:0.6050000190734863\n",
      "Epoch: 8/100... Step: 1550... Loss: 1.3139... Val Loss: 1.2911 acc:0.6052343845367432\n",
      "Epoch: 8/100... Step: 1560... Loss: 1.2930... Val Loss: 1.2922 acc:0.6072656512260437\n",
      "Epoch: 8/100... Step: 1570... Loss: 1.2851... Val Loss: 1.2937 acc:0.6051562428474426\n",
      "Epoch: 8/100... Step: 1580... Loss: 1.2871... Val Loss: 1.2907 acc:0.6078125238418579\n",
      "Epoch: 8/100... Step: 1590... Loss: 1.3199... Val Loss: 1.2897 acc:0.6053906083106995\n",
      "Epoch: 8/100... Step: 1600... Loss: 1.3421... Val Loss: 1.2890 acc:0.606640636920929\n",
      "Epoch: 9/100... Step: 1610... Loss: 1.2911... Val Loss: 1.2903 acc:0.6058593988418579\n",
      "Epoch: 9/100... Step: 1620... Loss: 1.2955... Val Loss: 1.2888 acc:0.60546875\n",
      "Epoch: 9/100... Step: 1630... Loss: 1.2598... Val Loss: 1.2890 acc:0.6064843535423279\n",
      "Epoch: 9/100... Step: 1640... Loss: 1.2489... Val Loss: 1.2883 acc:0.6071093678474426\n",
      "Epoch: 9/100... Step: 1650... Loss: 1.2689... Val Loss: 1.2876 acc:0.6056249737739563\n",
      "Epoch: 9/100... Step: 1660... Loss: 1.2795... Val Loss: 1.2859 acc:0.6046093702316284\n",
      "Epoch: 9/100... Step: 1670... Loss: 1.2687... Val Loss: 1.2838 acc:0.6107812523841858\n",
      "Epoch: 9/100... Step: 1680... Loss: 1.2813... Val Loss: 1.2837 acc:0.6084374785423279\n",
      "Epoch: 9/100... Step: 1690... Loss: 1.2617... Val Loss: 1.2822 acc:0.6081249713897705\n",
      "Epoch: 9/100... Step: 1700... Loss: 1.2615... Val Loss: 1.2849 acc:0.6075780987739563\n",
      "Epoch: 9/100... Step: 1710... Loss: 1.2665... Val Loss: 1.2825 acc:0.6075780987739563\n",
      "Epoch: 9/100... Step: 1720... Loss: 1.2578... Val Loss: 1.2806 acc:0.6092187762260437\n",
      "Epoch: 9/100... Step: 1730... Loss: 1.2655... Val Loss: 1.2817 acc:0.6103125214576721\n",
      "Epoch: 9/100... Step: 1740... Loss: 1.2964... Val Loss: 1.2809 acc:0.6106250286102295\n",
      "Epoch: 9/100... Step: 1750... Loss: 1.2987... Val Loss: 1.2799 acc:0.6083593964576721\n",
      "Epoch: 9/100... Step: 1760... Loss: 1.2795... Val Loss: 1.2782 acc:0.6097656488418579\n",
      "Epoch: 9/100... Step: 1770... Loss: 1.2873... Val Loss: 1.2798 acc:0.6092187762260437\n",
      "Epoch: 9/100... Step: 1780... Loss: 1.2773... Val Loss: 1.2800 acc:0.6097656488418579\n",
      "Epoch: 9/100... Step: 1790... Loss: 1.3075... Val Loss: 1.2769 acc:0.6068750023841858\n",
      "Epoch: 9/100... Step: 1800... Loss: 1.3330... Val Loss: 1.2780 acc:0.6085156202316284\n",
      "Epoch: 10/100... Step: 1810... Loss: 1.2693... Val Loss: 1.2801 acc:0.611328125\n",
      "Epoch: 10/100... Step: 1820... Loss: 1.2847... Val Loss: 1.2792 acc:0.6122656464576721\n",
      "Epoch: 10/100... Step: 1830... Loss: 1.2563... Val Loss: 1.2759 acc:0.6108593940734863\n",
      "Epoch: 10/100... Step: 1840... Loss: 1.2255... Val Loss: 1.2781 acc:0.6132031083106995\n",
      "Epoch: 10/100... Step: 1850... Loss: 1.2614... Val Loss: 1.2778 acc:0.6110937595367432\n",
      "Epoch: 10/100... Step: 1860... Loss: 1.2590... Val Loss: 1.2738 acc:0.6112499833106995\n",
      "Epoch: 10/100... Step: 1870... Loss: 1.2491... Val Loss: 1.2767 acc:0.6110156178474426\n",
      "Epoch: 10/100... Step: 1880... Loss: 1.2614... Val Loss: 1.2725 acc:0.612109363079071\n",
      "Epoch: 10/100... Step: 1890... Loss: 1.2471... Val Loss: 1.2710 acc:0.6117187738418579\n",
      "Epoch: 10/100... Step: 1900... Loss: 1.2390... Val Loss: 1.2740 acc:0.6110156178474426\n",
      "Epoch: 10/100... Step: 1910... Loss: 1.2603... Val Loss: 1.2723 acc:0.6120312213897705\n",
      "Epoch: 10/100... Step: 1920... Loss: 1.2479... Val Loss: 1.2733 acc:0.6100000143051147\n",
      "Epoch: 10/100... Step: 1930... Loss: 1.2489... Val Loss: 1.2723 acc:0.6133593916893005\n",
      "Epoch: 10/100... Step: 1940... Loss: 1.2808... Val Loss: 1.2725 acc:0.6107031106948853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100... Step: 1950... Loss: 1.2829... Val Loss: 1.2703 acc:0.6094531416893005\n",
      "Epoch: 10/100... Step: 1960... Loss: 1.2591... Val Loss: 1.2714 acc:0.6100780963897705\n",
      "Epoch: 10/100... Step: 1970... Loss: 1.2666... Val Loss: 1.2731 acc:0.6104687452316284\n",
      "Epoch: 10/100... Step: 1980... Loss: 1.2534... Val Loss: 1.2693 acc:0.6111719012260437\n",
      "Epoch: 10/100... Step: 1990... Loss: 1.2917... Val Loss: 1.2677 acc:0.610546886920929\n",
      "Epoch: 10/100... Step: 2000... Loss: 1.3183... Val Loss: 1.2669 acc:0.6103125214576721\n",
      "Epoch: 11/100... Step: 2010... Loss: 1.2534... Val Loss: 1.2689 acc:0.6100000143051147\n",
      "Epoch: 11/100... Step: 2020... Loss: 1.2606... Val Loss: 1.2688 acc:0.6115624904632568\n",
      "Epoch: 11/100... Step: 2030... Loss: 1.2401... Val Loss: 1.2652 acc:0.6121875047683716\n",
      "Epoch: 11/100... Step: 2040... Loss: 1.2184... Val Loss: 1.2654 acc:0.61328125\n",
      "Epoch: 11/100... Step: 2050... Loss: 1.2563... Val Loss: 1.2670 acc:0.6145312786102295\n",
      "Epoch: 11/100... Step: 2060... Loss: 1.2489... Val Loss: 1.2643 acc:0.6125781536102295\n",
      "Epoch: 11/100... Step: 2070... Loss: 1.2388... Val Loss: 1.2636 acc:0.6122656464576721\n",
      "Epoch: 11/100... Step: 2080... Loss: 1.2537... Val Loss: 1.2685 acc:0.6141406297683716\n",
      "Epoch: 11/100... Step: 2090... Loss: 1.2390... Val Loss: 1.2639 acc:0.612500011920929\n",
      "Epoch: 11/100... Step: 2100... Loss: 1.2332... Val Loss: 1.2647 acc:0.6121875047683716\n",
      "Epoch: 11/100... Step: 2110... Loss: 1.2526... Val Loss: 1.2648 acc:0.6135156154632568\n",
      "Epoch: 11/100... Step: 2120... Loss: 1.2353... Val Loss: 1.2605 acc:0.6122656464576721\n",
      "Epoch: 11/100... Step: 2130... Loss: 1.2282... Val Loss: 1.2613 acc:0.612109363079071\n",
      "Epoch: 11/100... Step: 2140... Loss: 1.2633... Val Loss: 1.2645 acc:0.6135937571525574\n",
      "Epoch: 11/100... Step: 2150... Loss: 1.2617... Val Loss: 1.2619 acc:0.6082812547683716\n",
      "Epoch: 11/100... Step: 2160... Loss: 1.2519... Val Loss: 1.2613 acc:0.6103125214576721\n",
      "Epoch: 11/100... Step: 2170... Loss: 1.2462... Val Loss: 1.2677 acc:0.6106250286102295\n",
      "Epoch: 11/100... Step: 2180... Loss: 1.2486... Val Loss: 1.2620 acc:0.6148437261581421\n",
      "Epoch: 11/100... Step: 2190... Loss: 1.2799... Val Loss: 1.2593 acc:0.6124218702316284\n",
      "Epoch: 11/100... Step: 2200... Loss: 1.3091... Val Loss: 1.2588 acc:0.6122656464576721\n",
      "Epoch: 12/100... Step: 2210... Loss: 1.2499... Val Loss: 1.2628 acc:0.6118749976158142\n",
      "Epoch: 12/100... Step: 2220... Loss: 1.2519... Val Loss: 1.2593 acc:0.6120312213897705\n",
      "Epoch: 12/100... Step: 2230... Loss: 1.2255... Val Loss: 1.2564 acc:0.6117187738418579\n",
      "Epoch: 12/100... Step: 2240... Loss: 1.1994... Val Loss: 1.2579 acc:0.6147656440734863\n",
      "Epoch: 12/100... Step: 2250... Loss: 1.2321... Val Loss: 1.2617 acc:0.6146093606948853\n",
      "Epoch: 12/100... Step: 2260... Loss: 1.2469... Val Loss: 1.2584 acc:0.6153125166893005\n",
      "Epoch: 12/100... Step: 2270... Loss: 1.2264... Val Loss: 1.2583 acc:0.6162499785423279\n",
      "Epoch: 12/100... Step: 2280... Loss: 1.2362... Val Loss: 1.2617 acc:0.6151562333106995\n",
      "Epoch: 12/100... Step: 2290... Loss: 1.2246... Val Loss: 1.2543 acc:0.6155468821525574\n",
      "Epoch: 12/100... Step: 2300... Loss: 1.2202... Val Loss: 1.2565 acc:0.6158593893051147\n",
      "Epoch: 12/100... Step: 2310... Loss: 1.2395... Val Loss: 1.2571 acc:0.6153905987739563\n",
      "Epoch: 12/100... Step: 2320... Loss: 1.2165... Val Loss: 1.2560 acc:0.616015613079071\n",
      "Epoch: 12/100... Step: 2330... Loss: 1.2155... Val Loss: 1.2537 acc:0.6156250238418579\n",
      "Epoch: 12/100... Step: 2340... Loss: 1.2590... Val Loss: 1.2550 acc:0.615234375\n",
      "Epoch: 12/100... Step: 2350... Loss: 1.2478... Val Loss: 1.2524 acc:0.6142968535423279\n",
      "Epoch: 12/100... Step: 2360... Loss: 1.2396... Val Loss: 1.2561 acc:0.6150781512260437\n",
      "Epoch: 12/100... Step: 2370... Loss: 1.2454... Val Loss: 1.2567 acc:0.6138281226158142\n",
      "Epoch: 12/100... Step: 2380... Loss: 1.2389... Val Loss: 1.2542 acc:0.6157812476158142\n",
      "Epoch: 12/100... Step: 2390... Loss: 1.2588... Val Loss: 1.2529 acc:0.6156250238418579\n",
      "Epoch: 12/100... Step: 2400... Loss: 1.2983... Val Loss: 1.2557 acc:0.6161718964576721\n",
      "Epoch: 13/100... Step: 2410... Loss: 1.2331... Val Loss: 1.2545 acc:0.6165624856948853\n",
      "Epoch: 13/100... Step: 2420... Loss: 1.2538... Val Loss: 1.2547 acc:0.6171875\n",
      "Epoch: 13/100... Step: 2430... Loss: 1.2151... Val Loss: 1.2502 acc:0.6148437261581421\n",
      "Epoch: 13/100... Step: 2440... Loss: 1.1973... Val Loss: 1.2510 acc:0.6182031035423279\n",
      "Epoch: 13/100... Step: 2450... Loss: 1.2228... Val Loss: 1.2559 acc:0.6171875\n",
      "Epoch: 13/100... Step: 2460... Loss: 1.2329... Val Loss: 1.2515 acc:0.6176562309265137\n",
      "Epoch: 13/100... Step: 2470... Loss: 1.2165... Val Loss: 1.2539 acc:0.6186718940734863\n",
      "Epoch: 13/100... Step: 2480... Loss: 1.2218... Val Loss: 1.2561 acc:0.619140625\n",
      "Epoch: 13/100... Step: 2490... Loss: 1.2121... Val Loss: 1.2536 acc:0.6184375286102295\n",
      "Epoch: 13/100... Step: 2500... Loss: 1.2163... Val Loss: 1.2551 acc:0.6180468797683716\n",
      "Epoch: 13/100... Step: 2510... Loss: 1.2239... Val Loss: 1.2518 acc:0.6171093583106995\n",
      "Epoch: 13/100... Step: 2520... Loss: 1.2101... Val Loss: 1.2522 acc:0.6176562309265137\n",
      "Epoch: 13/100... Step: 2530... Loss: 1.2030... Val Loss: 1.2463 acc:0.6188281178474426\n",
      "Epoch: 13/100... Step: 2540... Loss: 1.2423... Val Loss: 1.2510 acc:0.6167187690734863\n",
      "Epoch: 13/100... Step: 2550... Loss: 1.2438... Val Loss: 1.2488 acc:0.6162499785423279\n",
      "Epoch: 13/100... Step: 2560... Loss: 1.2256... Val Loss: 1.2469 acc:0.6180468797683716\n",
      "Epoch: 13/100... Step: 2570... Loss: 1.2301... Val Loss: 1.2488 acc:0.6196874976158142\n",
      "Epoch: 13/100... Step: 2580... Loss: 1.2245... Val Loss: 1.2518 acc:0.6188281178474426\n",
      "Epoch: 13/100... Step: 2590... Loss: 1.2616... Val Loss: 1.2497 acc:0.6177343726158142\n",
      "Epoch: 13/100... Step: 2600... Loss: 1.2857... Val Loss: 1.2472 acc:0.6160937547683716\n",
      "Epoch: 14/100... Step: 2610... Loss: 1.2275... Val Loss: 1.2484 acc:0.618359386920929\n",
      "Epoch: 14/100... Step: 2620... Loss: 1.2378... Val Loss: 1.2447 acc:0.6195312738418579\n",
      "Epoch: 14/100... Step: 2630... Loss: 1.2013... Val Loss: 1.2436 acc:0.6185156106948853\n",
      "Epoch: 14/100... Step: 2640... Loss: 1.1849... Val Loss: 1.2450 acc:0.620312511920929\n",
      "Epoch: 14/100... Step: 2650... Loss: 1.2131... Val Loss: 1.2448 acc:0.6215624809265137\n",
      "Epoch: 14/100... Step: 2660... Loss: 1.2139... Val Loss: 1.2436 acc:0.6207031011581421\n",
      "Epoch: 14/100... Step: 2670... Loss: 1.2008... Val Loss: 1.2471 acc:0.6201562285423279\n",
      "Epoch: 14/100... Step: 2680... Loss: 1.2320... Val Loss: 1.2479 acc:0.6207031011581421\n",
      "Epoch: 14/100... Step: 2690... Loss: 1.1995... Val Loss: 1.2458 acc:0.6197656393051147\n",
      "Epoch: 14/100... Step: 2700... Loss: 1.1973... Val Loss: 1.2457 acc:0.620312511920929\n",
      "Epoch: 14/100... Step: 2710... Loss: 1.2169... Val Loss: 1.2470 acc:0.6202343702316284\n",
      "Epoch: 14/100... Step: 2720... Loss: 1.2007... Val Loss: 1.2429 acc:0.6207031011581421\n",
      "Epoch: 14/100... Step: 2730... Loss: 1.1997... Val Loss: 1.2411 acc:0.6196093559265137\n",
      "Epoch: 14/100... Step: 2740... Loss: 1.2312... Val Loss: 1.2395 acc:0.6196093559265137\n",
      "Epoch: 14/100... Step: 2750... Loss: 1.2309... Val Loss: 1.2413 acc:0.6177343726158142\n",
      "Epoch: 14/100... Step: 2760... Loss: 1.2167... Val Loss: 1.2448 acc:0.6187499761581421\n",
      "Epoch: 14/100... Step: 2770... Loss: 1.2128... Val Loss: 1.2450 acc:0.616406261920929\n",
      "Epoch: 14/100... Step: 2780... Loss: 1.2199... Val Loss: 1.2441 acc:0.6207031011581421\n",
      "Epoch: 14/100... Step: 2790... Loss: 1.2420... Val Loss: 1.2435 acc:0.6181250214576721\n",
      "Epoch: 14/100... Step: 2800... Loss: 1.2771... Val Loss: 1.2430 acc:0.6197656393051147\n",
      "Epoch: 15/100... Step: 2810... Loss: 1.2162... Val Loss: 1.2397 acc:0.6188281178474426\n",
      "Epoch: 15/100... Step: 2820... Loss: 1.2231... Val Loss: 1.2426 acc:0.620312511920929\n",
      "Epoch: 15/100... Step: 2830... Loss: 1.1964... Val Loss: 1.2401 acc:0.6198437213897705\n",
      "Epoch: 15/100... Step: 2840... Loss: 1.1767... Val Loss: 1.2415 acc:0.6217187643051147\n",
      "Epoch: 15/100... Step: 2850... Loss: 1.2085... Val Loss: 1.2414 acc:0.6217968463897705\n",
      "Epoch: 15/100... Step: 2860... Loss: 1.2117... Val Loss: 1.2411 acc:0.6210156083106995\n",
      "Epoch: 15/100... Step: 2870... Loss: 1.1988... Val Loss: 1.2413 acc:0.6239843964576721\n",
      "Epoch: 15/100... Step: 2880... Loss: 1.2066... Val Loss: 1.2411 acc:0.6227343678474426\n",
      "Epoch: 15/100... Step: 2890... Loss: 1.1905... Val Loss: 1.2413 acc:0.6216406226158142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/100... Step: 2900... Loss: 1.1905... Val Loss: 1.2396 acc:0.622265636920929\n",
      "Epoch: 15/100... Step: 2910... Loss: 1.2085... Val Loss: 1.2431 acc:0.6189844012260437\n",
      "Epoch: 15/100... Step: 2920... Loss: 1.1920... Val Loss: 1.2388 acc:0.6224218606948853\n",
      "Epoch: 15/100... Step: 2930... Loss: 1.1870... Val Loss: 1.2347 acc:0.6206250190734863\n",
      "Epoch: 15/100... Step: 2940... Loss: 1.2240... Val Loss: 1.2388 acc:0.6184375286102295\n",
      "Epoch: 15/100... Step: 2950... Loss: 1.2201... Val Loss: 1.2392 acc:0.6200781464576721\n",
      "Epoch: 15/100... Step: 2960... Loss: 1.2119... Val Loss: 1.2426 acc:0.6197656393051147\n",
      "Epoch: 15/100... Step: 2970... Loss: 1.2080... Val Loss: 1.2401 acc:0.6193749904632568\n",
      "Epoch: 15/100... Step: 2980... Loss: 1.2075... Val Loss: 1.2413 acc:0.6211718916893005\n",
      "Epoch: 15/100... Step: 2990... Loss: 1.2371... Val Loss: 1.2407 acc:0.6236718893051147\n",
      "Epoch: 15/100... Step: 3000... Loss: 1.2705... Val Loss: 1.2357 acc:0.6200000047683716\n",
      "Epoch: 16/100... Step: 3010... Loss: 1.2095... Val Loss: 1.2379 acc:0.6206250190734863\n",
      "Epoch: 16/100... Step: 3020... Loss: 1.2180... Val Loss: 1.2396 acc:0.6217187643051147\n",
      "Epoch: 16/100... Step: 3030... Loss: 1.1776... Val Loss: 1.2377 acc:0.6209375262260437\n",
      "Epoch: 16/100... Step: 3040... Loss: 1.1672... Val Loss: 1.2390 acc:0.6221093535423279\n",
      "Epoch: 16/100... Step: 3050... Loss: 1.1979... Val Loss: 1.2382 acc:0.6232812404632568\n",
      "Epoch: 16/100... Step: 3060... Loss: 1.1937... Val Loss: 1.2349 acc:0.6216406226158142\n",
      "Epoch: 16/100... Step: 3070... Loss: 1.1952... Val Loss: 1.2367 acc:0.6225000023841858\n",
      "Epoch: 16/100... Step: 3080... Loss: 1.2086... Val Loss: 1.2384 acc:0.6214843988418579\n",
      "Epoch: 16/100... Step: 3090... Loss: 1.1809... Val Loss: 1.2377 acc:0.6193749904632568\n",
      "Epoch: 16/100... Step: 3100... Loss: 1.1759... Val Loss: 1.2346 acc:0.6247656345367432\n",
      "Epoch: 16/100... Step: 3110... Loss: 1.1956... Val Loss: 1.2381 acc:0.6231250166893005\n",
      "Epoch: 16/100... Step: 3120... Loss: 1.1770... Val Loss: 1.2351 acc:0.6232030987739563\n",
      "Epoch: 16/100... Step: 3130... Loss: 1.1800... Val Loss: 1.2334 acc:0.6211718916893005\n",
      "Epoch: 16/100... Step: 3140... Loss: 1.2113... Val Loss: 1.2342 acc:0.6204687356948853\n",
      "Epoch: 16/100... Step: 3150... Loss: 1.2106... Val Loss: 1.2335 acc:0.6207812428474426\n",
      "Epoch: 16/100... Step: 3160... Loss: 1.1980... Val Loss: 1.2329 acc:0.6256250143051147\n",
      "Epoch: 16/100... Step: 3170... Loss: 1.1967... Val Loss: 1.2345 acc:0.6226562261581421\n",
      "Epoch: 16/100... Step: 3180... Loss: 1.1991... Val Loss: 1.2322 acc:0.6241406202316284\n",
      "Epoch: 16/100... Step: 3190... Loss: 1.2170... Val Loss: 1.2324 acc:0.6257030963897705\n",
      "Epoch: 16/100... Step: 3200... Loss: 1.2569... Val Loss: 1.2336 acc:0.6235156059265137\n",
      "Epoch: 17/100... Step: 3210... Loss: 1.2027... Val Loss: 1.2285 acc:0.6212499737739563\n",
      "Epoch: 17/100... Step: 3220... Loss: 1.2099... Val Loss: 1.2265 acc:0.6205468773841858\n",
      "Epoch: 17/100... Step: 3230... Loss: 1.1750... Val Loss: 1.2277 acc:0.6228906512260437\n",
      "Epoch: 17/100... Step: 3240... Loss: 1.1602... Val Loss: 1.2255 acc:0.6253125071525574\n",
      "Epoch: 17/100... Step: 3250... Loss: 1.1877... Val Loss: 1.2272 acc:0.6259375214576721\n",
      "Epoch: 17/100... Step: 3260... Loss: 1.1927... Val Loss: 1.2299 acc:0.6258593797683716\n",
      "Epoch: 17/100... Step: 3270... Loss: 1.1812... Val Loss: 1.2389 acc:0.622265636920929\n",
      "Epoch: 17/100... Step: 3280... Loss: 1.1790... Val Loss: 1.2352 acc:0.6247656345367432\n",
      "Epoch: 17/100... Step: 3290... Loss: 1.1764... Val Loss: 1.2292 acc:0.6225000023841858\n",
      "Epoch: 17/100... Step: 3300... Loss: 1.1733... Val Loss: 1.2297 acc:0.6241406202316284\n",
      "Epoch: 17/100... Step: 3310... Loss: 1.1750... Val Loss: 1.2292 acc:0.6231250166893005\n",
      "Epoch: 17/100... Step: 3320... Loss: 1.1729... Val Loss: 1.2299 acc:0.6243749856948853\n",
      "Epoch: 17/100... Step: 3330... Loss: 1.1739... Val Loss: 1.2312 acc:0.6270312666893005\n",
      "Epoch: 17/100... Step: 3340... Loss: 1.2042... Val Loss: 1.2291 acc:0.6260156035423279\n",
      "Epoch: 17/100... Step: 3350... Loss: 1.2046... Val Loss: 1.2291 acc:0.62109375\n",
      "Epoch: 17/100... Step: 3360... Loss: 1.1844... Val Loss: 1.2281 acc:0.6253906488418579\n",
      "Epoch: 17/100... Step: 3370... Loss: 1.1956... Val Loss: 1.2287 acc:0.6240624785423279\n",
      "Epoch: 17/100... Step: 3380... Loss: 1.1850... Val Loss: 1.2273 acc:0.6285156011581421\n",
      "Epoch: 17/100... Step: 3390... Loss: 1.2135... Val Loss: 1.2294 acc:0.6251562237739563\n",
      "Epoch: 17/100... Step: 3400... Loss: 1.2387... Val Loss: 1.2273 acc:0.6258593797683716\n",
      "Epoch: 18/100... Step: 3410... Loss: 1.1843... Val Loss: 1.2269 acc:0.6228906512260437\n",
      "Epoch: 18/100... Step: 3420... Loss: 1.1974... Val Loss: 1.2286 acc:0.6250781416893005\n",
      "Epoch: 18/100... Step: 3430... Loss: 1.1652... Val Loss: 1.2294 acc:0.6267969012260437\n",
      "Epoch: 18/100... Step: 3440... Loss: 1.1467... Val Loss: 1.2281 acc:0.6245312690734863\n",
      "Epoch: 18/100... Step: 3450... Loss: 1.1812... Val Loss: 1.2299 acc:0.6272656321525574\n",
      "Epoch: 18/100... Step: 3460... Loss: 1.1949... Val Loss: 1.2293 acc:0.6235937476158142\n",
      "Epoch: 18/100... Step: 3470... Loss: 1.1757... Val Loss: 1.2322 acc:0.6253906488418579\n",
      "Epoch: 18/100... Step: 3480... Loss: 1.1860... Val Loss: 1.2329 acc:0.6267187595367432\n",
      "Epoch: 18/100... Step: 3490... Loss: 1.1684... Val Loss: 1.2271 acc:0.6251562237739563\n",
      "Epoch: 18/100... Step: 3500... Loss: 1.1656... Val Loss: 1.2251 acc:0.6253906488418579\n",
      "Epoch: 18/100... Step: 3510... Loss: 1.1860... Val Loss: 1.2285 acc:0.623828113079071\n",
      "Epoch: 18/100... Step: 3520... Loss: 1.1654... Val Loss: 1.2269 acc:0.626953125\n",
      "Epoch: 18/100... Step: 3530... Loss: 1.1706... Val Loss: 1.2240 acc:0.623828113079071\n",
      "Epoch: 18/100... Step: 3540... Loss: 1.1930... Val Loss: 1.2246 acc:0.625781238079071\n",
      "Epoch: 18/100... Step: 3550... Loss: 1.1891... Val Loss: 1.2246 acc:0.6232812404632568\n",
      "Epoch: 18/100... Step: 3560... Loss: 1.1716... Val Loss: 1.2235 acc:0.6236718893051147\n",
      "Epoch: 18/100... Step: 3570... Loss: 1.1841... Val Loss: 1.2239 acc:0.6267187595367432\n",
      "Epoch: 18/100... Step: 3580... Loss: 1.1775... Val Loss: 1.2272 acc:0.6265624761581421\n",
      "Epoch: 18/100... Step: 3590... Loss: 1.2074... Val Loss: 1.2307 acc:0.6274999976158142\n",
      "Epoch: 18/100... Step: 3600... Loss: 1.2460... Val Loss: 1.2275 acc:0.6265624761581421\n",
      "Epoch: 19/100... Step: 3610... Loss: 1.1901... Val Loss: 1.2266 acc:0.6244531273841858\n",
      "Epoch: 19/100... Step: 3620... Loss: 1.1897... Val Loss: 1.2250 acc:0.630078136920929\n",
      "Epoch: 19/100... Step: 3630... Loss: 1.1493... Val Loss: 1.2231 acc:0.6279687285423279\n",
      "Epoch: 19/100... Step: 3640... Loss: 1.1259... Val Loss: 1.2261 acc:0.6278906464576721\n",
      "Epoch: 19/100... Step: 3650... Loss: 1.1700... Val Loss: 1.2288 acc:0.6272656321525574\n",
      "Epoch: 19/100... Step: 3660... Loss: 1.1764... Val Loss: 1.2231 acc:0.62890625\n",
      "Epoch: 19/100... Step: 3670... Loss: 1.1603... Val Loss: 1.2261 acc:0.6306250095367432\n",
      "Epoch: 19/100... Step: 3680... Loss: 1.1752... Val Loss: 1.2232 acc:0.6304687261581421\n",
      "Epoch: 19/100... Step: 3690... Loss: 1.1537... Val Loss: 1.2234 acc:0.6272656321525574\n",
      "Epoch: 19/100... Step: 3700... Loss: 1.1492... Val Loss: 1.2247 acc:0.6267969012260437\n",
      "Epoch: 19/100... Step: 3710... Loss: 1.1822... Val Loss: 1.2244 acc:0.6283593773841858\n",
      "Epoch: 19/100... Step: 3720... Loss: 1.1574... Val Loss: 1.2226 acc:0.62890625\n",
      "Epoch: 19/100... Step: 3730... Loss: 1.1550... Val Loss: 1.2180 acc:0.6283593773841858\n",
      "Epoch: 19/100... Step: 3740... Loss: 1.1852... Val Loss: 1.2195 acc:0.6293749809265137\n",
      "Epoch: 19/100... Step: 3750... Loss: 1.1856... Val Loss: 1.2215 acc:0.6292187571525574\n",
      "Epoch: 19/100... Step: 3760... Loss: 1.1713... Val Loss: 1.2203 acc:0.628125011920929\n",
      "Epoch: 19/100... Step: 3770... Loss: 1.1761... Val Loss: 1.2225 acc:0.6262500286102295\n",
      "Epoch: 19/100... Step: 3780... Loss: 1.1688... Val Loss: 1.2208 acc:0.6295312643051147\n",
      "Epoch: 19/100... Step: 3790... Loss: 1.1972... Val Loss: 1.2209 acc:0.627734363079071\n",
      "Epoch: 19/100... Step: 3800... Loss: 1.2347... Val Loss: 1.2196 acc:0.6290624737739563\n",
      "Epoch: 20/100... Step: 3810... Loss: 1.1673... Val Loss: 1.2242 acc:0.6282812356948853\n",
      "Epoch: 20/100... Step: 3820... Loss: 1.1841... Val Loss: 1.2228 acc:0.62890625\n",
      "Epoch: 20/100... Step: 3830... Loss: 1.1555... Val Loss: 1.2194 acc:0.6298437714576721\n",
      "Epoch: 20/100... Step: 3840... Loss: 1.1340... Val Loss: 1.2232 acc:0.6278906464576721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/100... Step: 3850... Loss: 1.1669... Val Loss: 1.2238 acc:0.6278906464576721\n",
      "Epoch: 20/100... Step: 3860... Loss: 1.1775... Val Loss: 1.2195 acc:0.6271093487739563\n",
      "Epoch: 20/100... Step: 3870... Loss: 1.1473... Val Loss: 1.2232 acc:0.6303125023841858\n",
      "Epoch: 20/100... Step: 3880... Loss: 1.1628... Val Loss: 1.2201 acc:0.6287500262260437\n",
      "Epoch: 20/100... Step: 3890... Loss: 1.1515... Val Loss: 1.2202 acc:0.6324999928474426\n",
      "Epoch: 20/100... Step: 3900... Loss: 1.1283... Val Loss: 1.2210 acc:0.6288281083106995\n",
      "Epoch: 20/100... Step: 3910... Loss: 1.1612... Val Loss: 1.2221 acc:0.6268749833106995\n",
      "Epoch: 20/100... Step: 3920... Loss: 1.1467... Val Loss: 1.2212 acc:0.6304687261581421\n",
      "Epoch: 20/100... Step: 3930... Loss: 1.1456... Val Loss: 1.2178 acc:0.6301562786102295\n",
      "Epoch: 20/100... Step: 3940... Loss: 1.1811... Val Loss: 1.2152 acc:0.6287500262260437\n",
      "Epoch: 20/100... Step: 3950... Loss: 1.1784... Val Loss: 1.2161 acc:0.6271093487739563\n",
      "Epoch: 20/100... Step: 3960... Loss: 1.1695... Val Loss: 1.2172 acc:0.6282031536102295\n",
      "Epoch: 20/100... Step: 3970... Loss: 1.1613... Val Loss: 1.2219 acc:0.6272656321525574\n",
      "Epoch: 20/100... Step: 3980... Loss: 1.1635... Val Loss: 1.2232 acc:0.628125011920929\n",
      "Epoch: 20/100... Step: 3990... Loss: 1.1930... Val Loss: 1.2161 acc:0.6286718845367432\n",
      "Epoch: 20/100... Step: 4000... Loss: 1.2262... Val Loss: 1.2156 acc:0.6299218535423279\n",
      "Epoch: 21/100... Step: 4010... Loss: 1.1728... Val Loss: 1.2175 acc:0.6290624737739563\n",
      "Epoch: 21/100... Step: 4020... Loss: 1.1765... Val Loss: 1.2237 acc:0.6298437714576721\n",
      "Epoch: 21/100... Step: 4030... Loss: 1.1390... Val Loss: 1.2160 acc:0.6291406154632568\n",
      "Epoch: 21/100... Step: 4040... Loss: 1.1304... Val Loss: 1.2188 acc:0.6321874856948853\n",
      "Epoch: 21/100... Step: 4050... Loss: 1.1605... Val Loss: 1.2176 acc:0.6307812333106995\n",
      "Epoch: 21/100... Step: 4060... Loss: 1.1655... Val Loss: 1.2183 acc:0.6294531226158142\n",
      "Epoch: 21/100... Step: 4070... Loss: 1.1430... Val Loss: 1.2210 acc:0.6296093463897705\n",
      "Epoch: 21/100... Step: 4080... Loss: 1.1603... Val Loss: 1.2191 acc:0.6298437714576721\n",
      "Epoch: 21/100... Step: 4090... Loss: 1.1421... Val Loss: 1.2149 acc:0.6278125047683716\n",
      "Epoch: 21/100... Step: 4100... Loss: 1.1334... Val Loss: 1.2173 acc:0.6282812356948853\n",
      "Epoch: 21/100... Step: 4110... Loss: 1.1693... Val Loss: 1.2172 acc:0.6283593773841858\n",
      "Epoch: 21/100... Step: 4120... Loss: 1.1431... Val Loss: 1.2242 acc:0.6317187547683716\n",
      "Epoch: 21/100... Step: 4130... Loss: 1.1452... Val Loss: 1.2205 acc:0.62890625\n",
      "Epoch: 21/100... Step: 4140... Loss: 1.1723... Val Loss: 1.2177 acc:0.6282812356948853\n",
      "Epoch: 21/100... Step: 4150... Loss: 1.1728... Val Loss: 1.2162 acc:0.6302343606948853\n",
      "Epoch: 21/100... Step: 4160... Loss: 1.1657... Val Loss: 1.2136 acc:0.6312500238418579\n",
      "Epoch: 21/100... Step: 4170... Loss: 1.1570... Val Loss: 1.2188 acc:0.6284375190734863\n",
      "Epoch: 21/100... Step: 4180... Loss: 1.1583... Val Loss: 1.2173 acc:0.6290624737739563\n",
      "Epoch: 21/100... Step: 4190... Loss: 1.1869... Val Loss: 1.2162 acc:0.6272656321525574\n",
      "Epoch: 21/100... Step: 4200... Loss: 1.2243... Val Loss: 1.2190 acc:0.6288281083106995\n",
      "Epoch: 22/100... Step: 4210... Loss: 1.1655... Val Loss: 1.2114 acc:0.6305468678474426\n",
      "Epoch: 22/100... Step: 4220... Loss: 1.1672... Val Loss: 1.2116 acc:0.6285156011581421\n",
      "Epoch: 22/100... Step: 4230... Loss: 1.1391... Val Loss: 1.2123 acc:0.6255468726158142\n",
      "Epoch: 22/100... Step: 4240... Loss: 1.1248... Val Loss: 1.2113 acc:0.6315624713897705\n",
      "Epoch: 22/100... Step: 4250... Loss: 1.1429... Val Loss: 1.2136 acc:0.6286718845367432\n",
      "Epoch: 22/100... Step: 4260... Loss: 1.1605... Val Loss: 1.2108 acc:0.6299218535423279\n",
      "Epoch: 22/100... Step: 4270... Loss: 1.1380... Val Loss: 1.2122 acc:0.6285937428474426\n",
      "Epoch: 22/100... Step: 4280... Loss: 1.1553... Val Loss: 1.2102 acc:0.6296093463897705\n",
      "Epoch: 22/100... Step: 4290... Loss: 1.1338... Val Loss: 1.2084 acc:0.6285937428474426\n",
      "Epoch: 22/100... Step: 4300... Loss: 1.1396... Val Loss: 1.2092 acc:0.628125011920929\n",
      "Epoch: 22/100... Step: 4310... Loss: 1.1466... Val Loss: 1.2081 acc:0.6294531226158142\n",
      "Epoch: 22/100... Step: 4320... Loss: 1.1354... Val Loss: 1.2078 acc:0.6293749809265137\n",
      "Epoch: 22/100... Step: 4330... Loss: 1.1319... Val Loss: 1.2058 acc:0.6302343606948853\n",
      "Epoch: 22/100... Step: 4340... Loss: 1.1633... Val Loss: 1.2054 acc:0.6293749809265137\n",
      "Epoch: 22/100... Step: 4350... Loss: 1.1635... Val Loss: 1.2104 acc:0.6284375190734863\n",
      "Epoch: 22/100... Step: 4360... Loss: 1.1544... Val Loss: 1.2060 acc:0.6318749785423279\n",
      "Epoch: 22/100... Step: 4370... Loss: 1.1456... Val Loss: 1.2100 acc:0.6307031512260437\n",
      "Epoch: 22/100... Step: 4380... Loss: 1.1478... Val Loss: 1.2073 acc:0.6332812309265137\n",
      "Epoch: 22/100... Step: 4390... Loss: 1.1808... Val Loss: 1.2057 acc:0.6296093463897705\n",
      "Epoch: 22/100... Step: 4400... Loss: 1.2192... Val Loss: 1.2059 acc:0.6312500238418579\n",
      "Epoch: 23/100... Step: 4410... Loss: 1.1588... Val Loss: 1.2124 acc:0.6295312643051147\n",
      "Epoch: 23/100... Step: 4420... Loss: 1.1567... Val Loss: 1.2100 acc:0.630859375\n",
      "Epoch: 23/100... Step: 4430... Loss: 1.1279... Val Loss: 1.2073 acc:0.6314062476158142\n",
      "Epoch: 23/100... Step: 4440... Loss: 1.1182... Val Loss: 1.2090 acc:0.6324999928474426\n",
      "Epoch: 23/100... Step: 4450... Loss: 1.1363... Val Loss: 1.2086 acc:0.6307812333106995\n",
      "Epoch: 23/100... Step: 4460... Loss: 1.1475... Val Loss: 1.2095 acc:0.6307031512260437\n",
      "Epoch: 23/100... Step: 4470... Loss: 1.1449... Val Loss: 1.2142 acc:0.631640613079071\n",
      "Epoch: 23/100... Step: 4480... Loss: 1.1566... Val Loss: 1.2109 acc:0.6321874856948853\n",
      "Epoch: 23/100... Step: 4490... Loss: 1.1255... Val Loss: 1.2095 acc:0.6302343606948853\n",
      "Epoch: 23/100... Step: 4500... Loss: 1.1311... Val Loss: 1.2093 acc:0.6287500262260437\n",
      "Epoch: 23/100... Step: 4510... Loss: 1.1479... Val Loss: 1.2102 acc:0.6315624713897705\n",
      "Epoch: 23/100... Step: 4520... Loss: 1.1231... Val Loss: 1.2105 acc:0.6313281059265137\n",
      "Epoch: 23/100... Step: 4530... Loss: 1.1283... Val Loss: 1.2079 acc:0.632031261920929\n",
      "Epoch: 23/100... Step: 4540... Loss: 1.1568... Val Loss: 1.2061 acc:0.6302343606948853\n",
      "Epoch: 23/100... Step: 4550... Loss: 1.1599... Val Loss: 1.2085 acc:0.6301562786102295\n",
      "Epoch: 23/100... Step: 4560... Loss: 1.1544... Val Loss: 1.2035 acc:0.6323437690734863\n",
      "Epoch: 23/100... Step: 4570... Loss: 1.1429... Val Loss: 1.2062 acc:0.6324218511581421\n",
      "Epoch: 23/100... Step: 4580... Loss: 1.1486... Val Loss: 1.2075 acc:0.6323437690734863\n",
      "Epoch: 23/100... Step: 4590... Loss: 1.1744... Val Loss: 1.2020 acc:0.6313281059265137\n",
      "Epoch: 23/100... Step: 4600... Loss: 1.2071... Val Loss: 1.2026 acc:0.6332812309265137\n",
      "Epoch: 24/100... Step: 4610... Loss: 1.1433... Val Loss: 1.2110 acc:0.6318749785423279\n",
      "Epoch: 24/100... Step: 4620... Loss: 1.1554... Val Loss: 1.2064 acc:0.6337500214576721\n",
      "Epoch: 24/100... Step: 4630... Loss: 1.1303... Val Loss: 1.2080 acc:0.6321094036102295\n",
      "Epoch: 24/100... Step: 4640... Loss: 1.1156... Val Loss: 1.2088 acc:0.6333593726158142\n",
      "Epoch: 24/100... Step: 4650... Loss: 1.1365... Val Loss: 1.2053 acc:0.6317187547683716\n",
      "Epoch: 24/100... Step: 4660... Loss: 1.1437... Val Loss: 1.2037 acc:0.6345312595367432\n",
      "Epoch: 24/100... Step: 4670... Loss: 1.1274... Val Loss: 1.2092 acc:0.6326562762260437\n",
      "Epoch: 24/100... Step: 4680... Loss: 1.1408... Val Loss: 1.2089 acc:0.6349218487739563\n",
      "Epoch: 24/100... Step: 4690... Loss: 1.1268... Val Loss: 1.2079 acc:0.6332812309265137\n",
      "Epoch: 24/100... Step: 4700... Loss: 1.1331... Val Loss: 1.2101 acc:0.6305468678474426\n",
      "Epoch: 24/100... Step: 4710... Loss: 1.1354... Val Loss: 1.2112 acc:0.6319531202316284\n",
      "Epoch: 24/100... Step: 4720... Loss: 1.1282... Val Loss: 1.2089 acc:0.631640613079071\n",
      "Epoch: 24/100... Step: 4730... Loss: 1.1311... Val Loss: 1.2050 acc:0.6342187523841858\n",
      "Epoch: 24/100... Step: 4740... Loss: 1.1582... Val Loss: 1.2042 acc:0.6321094036102295\n",
      "Epoch: 24/100... Step: 4750... Loss: 1.1489... Val Loss: 1.2067 acc:0.631640613079071\n",
      "Epoch: 24/100... Step: 4760... Loss: 1.1399... Val Loss: 1.2032 acc:0.6352343559265137\n",
      "Epoch: 24/100... Step: 4770... Loss: 1.1435... Val Loss: 1.2070 acc:0.6330468654632568\n",
      "Epoch: 24/100... Step: 4780... Loss: 1.1493... Val Loss: 1.2109 acc:0.6346094012260437\n",
      "Epoch: 24/100... Step: 4790... Loss: 1.1741... Val Loss: 1.2071 acc:0.6328906416893005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/100... Step: 4800... Loss: 1.2084... Val Loss: 1.2053 acc:0.6334375143051147\n",
      "Epoch: 25/100... Step: 4810... Loss: 1.1435... Val Loss: 1.2099 acc:0.6310937404632568\n",
      "Epoch: 25/100... Step: 4820... Loss: 1.1580... Val Loss: 1.2077 acc:0.633593738079071\n",
      "Epoch: 25/100... Step: 4830... Loss: 1.1104... Val Loss: 1.2097 acc:0.6315624713897705\n",
      "Epoch: 25/100... Step: 4840... Loss: 1.1081... Val Loss: 1.2092 acc:0.6365625262260437\n",
      "Epoch: 25/100... Step: 4850... Loss: 1.1281... Val Loss: 1.2085 acc:0.6353906393051147\n",
      "Epoch: 25/100... Step: 4860... Loss: 1.1370... Val Loss: 1.2060 acc:0.6337500214576721\n",
      "Epoch: 25/100... Step: 4870... Loss: 1.1256... Val Loss: 1.2130 acc:0.6327343583106995\n",
      "Epoch: 25/100... Step: 4880... Loss: 1.1356... Val Loss: 1.2084 acc:0.635546863079071\n",
      "Epoch: 25/100... Step: 4890... Loss: 1.1241... Val Loss: 1.2055 acc:0.6318749785423279\n",
      "Epoch: 25/100... Step: 4900... Loss: 1.1185... Val Loss: 1.2056 acc:0.6341406106948853\n",
      "Epoch: 25/100... Step: 4910... Loss: 1.1365... Val Loss: 1.2146 acc:0.6309375166893005\n",
      "Epoch: 25/100... Step: 4920... Loss: 1.1185... Val Loss: 1.2093 acc:0.6323437690734863\n",
      "Epoch: 25/100... Step: 4930... Loss: 1.1233... Val Loss: 1.2056 acc:0.6343749761581421\n",
      "Epoch: 25/100... Step: 4940... Loss: 1.1563... Val Loss: 1.2049 acc:0.6353906393051147\n",
      "Epoch: 25/100... Step: 4950... Loss: 1.1496... Val Loss: 1.2079 acc:0.6309375166893005\n",
      "Epoch: 25/100... Step: 4960... Loss: 1.1345... Val Loss: 1.2058 acc:0.6343749761581421\n",
      "Epoch: 25/100... Step: 4970... Loss: 1.1384... Val Loss: 1.2091 acc:0.6326562762260437\n",
      "Epoch: 25/100... Step: 4980... Loss: 1.1419... Val Loss: 1.2060 acc:0.6356250047683716\n",
      "Epoch: 25/100... Step: 4990... Loss: 1.1615... Val Loss: 1.2030 acc:0.6349999904632568\n",
      "Epoch: 25/100... Step: 5000... Loss: 1.1966... Val Loss: 1.2014 acc:0.6335155963897705\n",
      "Epoch: 26/100... Step: 5010... Loss: 1.1356... Val Loss: 1.2069 acc:0.6324999928474426\n",
      "Epoch: 26/100... Step: 5020... Loss: 1.1493... Val Loss: 1.2045 acc:0.6353124976158142\n",
      "Epoch: 26/100... Step: 5030... Loss: 1.1064... Val Loss: 1.2061 acc:0.6340625286102295\n",
      "Epoch: 26/100... Step: 5040... Loss: 1.1010... Val Loss: 1.2112 acc:0.6339062452316284\n",
      "Epoch: 26/100... Step: 5050... Loss: 1.1192... Val Loss: 1.2097 acc:0.6353124976158142\n",
      "Epoch: 26/100... Step: 5060... Loss: 1.1278... Val Loss: 1.2051 acc:0.6335155963897705\n",
      "Epoch: 26/100... Step: 5070... Loss: 1.1272... Val Loss: 1.2134 acc:0.6332031488418579\n",
      "Epoch: 26/100... Step: 5080... Loss: 1.1309... Val Loss: 1.2048 acc:0.6384375095367432\n",
      "Epoch: 26/100... Step: 5090... Loss: 1.1157... Val Loss: 1.2028 acc:0.6364062428474426\n",
      "Epoch: 26/100... Step: 5100... Loss: 1.1161... Val Loss: 1.2094 acc:0.6353906393051147\n",
      "Epoch: 26/100... Step: 5110... Loss: 1.1368... Val Loss: 1.2102 acc:0.6341406106948853\n",
      "Epoch: 26/100... Step: 5120... Loss: 1.1152... Val Loss: 1.2074 acc:0.6356250047683716\n",
      "Epoch: 26/100... Step: 5130... Loss: 1.1133... Val Loss: 1.2085 acc:0.6357812285423279\n",
      "Epoch: 26/100... Step: 5140... Loss: 1.1424... Val Loss: 1.2032 acc:0.6372656226158142\n",
      "Epoch: 26/100... Step: 5150... Loss: 1.1428... Val Loss: 1.2052 acc:0.6354687213897705\n",
      "Epoch: 26/100... Step: 5160... Loss: 1.1358... Val Loss: 1.2018 acc:0.6363281011581421\n",
      "Epoch: 26/100... Step: 5170... Loss: 1.1317... Val Loss: 1.2069 acc:0.6353906393051147\n",
      "Epoch: 26/100... Step: 5180... Loss: 1.1390... Val Loss: 1.2103 acc:0.6348437666893005\n",
      "Epoch: 26/100... Step: 5190... Loss: 1.1527... Val Loss: 1.2019 acc:0.6318749785423279\n",
      "Epoch: 26/100... Step: 5200... Loss: 1.1978... Val Loss: 1.2029 acc:0.6332812309265137\n",
      "Epoch: 27/100... Step: 5210... Loss: 1.1258... Val Loss: 1.2102 acc:0.6332031488418579\n",
      "Epoch: 27/100... Step: 5220... Loss: 1.1470... Val Loss: 1.2147 acc:0.633593738079071\n",
      "Epoch: 27/100... Step: 5230... Loss: 1.1098... Val Loss: 1.2059 acc:0.6324999928474426\n",
      "Epoch: 27/100... Step: 5240... Loss: 1.1000... Val Loss: 1.2116 acc:0.6366406083106995\n",
      "Epoch: 27/100... Step: 5250... Loss: 1.1155... Val Loss: 1.2082 acc:0.637499988079071\n",
      "Epoch: 27/100... Step: 5260... Loss: 1.1229... Val Loss: 1.2035 acc:0.6365625262260437\n",
      "Epoch: 27/100... Step: 5270... Loss: 1.1186... Val Loss: 1.2090 acc:0.6357812285423279\n",
      "Epoch: 27/100... Step: 5280... Loss: 1.1280... Val Loss: 1.2079 acc:0.6342968940734863\n",
      "Epoch: 27/100... Step: 5290... Loss: 1.1143... Val Loss: 1.2085 acc:0.6357031464576721\n",
      "Epoch: 27/100... Step: 5300... Loss: 1.1105... Val Loss: 1.2073 acc:0.6357812285423279\n",
      "Epoch: 27/100... Step: 5310... Loss: 1.1303... Val Loss: 1.2114 acc:0.6341406106948853\n",
      "Epoch: 27/100... Step: 5320... Loss: 1.1039... Val Loss: 1.2095 acc:0.6353906393051147\n",
      "Epoch: 27/100... Step: 5330... Loss: 1.1121... Val Loss: 1.2032 acc:0.6346094012260437\n",
      "Epoch: 27/100... Step: 5340... Loss: 1.1373... Val Loss: 1.2023 acc:0.6350781321525574\n",
      "Epoch: 27/100... Step: 5350... Loss: 1.1323... Val Loss: 1.2091 acc:0.6314843893051147\n",
      "Epoch: 27/100... Step: 5360... Loss: 1.1153... Val Loss: 1.2048 acc:0.6327343583106995\n",
      "Epoch: 27/100... Step: 5370... Loss: 1.1253... Val Loss: 1.2091 acc:0.6321874856948853\n",
      "Epoch: 27/100... Step: 5380... Loss: 1.1321... Val Loss: 1.2103 acc:0.6328906416893005\n",
      "Epoch: 27/100... Step: 5390... Loss: 1.1569... Val Loss: 1.2060 acc:0.6324999928474426\n",
      "Epoch: 27/100... Step: 5400... Loss: 1.1928... Val Loss: 1.2051 acc:0.633984386920929\n",
      "Epoch: 28/100... Step: 5410... Loss: 1.1269... Val Loss: 1.2038 acc:0.6340625286102295\n",
      "Epoch: 28/100... Step: 5420... Loss: 1.1438... Val Loss: 1.2086 acc:0.6332031488418579\n",
      "Epoch: 28/100... Step: 5430... Loss: 1.1053... Val Loss: 1.2067 acc:0.6342187523841858\n",
      "Epoch: 28/100... Step: 5440... Loss: 1.0944... Val Loss: 1.2095 acc:0.6332812309265137\n",
      "Epoch: 28/100... Step: 5450... Loss: 1.1217... Val Loss: 1.2028 acc:0.6370312571525574\n",
      "Epoch: 28/100... Step: 5460... Loss: 1.1203... Val Loss: 1.2044 acc:0.6364062428474426\n",
      "Epoch: 28/100... Step: 5470... Loss: 1.1123... Val Loss: 1.2067 acc:0.6363281011581421\n",
      "Epoch: 28/100... Step: 5480... Loss: 1.1243... Val Loss: 1.2017 acc:0.6361718773841858\n",
      "Epoch: 28/100... Step: 5490... Loss: 1.1111... Val Loss: 1.2026 acc:0.6330468654632568\n",
      "Epoch: 28/100... Step: 5500... Loss: 1.1118... Val Loss: 1.2013 acc:0.6348437666893005\n",
      "Epoch: 28/100... Step: 5510... Loss: 1.1290... Val Loss: 1.2066 acc:0.6365625262260437\n",
      "Epoch: 28/100... Step: 5520... Loss: 1.1002... Val Loss: 1.2067 acc:0.6393749713897705\n",
      "Epoch: 28/100... Step: 5530... Loss: 1.1096... Val Loss: 1.2041 acc:0.635937511920929\n",
      "Epoch: 28/100... Step: 5540... Loss: 1.1335... Val Loss: 1.2031 acc:0.6362500190734863\n",
      "Epoch: 28/100... Step: 5550... Loss: 1.1330... Val Loss: 1.2071 acc:0.6342187523841858\n",
      "Epoch: 28/100... Step: 5560... Loss: 1.1194... Val Loss: 1.2045 acc:0.6350781321525574\n",
      "Epoch: 28/100... Step: 5570... Loss: 1.1196... Val Loss: 1.2099 acc:0.6344531178474426\n",
      "Epoch: 28/100... Step: 5580... Loss: 1.1183... Val Loss: 1.2087 acc:0.6365625262260437\n",
      "Epoch: 28/100... Step: 5590... Loss: 1.1463... Val Loss: 1.2002 acc:0.6370312571525574\n",
      "Epoch: 28/100... Step: 5600... Loss: 1.1786... Val Loss: 1.2036 acc:0.6342187523841858\n",
      "Epoch: 29/100... Step: 5610... Loss: 1.1285... Val Loss: 1.2045 acc:0.6332031488418579\n",
      "Epoch: 29/100... Step: 5620... Loss: 1.1391... Val Loss: 1.2081 acc:0.6362500190734863\n",
      "Epoch: 29/100... Step: 5630... Loss: 1.1017... Val Loss: 1.2074 acc:0.634765625\n",
      "Epoch: 29/100... Step: 5640... Loss: 1.0952... Val Loss: 1.2066 acc:0.6373437643051147\n",
      "Epoch: 29/100... Step: 5650... Loss: 1.1141... Val Loss: 1.2063 acc:0.6379687786102295\n",
      "Epoch: 29/100... Step: 5660... Loss: 1.1202... Val Loss: 1.2038 acc:0.637890636920929\n",
      "Epoch: 29/100... Step: 5670... Loss: 1.1020... Val Loss: 1.2105 acc:0.6364062428474426\n",
      "Epoch: 29/100... Step: 5680... Loss: 1.1157... Val Loss: 1.2086 acc:0.6360156536102295\n",
      "Epoch: 29/100... Step: 5690... Loss: 1.1020... Val Loss: 1.2078 acc:0.6357031464576721\n",
      "Epoch: 29/100... Step: 5700... Loss: 1.1019... Val Loss: 1.2062 acc:0.6360937356948853\n",
      "Epoch: 29/100... Step: 5710... Loss: 1.1214... Val Loss: 1.2082 acc:0.6352343559265137\n",
      "Epoch: 29/100... Step: 5720... Loss: 1.0994... Val Loss: 1.2058 acc:0.6317187547683716\n",
      "Epoch: 29/100... Step: 5730... Loss: 1.1026... Val Loss: 1.2027 acc:0.6372656226158142\n",
      "Epoch: 29/100... Step: 5740... Loss: 1.1253... Val Loss: 1.2032 acc:0.6368749737739563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/100... Step: 5750... Loss: 1.1321... Val Loss: 1.2077 acc:0.6331250071525574\n",
      "Epoch: 29/100... Step: 5760... Loss: 1.1080... Val Loss: 1.2014 acc:0.6367968916893005\n",
      "Epoch: 29/100... Step: 5770... Loss: 1.1161... Val Loss: 1.2224 acc:0.6333593726158142\n",
      "Epoch: 29/100... Step: 5780... Loss: 1.1159... Val Loss: 1.2093 acc:0.6357031464576721\n",
      "Epoch: 29/100... Step: 5790... Loss: 1.1379... Val Loss: 1.2070 acc:0.6327343583106995\n",
      "Epoch: 29/100... Step: 5800... Loss: 1.1848... Val Loss: 1.2027 acc:0.6360156536102295\n",
      "Epoch: 30/100... Step: 5810... Loss: 1.1183... Val Loss: 1.2090 acc:0.6344531178474426\n",
      "Epoch: 30/100... Step: 5820... Loss: 1.1350... Val Loss: 1.2043 acc:0.6392968893051147\n",
      "Epoch: 30/100... Step: 5830... Loss: 1.1016... Val Loss: 1.2074 acc:0.634765625\n",
      "Epoch: 30/100... Step: 5840... Loss: 1.0835... Val Loss: 1.2091 acc:0.6354687213897705\n",
      "Epoch: 30/100... Step: 5850... Loss: 1.1178... Val Loss: 1.2004 acc:0.6360937356948853\n",
      "Epoch: 30/100... Step: 5860... Loss: 1.1179... Val Loss: 1.2059 acc:0.6356250047683716\n",
      "Epoch: 30/100... Step: 5870... Loss: 1.1067... Val Loss: 1.2083 acc:0.6374218463897705\n",
      "Epoch: 30/100... Step: 5880... Loss: 1.1176... Val Loss: 1.2064 acc:0.6387500166893005\n",
      "Epoch: 30/100... Step: 5890... Loss: 1.1035... Val Loss: 1.2055 acc:0.6350781321525574\n",
      "Epoch: 30/100... Step: 5900... Loss: 1.1023... Val Loss: 1.2032 acc:0.635937511920929\n",
      "Epoch: 30/100... Step: 5910... Loss: 1.1133... Val Loss: 1.2122 acc:0.6324218511581421\n",
      "Epoch: 30/100... Step: 5920... Loss: 1.0929... Val Loss: 1.2053 acc:0.6364062428474426\n",
      "Epoch: 30/100... Step: 5930... Loss: 1.1001... Val Loss: 1.2009 acc:0.6361718773841858\n",
      "Epoch: 30/100... Step: 5940... Loss: 1.1235... Val Loss: 1.2013 acc:0.6357031464576721\n",
      "Epoch: 30/100... Step: 5950... Loss: 1.1216... Val Loss: 1.2072 acc:0.6333593726158142\n",
      "Epoch: 30/100... Step: 5960... Loss: 1.1108... Val Loss: 1.2000 acc:0.633984386920929\n",
      "Epoch: 30/100... Step: 5970... Loss: 1.1221... Val Loss: 1.2058 acc:0.6336718797683716\n",
      "Epoch: 30/100... Step: 5980... Loss: 1.1088... Val Loss: 1.2078 acc:0.6338281035423279\n",
      "Epoch: 30/100... Step: 5990... Loss: 1.1363... Val Loss: 1.2007 acc:0.6370312571525574\n",
      "Epoch: 30/100... Step: 6000... Loss: 1.1734... Val Loss: 1.2047 acc:0.6371093988418579\n",
      "Epoch: 31/100... Step: 6010... Loss: 1.1038... Val Loss: 1.2107 acc:0.6340625286102295\n",
      "Epoch: 31/100... Step: 6020... Loss: 1.1282... Val Loss: 1.2061 acc:0.6374218463897705\n",
      "Epoch: 31/100... Step: 6030... Loss: 1.0903... Val Loss: 1.2020 acc:0.6373437643051147\n",
      "Epoch: 31/100... Step: 6040... Loss: 1.0824... Val Loss: 1.2060 acc:0.6364062428474426\n",
      "Epoch: 31/100... Step: 6050... Loss: 1.0988... Val Loss: 1.2045 acc:0.6376562714576721\n",
      "Epoch: 31/100... Step: 6060... Loss: 1.1015... Val Loss: 1.2023 acc:0.6383593678474426\n",
      "Epoch: 31/100... Step: 6070... Loss: 1.1012... Val Loss: 1.2071 acc:0.63671875\n",
      "Epoch: 31/100... Step: 6080... Loss: 1.1118... Val Loss: 1.2052 acc:0.6391406059265137\n",
      "Epoch: 31/100... Step: 6090... Loss: 1.0865... Val Loss: 1.2047 acc:0.6372656226158142\n",
      "Epoch: 31/100... Step: 6100... Loss: 1.0911... Val Loss: 1.2040 acc:0.6382812261581421\n",
      "Epoch: 31/100... Step: 6110... Loss: 1.1121... Val Loss: 1.2104 acc:0.633984386920929\n",
      "Epoch: 31/100... Step: 6120... Loss: 1.0897... Val Loss: 1.2048 acc:0.638671875\n",
      "Epoch: 31/100... Step: 6130... Loss: 1.1045... Val Loss: 1.2000 acc:0.638671875\n",
      "Epoch: 31/100... Step: 6140... Loss: 1.1249... Val Loss: 1.2011 acc:0.6396093964576721\n",
      "Epoch: 31/100... Step: 6150... Loss: 1.1189... Val Loss: 1.2054 acc:0.6344531178474426\n",
      "Epoch: 31/100... Step: 6160... Loss: 1.1083... Val Loss: 1.2004 acc:0.6362500190734863\n",
      "Epoch: 31/100... Step: 6170... Loss: 1.1133... Val Loss: 1.2052 acc:0.6364843845367432\n",
      "Epoch: 31/100... Step: 6180... Loss: 1.1075... Val Loss: 1.2060 acc:0.6371093988418579\n",
      "Epoch: 31/100... Step: 6190... Loss: 1.1348... Val Loss: 1.2004 acc:0.6388280987739563\n",
      "Epoch: 31/100... Step: 6200... Loss: 1.1752... Val Loss: 1.2052 acc:0.6358593702316284\n",
      "Epoch: 32/100... Step: 6210... Loss: 1.1080... Val Loss: 1.2061 acc:0.6349999904632568\n",
      "Epoch: 32/100... Step: 6220... Loss: 1.1271... Val Loss: 1.2068 acc:0.6358593702316284\n",
      "Epoch: 32/100... Step: 6230... Loss: 1.0875... Val Loss: 1.2048 acc:0.6360156536102295\n",
      "Epoch: 32/100... Step: 6240... Loss: 1.0754... Val Loss: 1.2078 acc:0.6375781297683716\n",
      "Epoch: 32/100... Step: 6250... Loss: 1.1023... Val Loss: 1.2055 acc:0.6382031440734863\n",
      "Epoch: 32/100... Step: 6260... Loss: 1.1057... Val Loss: 1.2036 acc:0.6384375095367432\n",
      "Epoch: 32/100... Step: 6270... Loss: 1.0898... Val Loss: 1.2075 acc:0.6353906393051147\n",
      "Epoch: 32/100... Step: 6280... Loss: 1.1013... Val Loss: 1.2038 acc:0.6391406059265137\n",
      "Epoch: 32/100... Step: 6290... Loss: 1.0880... Val Loss: 1.2018 acc:0.6380468606948853\n",
      "Epoch: 32/100... Step: 6300... Loss: 1.0936... Val Loss: 1.2000 acc:0.6384375095367432\n",
      "Epoch: 32/100... Step: 6310... Loss: 1.1080... Val Loss: 1.2064 acc:0.63671875\n",
      "Epoch: 32/100... Step: 6320... Loss: 1.0798... Val Loss: 1.2025 acc:0.6385156512260437\n",
      "Epoch: 32/100... Step: 6330... Loss: 1.0915... Val Loss: 1.2006 acc:0.6373437643051147\n",
      "Epoch: 32/100... Step: 6340... Loss: 1.1159... Val Loss: 1.2028 acc:0.6387500166893005\n",
      "Epoch: 32/100... Step: 6350... Loss: 1.1187... Val Loss: 1.2060 acc:0.6370312571525574\n",
      "Epoch: 32/100... Step: 6360... Loss: 1.1118... Val Loss: 1.2037 acc:0.6346874833106995\n",
      "Epoch: 32/100... Step: 6370... Loss: 1.1117... Val Loss: 1.2106 acc:0.6365625262260437\n",
      "Epoch: 32/100... Step: 6380... Loss: 1.1060... Val Loss: 1.2114 acc:0.637499988079071\n",
      "Epoch: 32/100... Step: 6390... Loss: 1.1245... Val Loss: 1.2036 acc:0.6349999904632568\n",
      "Epoch: 32/100... Step: 6400... Loss: 1.1664... Val Loss: 1.2014 acc:0.6378124952316284\n",
      "Epoch: 33/100... Step: 6410... Loss: 1.1100... Val Loss: 1.2083 acc:0.6329687237739563\n",
      "Epoch: 33/100... Step: 6420... Loss: 1.1194... Val Loss: 1.2035 acc:0.637499988079071\n",
      "Epoch: 33/100... Step: 6430... Loss: 1.0857... Val Loss: 1.2013 acc:0.6368749737739563\n",
      "Epoch: 33/100... Step: 6440... Loss: 1.0734... Val Loss: 1.2034 acc:0.6372656226158142\n",
      "Epoch: 33/100... Step: 6450... Loss: 1.1022... Val Loss: 1.2035 acc:0.6380468606948853\n",
      "Epoch: 33/100... Step: 6460... Loss: 1.1031... Val Loss: 1.2049 acc:0.6360937356948853\n",
      "Epoch: 33/100... Step: 6470... Loss: 1.0841... Val Loss: 1.2064 acc:0.638671875\n",
      "Epoch: 33/100... Step: 6480... Loss: 1.1007... Val Loss: 1.2013 acc:0.639843761920929\n",
      "Epoch: 33/100... Step: 6490... Loss: 1.0834... Val Loss: 1.2023 acc:0.6382812261581421\n",
      "Epoch: 33/100... Step: 6500... Loss: 1.0897... Val Loss: 1.1989 acc:0.638671875\n",
      "Epoch: 33/100... Step: 6510... Loss: 1.1050... Val Loss: 1.2080 acc:0.6334375143051147\n",
      "Epoch: 33/100... Step: 6520... Loss: 1.0865... Val Loss: 1.1996 acc:0.637499988079071\n",
      "Epoch: 33/100... Step: 6530... Loss: 1.0905... Val Loss: 1.2003 acc:0.6378124952316284\n",
      "Epoch: 33/100... Step: 6540... Loss: 1.1084... Val Loss: 1.2045 acc:0.6353124976158142\n",
      "Epoch: 33/100... Step: 6550... Loss: 1.1127... Val Loss: 1.2067 acc:0.63671875\n",
      "Epoch: 33/100... Step: 6560... Loss: 1.1030... Val Loss: 1.2068 acc:0.6348437666893005\n",
      "Epoch: 33/100... Step: 6570... Loss: 1.0999... Val Loss: 1.2110 acc:0.6353124976158142\n",
      "Epoch: 33/100... Step: 6580... Loss: 1.0956... Val Loss: 1.2115 acc:0.6349218487739563\n",
      "Epoch: 33/100... Step: 6590... Loss: 1.1226... Val Loss: 1.2004 acc:0.6383593678474426\n",
      "Epoch: 33/100... Step: 6600... Loss: 1.1687... Val Loss: 1.2023 acc:0.6385156512260437\n",
      "Epoch: 34/100... Step: 6610... Loss: 1.1004... Val Loss: 1.2105 acc:0.6375781297683716\n",
      "Epoch: 34/100... Step: 6620... Loss: 1.1111... Val Loss: 1.2037 acc:0.638671875\n",
      "Epoch: 34/100... Step: 6630... Loss: 1.0811... Val Loss: 1.2018 acc:0.6372656226158142\n",
      "Epoch: 34/100... Step: 6640... Loss: 1.0707... Val Loss: 1.2019 acc:0.6399999856948853\n",
      "Epoch: 34/100... Step: 6650... Loss: 1.0946... Val Loss: 1.2020 acc:0.6410156488418579\n",
      "Epoch: 34/100... Step: 6660... Loss: 1.0944... Val Loss: 1.2037 acc:0.6416406035423279\n",
      "Epoch: 34/100... Step: 6670... Loss: 1.0852... Val Loss: 1.2069 acc:0.6381250023841858\n",
      "Epoch: 34/100... Step: 6680... Loss: 1.0966... Val Loss: 1.2023 acc:0.6412500143051147\n",
      "Epoch: 34/100... Step: 6690... Loss: 1.0801... Val Loss: 1.2059 acc:0.637890636920929\n",
      "Epoch: 34/100... Step: 6700... Loss: 1.0832... Val Loss: 1.2014 acc:0.6358593702316284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34/100... Step: 6710... Loss: 1.1036... Val Loss: 1.2043 acc:0.6392187476158142\n",
      "Epoch: 34/100... Step: 6720... Loss: 1.0793... Val Loss: 1.2019 acc:0.6407812237739563\n",
      "Epoch: 34/100... Step: 6730... Loss: 1.0804... Val Loss: 1.2016 acc:0.6353906393051147\n",
      "Epoch: 34/100... Step: 6740... Loss: 1.1122... Val Loss: 1.2009 acc:0.6385937333106995\n",
      "Epoch: 34/100... Step: 6750... Loss: 1.1155... Val Loss: 1.2105 acc:0.6382812261581421\n",
      "Epoch: 34/100... Step: 6760... Loss: 1.1063... Val Loss: 1.2072 acc:0.6352343559265137\n",
      "Epoch: 34/100... Step: 6770... Loss: 1.1072... Val Loss: 1.2128 acc:0.6360937356948853\n",
      "Epoch: 34/100... Step: 6780... Loss: 1.0983... Val Loss: 1.2113 acc:0.6362500190734863\n",
      "Epoch: 34/100... Step: 6790... Loss: 1.1183... Val Loss: 1.2021 acc:0.6391406059265137\n",
      "Epoch: 34/100... Step: 6800... Loss: 1.1604... Val Loss: 1.2011 acc:0.637890636920929\n",
      "Epoch: 35/100... Step: 6810... Loss: 1.0942... Val Loss: 1.2074 acc:0.6368749737739563\n",
      "Epoch: 35/100... Step: 6820... Loss: 1.1107... Val Loss: 1.2033 acc:0.6385156512260437\n",
      "Epoch: 35/100... Step: 6830... Loss: 1.0710... Val Loss: 1.1999 acc:0.6385156512260437\n",
      "Epoch: 35/100... Step: 6840... Loss: 1.0641... Val Loss: 1.2052 acc:0.6356250047683716\n",
      "Epoch: 35/100... Step: 6850... Loss: 1.0941... Val Loss: 1.2045 acc:0.6376562714576721\n",
      "Epoch: 35/100... Step: 6860... Loss: 1.0873... Val Loss: 1.2039 acc:0.640625\n",
      "Epoch: 35/100... Step: 6870... Loss: 1.0857... Val Loss: 1.2074 acc:0.6368749737739563\n",
      "Epoch: 35/100... Step: 6880... Loss: 1.0942... Val Loss: 1.2031 acc:0.6403124928474426\n",
      "Epoch: 35/100... Step: 6890... Loss: 1.0758... Val Loss: 1.2043 acc:0.6397656202316284\n",
      "Epoch: 35/100... Step: 6900... Loss: 1.0775... Val Loss: 1.1993 acc:0.6415625214576721\n",
      "Epoch: 35/100... Step: 6910... Loss: 1.0929... Val Loss: 1.2040 acc:0.6391406059265137\n",
      "Epoch: 35/100... Step: 6920... Loss: 1.0751... Val Loss: 1.1992 acc:0.6407812237739563\n",
      "Epoch: 35/100... Step: 6930... Loss: 1.0901... Val Loss: 1.1999 acc:0.6389843821525574\n",
      "Epoch: 35/100... Step: 6940... Loss: 1.1133... Val Loss: 1.2041 acc:0.6389062404632568\n",
      "Epoch: 35/100... Step: 6950... Loss: 1.0993... Val Loss: 1.2083 acc:0.6370312571525574\n",
      "Epoch: 35/100... Step: 6960... Loss: 1.0985... Val Loss: 1.2072 acc:0.6346874833106995\n",
      "Epoch: 35/100... Step: 6970... Loss: 1.0940... Val Loss: 1.2112 acc:0.6362500190734863\n",
      "Epoch: 35/100... Step: 6980... Loss: 1.0983... Val Loss: 1.2119 acc:0.6345312595367432\n",
      "Epoch: 35/100... Step: 6990... Loss: 1.1150... Val Loss: 1.2031 acc:0.637890636920929\n",
      "Epoch: 35/100... Step: 7000... Loss: 1.1601... Val Loss: 1.2041 acc:0.6375781297683716\n",
      "Epoch: 36/100... Step: 7010... Loss: 1.1024... Val Loss: 1.2088 acc:0.6357812285423279\n",
      "Epoch: 36/100... Step: 7020... Loss: 1.1102... Val Loss: 1.2020 acc:0.6393749713897705\n",
      "Epoch: 36/100... Step: 7030... Loss: 1.0693... Val Loss: 1.2007 acc:0.6373437643051147\n",
      "Epoch: 36/100... Step: 7040... Loss: 1.0606... Val Loss: 1.2048 acc:0.6373437643051147\n",
      "Epoch: 36/100... Step: 7050... Loss: 1.0838... Val Loss: 1.2028 acc:0.6402343511581421\n",
      "Epoch: 36/100... Step: 7060... Loss: 1.0894... Val Loss: 1.2018 acc:0.6413280963897705\n",
      "Epoch: 36/100... Step: 7070... Loss: 1.0765... Val Loss: 1.2063 acc:0.6380468606948853\n",
      "Epoch: 36/100... Step: 7080... Loss: 1.0927... Val Loss: 1.2040 acc:0.637499988079071\n",
      "Epoch: 36/100... Step: 7090... Loss: 1.0757... Val Loss: 1.2058 acc:0.6349999904632568\n",
      "Epoch: 36/100... Step: 7100... Loss: 1.0757... Val Loss: 1.1996 acc:0.6387500166893005\n",
      "Epoch: 36/100... Step: 7110... Loss: 1.0930... Val Loss: 1.2040 acc:0.6367968916893005\n",
      "Epoch: 36/100... Step: 7120... Loss: 1.0712... Val Loss: 1.2049 acc:0.6353906393051147\n",
      "Epoch: 36/100... Step: 7130... Loss: 1.0784... Val Loss: 1.2038 acc:0.6370312571525574\n",
      "Epoch: 36/100... Step: 7140... Loss: 1.1124... Val Loss: 1.2042 acc:0.6372656226158142\n",
      "Epoch: 36/100... Step: 7150... Loss: 1.0965... Val Loss: 1.2060 acc:0.6375781297683716\n",
      "Epoch: 36/100... Step: 7160... Loss: 1.0912... Val Loss: 1.2025 acc:0.6388280987739563\n",
      "Epoch: 36/100... Step: 7170... Loss: 1.0858... Val Loss: 1.2150 acc:0.6362500190734863\n",
      "Epoch: 36/100... Step: 7180... Loss: 1.0909... Val Loss: 1.2102 acc:0.635937511920929\n",
      "Epoch: 36/100... Step: 7190... Loss: 1.1172... Val Loss: 1.1998 acc:0.6372656226158142\n",
      "Epoch: 36/100... Step: 7200... Loss: 1.1569... Val Loss: 1.2036 acc:0.635937511920929\n",
      "Epoch: 37/100... Step: 7210... Loss: 1.0900... Val Loss: 1.2127 acc:0.6328125\n",
      "Epoch: 37/100... Step: 7220... Loss: 1.1086... Val Loss: 1.2048 acc:0.6364843845367432\n",
      "Epoch: 37/100... Step: 7230... Loss: 1.0800... Val Loss: 1.2014 acc:0.6380468606948853\n",
      "Epoch: 37/100... Step: 7240... Loss: 1.0630... Val Loss: 1.2027 acc:0.6396093964576721\n",
      "Epoch: 37/100... Step: 7250... Loss: 1.0827... Val Loss: 1.2008 acc:0.6395312547683716\n",
      "Epoch: 37/100... Step: 7260... Loss: 1.0787... Val Loss: 1.2008 acc:0.637890636920929\n",
      "Epoch: 37/100... Step: 7270... Loss: 1.0769... Val Loss: 1.2032 acc:0.6390625238418579\n",
      "Epoch: 37/100... Step: 7280... Loss: 1.0878... Val Loss: 1.2018 acc:0.6393749713897705\n",
      "Epoch: 37/100... Step: 7290... Loss: 1.0588... Val Loss: 1.2046 acc:0.637499988079071\n",
      "Epoch: 37/100... Step: 7300... Loss: 1.0757... Val Loss: 1.1988 acc:0.6403124928474426\n",
      "Epoch: 37/100... Step: 7310... Loss: 1.0865... Val Loss: 1.2064 acc:0.6387500166893005\n",
      "Epoch: 37/100... Step: 7320... Loss: 1.0690... Val Loss: 1.2008 acc:0.6395312547683716\n",
      "Epoch: 37/100... Step: 7330... Loss: 1.0669... Val Loss: 1.1976 acc:0.6390625238418579\n",
      "Epoch: 37/100... Step: 7340... Loss: 1.0926... Val Loss: 1.2031 acc:0.6372656226158142\n",
      "Epoch: 37/100... Step: 7350... Loss: 1.0989... Val Loss: 1.2066 acc:0.6381250023841858\n",
      "Epoch: 37/100... Step: 7360... Loss: 1.0892... Val Loss: 1.2059 acc:0.6350781321525574\n",
      "Epoch: 37/100... Step: 7370... Loss: 1.0885... Val Loss: 1.2100 acc:0.6377343535423279\n",
      "Epoch: 37/100... Step: 7380... Loss: 1.0912... Val Loss: 1.2061 acc:0.6373437643051147\n",
      "Epoch: 37/100... Step: 7390... Loss: 1.1040... Val Loss: 1.1990 acc:0.6392968893051147\n",
      "Epoch: 37/100... Step: 7400... Loss: 1.1446... Val Loss: 1.2013 acc:0.6371874809265137\n",
      "Epoch: 38/100... Step: 7410... Loss: 1.0764... Val Loss: 1.2074 acc:0.6379687786102295\n",
      "Epoch: 38/100... Step: 7420... Loss: 1.0974... Val Loss: 1.2015 acc:0.6399219036102295\n",
      "Epoch: 38/100... Step: 7430... Loss: 1.0732... Val Loss: 1.2006 acc:0.6373437643051147\n",
      "Epoch: 38/100... Step: 7440... Loss: 1.0530... Val Loss: 1.2017 acc:0.639453113079071\n",
      "Epoch: 38/100... Step: 7450... Loss: 1.0722... Val Loss: 1.2019 acc:0.6400781273841858\n",
      "Epoch: 38/100... Step: 7460... Loss: 1.0902... Val Loss: 1.2060 acc:0.6343749761581421\n",
      "Epoch: 38/100... Step: 7470... Loss: 1.0741... Val Loss: 1.2039 acc:0.6392968893051147\n",
      "Epoch: 38/100... Step: 7480... Loss: 1.0854... Val Loss: 1.2031 acc:0.6407812237739563\n",
      "Epoch: 38/100... Step: 7490... Loss: 1.0658... Val Loss: 1.2055 acc:0.635937511920929\n",
      "Epoch: 38/100... Step: 7500... Loss: 1.0734... Val Loss: 1.2021 acc:0.6388280987739563\n",
      "Epoch: 38/100... Step: 7510... Loss: 1.0862... Val Loss: 1.2058 acc:0.641406238079071\n",
      "Epoch: 38/100... Step: 7520... Loss: 1.0622... Val Loss: 1.2028 acc:0.6378124952316284\n",
      "Epoch: 38/100... Step: 7530... Loss: 1.0757... Val Loss: 1.1978 acc:0.6404687762260437\n",
      "Epoch: 38/100... Step: 7540... Loss: 1.0973... Val Loss: 1.2031 acc:0.6396874785423279\n",
      "Epoch: 38/100... Step: 7550... Loss: 1.0935... Val Loss: 1.2046 acc:0.6407031416893005\n",
      "Epoch: 38/100... Step: 7560... Loss: 1.0853... Val Loss: 1.2048 acc:0.637499988079071\n",
      "Epoch: 38/100... Step: 7570... Loss: 1.0809... Val Loss: 1.2087 acc:0.6400781273841858\n",
      "Epoch: 38/100... Step: 7580... Loss: 1.0822... Val Loss: 1.2043 acc:0.6383593678474426\n",
      "Epoch: 38/100... Step: 7590... Loss: 1.1027... Val Loss: 1.1986 acc:0.6383593678474426\n",
      "Epoch: 38/100... Step: 7600... Loss: 1.1462... Val Loss: 1.2013 acc:0.6383593678474426\n",
      "Epoch: 39/100... Step: 7610... Loss: 1.0804... Val Loss: 1.2067 acc:0.6385156512260437\n",
      "Epoch: 39/100... Step: 7620... Loss: 1.0987... Val Loss: 1.2048 acc:0.6368749737739563\n",
      "Epoch: 39/100... Step: 7630... Loss: 1.0631... Val Loss: 1.2030 acc:0.6392968893051147\n",
      "Epoch: 39/100... Step: 7640... Loss: 1.0490... Val Loss: 1.2004 acc:0.6397656202316284\n",
      "Epoch: 39/100... Step: 7650... Loss: 1.0765... Val Loss: 1.2003 acc:0.6389843821525574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39/100... Step: 7660... Loss: 1.0740... Val Loss: 1.2015 acc:0.6396093964576721\n",
      "Epoch: 39/100... Step: 7670... Loss: 1.0665... Val Loss: 1.2062 acc:0.639453113079071\n",
      "Epoch: 39/100... Step: 7680... Loss: 1.0789... Val Loss: 1.2051 acc:0.639843761920929\n",
      "Epoch: 39/100... Step: 7690... Loss: 1.0615... Val Loss: 1.2032 acc:0.637499988079071\n",
      "Epoch: 39/100... Step: 7700... Loss: 1.0676... Val Loss: 1.2016 acc:0.6389843821525574\n",
      "Epoch: 39/100... Step: 7710... Loss: 1.0749... Val Loss: 1.2051 acc:0.6392968893051147\n",
      "Epoch: 39/100... Step: 7720... Loss: 1.0634... Val Loss: 1.1996 acc:0.6407031416893005\n",
      "Epoch: 39/100... Step: 7730... Loss: 1.0659... Val Loss: 1.1975 acc:0.6397656202316284\n",
      "Epoch: 39/100... Step: 7740... Loss: 1.0905... Val Loss: 1.2031 acc:0.638671875\n",
      "Epoch: 39/100... Step: 7750... Loss: 1.0928... Val Loss: 1.2083 acc:0.6382031440734863\n",
      "Epoch: 39/100... Step: 7760... Loss: 1.0841... Val Loss: 1.2049 acc:0.6381250023841858\n",
      "Epoch: 39/100... Step: 7770... Loss: 1.0816... Val Loss: 1.2092 acc:0.6369531154632568\n",
      "Epoch: 39/100... Step: 7780... Loss: 1.0873... Val Loss: 1.2051 acc:0.6395312547683716\n",
      "Epoch: 39/100... Step: 7790... Loss: 1.1084... Val Loss: 1.1995 acc:0.6379687786102295\n",
      "Epoch: 39/100... Step: 7800... Loss: 1.1360... Val Loss: 1.2012 acc:0.6395312547683716\n",
      "Epoch: 40/100... Step: 7810... Loss: 1.0721... Val Loss: 1.2079 acc:0.6378124952316284\n",
      "Epoch: 40/100... Step: 7820... Loss: 1.0919... Val Loss: 1.2023 acc:0.6389062404632568\n",
      "Epoch: 40/100... Step: 7830... Loss: 1.0650... Val Loss: 1.1991 acc:0.6399999856948853\n",
      "Epoch: 40/100... Step: 7840... Loss: 1.0520... Val Loss: 1.2040 acc:0.6390625238418579\n",
      "Epoch: 40/100... Step: 7850... Loss: 1.0742... Val Loss: 1.2014 acc:0.641406238079071\n",
      "Epoch: 40/100... Step: 7860... Loss: 1.0779... Val Loss: 1.2025 acc:0.638671875\n",
      "Epoch: 40/100... Step: 7870... Loss: 1.0614... Val Loss: 1.2073 acc:0.6383593678474426\n",
      "Epoch: 40/100... Step: 7880... Loss: 1.0765... Val Loss: 1.2030 acc:0.6410156488418579\n",
      "Epoch: 40/100... Step: 7890... Loss: 1.0592... Val Loss: 1.2035 acc:0.6364843845367432\n",
      "Epoch: 40/100... Step: 7900... Loss: 1.0686... Val Loss: 1.2011 acc:0.6392187476158142\n",
      "Epoch: 40/100... Step: 7910... Loss: 1.0777... Val Loss: 1.2074 acc:0.6381250023841858\n",
      "Epoch: 40/100... Step: 7920... Loss: 1.0595... Val Loss: 1.2006 acc:0.6399999856948853\n",
      "Epoch: 40/100... Step: 7930... Loss: 1.0643... Val Loss: 1.1980 acc:0.6392187476158142\n",
      "Epoch: 40/100... Step: 7940... Loss: 1.0878... Val Loss: 1.2044 acc:0.6372656226158142\n",
      "Epoch: 40/100... Step: 7950... Loss: 1.0828... Val Loss: 1.2086 acc:0.6375781297683716\n",
      "Epoch: 40/100... Step: 7960... Loss: 1.0826... Val Loss: 1.2050 acc:0.635937511920929\n",
      "Epoch: 40/100... Step: 7970... Loss: 1.0769... Val Loss: 1.2134 acc:0.6361718773841858\n",
      "Epoch: 40/100... Step: 7980... Loss: 1.0796... Val Loss: 1.2056 acc:0.6389062404632568\n",
      "Epoch: 40/100... Step: 7990... Loss: 1.0933... Val Loss: 1.2008 acc:0.6401562690734863\n",
      "Epoch: 40/100... Step: 8000... Loss: 1.1393... Val Loss: 1.2027 acc:0.6399999856948853\n",
      "Epoch: 41/100... Step: 8010... Loss: 1.0875... Val Loss: 1.2198 acc:0.6353906393051147\n",
      "Epoch: 41/100... Step: 8020... Loss: 1.0917... Val Loss: 1.2022 acc:0.6392187476158142\n",
      "Epoch: 41/100... Step: 8030... Loss: 1.0561... Val Loss: 1.2039 acc:0.6403124928474426\n",
      "Epoch: 41/100... Step: 8040... Loss: 1.0393... Val Loss: 1.2045 acc:0.639843761920929\n",
      "Epoch: 41/100... Step: 8050... Loss: 1.0693... Val Loss: 1.2052 acc:0.6417187452316284\n",
      "Epoch: 41/100... Step: 8060... Loss: 1.0677... Val Loss: 1.2012 acc:0.6435937285423279\n",
      "Epoch: 41/100... Step: 8070... Loss: 1.0599... Val Loss: 1.2059 acc:0.639843761920929\n",
      "Epoch: 41/100... Step: 8080... Loss: 1.0826... Val Loss: 1.2025 acc:0.640625\n",
      "Epoch: 41/100... Step: 8090... Loss: 1.0550... Val Loss: 1.2066 acc:0.6373437643051147\n",
      "Epoch: 41/100... Step: 8100... Loss: 1.0603... Val Loss: 1.1986 acc:0.6402343511581421\n",
      "Epoch: 41/100... Step: 8110... Loss: 1.0765... Val Loss: 1.2098 acc:0.6396874785423279\n",
      "Epoch: 41/100... Step: 8120... Loss: 1.0501... Val Loss: 1.2025 acc:0.6367968916893005\n",
      "Epoch: 41/100... Step: 8130... Loss: 1.0649... Val Loss: 1.2008 acc:0.6401562690734863\n",
      "Epoch: 41/100... Step: 8140... Loss: 1.0880... Val Loss: 1.2075 acc:0.6360156536102295\n",
      "Epoch: 41/100... Step: 8150... Loss: 1.0823... Val Loss: 1.2119 acc:0.6379687786102295\n",
      "Epoch: 41/100... Step: 8160... Loss: 1.0816... Val Loss: 1.2030 acc:0.6389843821525574\n",
      "Epoch: 41/100... Step: 8170... Loss: 1.0710... Val Loss: 1.2117 acc:0.6362500190734863\n",
      "Epoch: 41/100... Step: 8180... Loss: 1.0739... Val Loss: 1.2055 acc:0.637890636920929\n",
      "Epoch: 41/100... Step: 8190... Loss: 1.0940... Val Loss: 1.2022 acc:0.6382812261581421\n",
      "Epoch: 41/100... Step: 8200... Loss: 1.1328... Val Loss: 1.2025 acc:0.639453113079071\n",
      "Epoch: 42/100... Step: 8210... Loss: 1.0739... Val Loss: 1.2067 acc:0.6362500190734863\n",
      "Epoch: 42/100... Step: 8220... Loss: 1.0959... Val Loss: 1.2021 acc:0.6392968893051147\n",
      "Epoch: 42/100... Step: 8230... Loss: 1.0526... Val Loss: 1.2031 acc:0.6389843821525574\n",
      "Epoch: 42/100... Step: 8240... Loss: 1.0496... Val Loss: 1.2046 acc:0.6385937333106995\n",
      "Epoch: 42/100... Step: 8250... Loss: 1.0625... Val Loss: 1.2031 acc:0.6428124904632568\n",
      "Epoch: 42/100... Step: 8260... Loss: 1.0681... Val Loss: 1.2031 acc:0.6390625238418579\n",
      "Epoch: 42/100... Step: 8270... Loss: 1.0616... Val Loss: 1.2079 acc:0.6389062404632568\n",
      "Epoch: 42/100... Step: 8280... Loss: 1.0735... Val Loss: 1.2052 acc:0.6392968893051147\n",
      "Epoch: 42/100... Step: 8290... Loss: 1.0596... Val Loss: 1.2034 acc:0.6374218463897705\n",
      "Epoch: 42/100... Step: 8300... Loss: 1.0547... Val Loss: 1.2002 acc:0.6402343511581421\n",
      "Epoch: 42/100... Step: 8310... Loss: 1.0782... Val Loss: 1.2090 acc:0.6379687786102295\n",
      "Epoch: 42/100... Step: 8320... Loss: 1.0495... Val Loss: 1.2025 acc:0.6407812237739563\n",
      "Epoch: 42/100... Step: 8330... Loss: 1.0582... Val Loss: 1.2015 acc:0.6420312523841858\n",
      "Epoch: 42/100... Step: 8340... Loss: 1.0756... Val Loss: 1.2026 acc:0.6413280963897705\n",
      "Epoch: 42/100... Step: 8350... Loss: 1.0719... Val Loss: 1.2125 acc:0.6391406059265137\n",
      "Epoch: 42/100... Step: 8360... Loss: 1.0756... Val Loss: 1.2054 acc:0.6388280987739563\n",
      "Epoch: 42/100... Step: 8370... Loss: 1.0768... Val Loss: 1.2084 acc:0.6414843797683716\n",
      "Epoch: 42/100... Step: 8380... Loss: 1.0759... Val Loss: 1.2048 acc:0.6402343511581421\n",
      "Epoch: 42/100... Step: 8390... Loss: 1.0848... Val Loss: 1.1993 acc:0.6384375095367432\n",
      "Epoch: 42/100... Step: 8400... Loss: 1.1301... Val Loss: 1.2029 acc:0.6377343535423279\n",
      "Epoch: 43/100... Step: 8410... Loss: 1.0788... Val Loss: 1.2161 acc:0.638671875\n",
      "Epoch: 43/100... Step: 8420... Loss: 1.0832... Val Loss: 1.2063 acc:0.6389062404632568\n",
      "Epoch: 43/100... Step: 8430... Loss: 1.0515... Val Loss: 1.2034 acc:0.6407812237739563\n",
      "Epoch: 43/100... Step: 8440... Loss: 1.0503... Val Loss: 1.2063 acc:0.6396093964576721\n",
      "Epoch: 43/100... Step: 8450... Loss: 1.0603... Val Loss: 1.2057 acc:0.6447656154632568\n",
      "Epoch: 43/100... Step: 8460... Loss: 1.0650... Val Loss: 1.2021 acc:0.6423437595367432\n",
      "Epoch: 43/100... Step: 8470... Loss: 1.0489... Val Loss: 1.2095 acc:0.6400781273841858\n",
      "Epoch: 43/100... Step: 8480... Loss: 1.0750... Val Loss: 1.2100 acc:0.6407031416893005\n",
      "Epoch: 43/100... Step: 8490... Loss: 1.0583... Val Loss: 1.2060 acc:0.6390625238418579\n",
      "Epoch: 43/100... Step: 8500... Loss: 1.0609... Val Loss: 1.2023 acc:0.6383593678474426\n",
      "Epoch: 43/100... Step: 8510... Loss: 1.0745... Val Loss: 1.2187 acc:0.6375781297683716\n",
      "Epoch: 43/100... Step: 8520... Loss: 1.0466... Val Loss: 1.2053 acc:0.640625\n",
      "Epoch: 43/100... Step: 8530... Loss: 1.0536... Val Loss: 1.2037 acc:0.6429687738418579\n",
      "Epoch: 43/100... Step: 8540... Loss: 1.0777... Val Loss: 1.2053 acc:0.6403124928474426\n",
      "Epoch: 43/100... Step: 8550... Loss: 1.0788... Val Loss: 1.2098 acc:0.6393749713897705\n",
      "Epoch: 43/100... Step: 8560... Loss: 1.0703... Val Loss: 1.2037 acc:0.6390625238418579\n",
      "Epoch: 43/100... Step: 8570... Loss: 1.0672... Val Loss: 1.2063 acc:0.6393749713897705\n",
      "Epoch: 43/100... Step: 8580... Loss: 1.0681... Val Loss: 1.2067 acc:0.6399219036102295\n",
      "Epoch: 43/100... Step: 8590... Loss: 1.0911... Val Loss: 1.2008 acc:0.6392968893051147\n",
      "Epoch: 43/100... Step: 8600... Loss: 1.1376... Val Loss: 1.2049 acc:0.6382812261581421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44/100... Step: 8610... Loss: 1.0727... Val Loss: 1.2122 acc:0.6379687786102295\n",
      "Epoch: 44/100... Step: 8620... Loss: 1.0718... Val Loss: 1.2046 acc:0.6401562690734863\n",
      "Epoch: 44/100... Step: 8630... Loss: 1.0481... Val Loss: 1.2020 acc:0.6407812237739563\n",
      "Epoch: 44/100... Step: 8640... Loss: 1.0433... Val Loss: 1.2101 acc:0.6380468606948853\n",
      "Epoch: 44/100... Step: 8650... Loss: 1.0658... Val Loss: 1.2067 acc:0.6396093964576721\n",
      "Epoch: 44/100... Step: 8660... Loss: 1.0655... Val Loss: 1.2036 acc:0.6404687762260437\n",
      "Epoch: 44/100... Step: 8670... Loss: 1.0541... Val Loss: 1.2060 acc:0.6393749713897705\n",
      "Epoch: 44/100... Step: 8680... Loss: 1.0677... Val Loss: 1.2081 acc:0.6390625238418579\n",
      "Epoch: 44/100... Step: 8690... Loss: 1.0430... Val Loss: 1.2042 acc:0.6384375095367432\n",
      "Epoch: 44/100... Step: 8700... Loss: 1.0586... Val Loss: 1.2006 acc:0.6389843821525574\n",
      "Epoch: 44/100... Step: 8710... Loss: 1.0682... Val Loss: 1.2145 acc:0.6397656202316284\n",
      "Epoch: 44/100... Step: 8720... Loss: 1.0486... Val Loss: 1.2026 acc:0.6403124928474426\n",
      "Epoch: 44/100... Step: 8730... Loss: 1.0571... Val Loss: 1.2055 acc:0.6405468583106995\n",
      "Epoch: 44/100... Step: 8740... Loss: 1.0799... Val Loss: 1.2041 acc:0.6396874785423279\n",
      "Epoch: 44/100... Step: 8750... Loss: 1.0754... Val Loss: 1.2123 acc:0.6379687786102295\n",
      "Epoch: 44/100... Step: 8760... Loss: 1.0720... Val Loss: 1.2053 acc:0.6362500190734863\n",
      "Epoch: 44/100... Step: 8770... Loss: 1.0734... Val Loss: 1.2074 acc:0.6376562714576721\n",
      "Epoch: 44/100... Step: 8780... Loss: 1.0638... Val Loss: 1.2070 acc:0.6371093988418579\n",
      "Epoch: 44/100... Step: 8790... Loss: 1.0921... Val Loss: 1.1991 acc:0.6402343511581421\n",
      "Epoch: 44/100... Step: 8800... Loss: 1.1273... Val Loss: 1.2020 acc:0.6412500143051147\n",
      "Epoch: 45/100... Step: 8810... Loss: 1.0710... Val Loss: 1.2147 acc:0.6387500166893005\n",
      "Epoch: 45/100... Step: 8820... Loss: 1.0731... Val Loss: 1.2044 acc:0.6414843797683716\n",
      "Epoch: 45/100... Step: 8830... Loss: 1.0431... Val Loss: 1.2040 acc:0.6435937285423279\n",
      "Epoch: 45/100... Step: 8840... Loss: 1.0357... Val Loss: 1.2091 acc:0.641406238079071\n",
      "Epoch: 45/100... Step: 8850... Loss: 1.0601... Val Loss: 1.2033 acc:0.6429687738418579\n",
      "Epoch: 45/100... Step: 8860... Loss: 1.0661... Val Loss: 1.2012 acc:0.6410156488418579\n",
      "Epoch: 45/100... Step: 8870... Loss: 1.0466... Val Loss: 1.2102 acc:0.639843761920929\n",
      "Epoch: 45/100... Step: 8880... Loss: 1.0590... Val Loss: 1.2087 acc:0.6396874785423279\n",
      "Epoch: 45/100... Step: 8890... Loss: 1.0487... Val Loss: 1.2055 acc:0.6380468606948853\n",
      "Epoch: 45/100... Step: 8900... Loss: 1.0559... Val Loss: 1.2039 acc:0.6414843797683716\n",
      "Epoch: 45/100... Step: 8910... Loss: 1.0751... Val Loss: 1.2090 acc:0.641406238079071\n",
      "Epoch: 45/100... Step: 8920... Loss: 1.0443... Val Loss: 1.2040 acc:0.6407031416893005\n",
      "Epoch: 45/100... Step: 8930... Loss: 1.0533... Val Loss: 1.2065 acc:0.6378124952316284\n",
      "Epoch: 45/100... Step: 8940... Loss: 1.0698... Val Loss: 1.2080 acc:0.6392968893051147\n",
      "Epoch: 45/100... Step: 8950... Loss: 1.0730... Val Loss: 1.2140 acc:0.6400781273841858\n",
      "Epoch: 45/100... Step: 8960... Loss: 1.0619... Val Loss: 1.2072 acc:0.6376562714576721\n",
      "Epoch: 45/100... Step: 8970... Loss: 1.0652... Val Loss: 1.2063 acc:0.639843761920929\n",
      "Epoch: 45/100... Step: 8980... Loss: 1.0664... Val Loss: 1.2083 acc:0.6397656202316284\n",
      "Epoch: 45/100... Step: 8990... Loss: 1.0866... Val Loss: 1.2024 acc:0.6432031393051147\n",
      "Epoch: 45/100... Step: 9000... Loss: 1.1242... Val Loss: 1.2077 acc:0.6401562690734863\n",
      "Epoch: 46/100... Step: 9010... Loss: 1.0619... Val Loss: 1.2184 acc:0.639453113079071\n",
      "Epoch: 46/100... Step: 9020... Loss: 1.0750... Val Loss: 1.2031 acc:0.6400781273841858\n",
      "Epoch: 46/100... Step: 9030... Loss: 1.0377... Val Loss: 1.2027 acc:0.642578125\n",
      "Epoch: 46/100... Step: 9040... Loss: 1.0255... Val Loss: 1.2086 acc:0.6389843821525574\n",
      "Epoch: 46/100... Step: 9050... Loss: 1.0605... Val Loss: 1.2039 acc:0.6414843797683716\n",
      "Epoch: 46/100... Step: 9060... Loss: 1.0577... Val Loss: 1.2022 acc:0.6413280963897705\n",
      "Epoch: 46/100... Step: 9070... Loss: 1.0481... Val Loss: 1.2119 acc:0.6389062404632568\n",
      "Epoch: 46/100... Step: 9080... Loss: 1.0595... Val Loss: 1.2122 acc:0.6405468583106995\n",
      "Epoch: 46/100... Step: 9090... Loss: 1.0392... Val Loss: 1.2034 acc:0.6395312547683716\n",
      "Epoch: 46/100... Step: 9100... Loss: 1.0425... Val Loss: 1.2015 acc:0.6409375071525574\n",
      "Epoch: 46/100... Step: 9110... Loss: 1.0624... Val Loss: 1.2143 acc:0.6409375071525574\n",
      "Epoch: 46/100... Step: 9120... Loss: 1.0438... Val Loss: 1.2032 acc:0.6378124952316284\n",
      "Epoch: 46/100... Step: 9130... Loss: 1.0474... Val Loss: 1.2077 acc:0.6388280987739563\n",
      "Epoch: 46/100... Step: 9140... Loss: 1.0686... Val Loss: 1.2075 acc:0.6403124928474426\n",
      "Epoch: 46/100... Step: 9150... Loss: 1.0659... Val Loss: 1.2135 acc:0.6410937309265137\n",
      "Epoch: 46/100... Step: 9160... Loss: 1.0595... Val Loss: 1.2076 acc:0.6379687786102295\n",
      "Epoch: 46/100... Step: 9170... Loss: 1.0587... Val Loss: 1.2097 acc:0.6391406059265137\n",
      "Epoch: 46/100... Step: 9180... Loss: 1.0602... Val Loss: 1.2083 acc:0.6382031440734863\n",
      "Epoch: 46/100... Step: 9190... Loss: 1.0865... Val Loss: 1.2032 acc:0.6405468583106995\n",
      "Epoch: 46/100... Step: 9200... Loss: 1.1311... Val Loss: 1.2046 acc:0.6413280963897705\n",
      "Epoch: 47/100... Step: 9210... Loss: 1.0603... Val Loss: 1.2131 acc:0.6393749713897705\n",
      "Epoch: 47/100... Step: 9220... Loss: 1.0739... Val Loss: 1.2034 acc:0.6421874761581421\n",
      "Epoch: 47/100... Step: 9230... Loss: 1.0281... Val Loss: 1.2059 acc:0.6411718726158142\n",
      "Epoch: 47/100... Step: 9240... Loss: 1.0352... Val Loss: 1.2078 acc:0.6421874761581421\n",
      "Epoch: 47/100... Step: 9250... Loss: 1.0506... Val Loss: 1.2069 acc:0.6415625214576721\n",
      "Epoch: 47/100... Step: 9260... Loss: 1.0568... Val Loss: 1.2063 acc:0.6403906345367432\n",
      "Epoch: 47/100... Step: 9270... Loss: 1.0469... Val Loss: 1.2107 acc:0.638671875\n",
      "Epoch: 47/100... Step: 9280... Loss: 1.0573... Val Loss: 1.2074 acc:0.6378124952316284\n",
      "Epoch: 47/100... Step: 9290... Loss: 1.0401... Val Loss: 1.2031 acc:0.6410937309265137\n",
      "Epoch: 47/100... Step: 9300... Loss: 1.0466... Val Loss: 1.2072 acc:0.6405468583106995\n",
      "Epoch: 47/100... Step: 9310... Loss: 1.0587... Val Loss: 1.2177 acc:0.6376562714576721\n",
      "Epoch: 47/100... Step: 9320... Loss: 1.0336... Val Loss: 1.2076 acc:0.6410937309265137\n",
      "Epoch: 47/100... Step: 9330... Loss: 1.0453... Val Loss: 1.2114 acc:0.640625\n",
      "Epoch: 47/100... Step: 9340... Loss: 1.0668... Val Loss: 1.2061 acc:0.6397656202316284\n",
      "Epoch: 47/100... Step: 9350... Loss: 1.0586... Val Loss: 1.2135 acc:0.6397656202316284\n",
      "Epoch: 47/100... Step: 9360... Loss: 1.0660... Val Loss: 1.2042 acc:0.6378124952316284\n",
      "Epoch: 47/100... Step: 9370... Loss: 1.0543... Val Loss: 1.2087 acc:0.6399219036102295\n",
      "Epoch: 47/100... Step: 9380... Loss: 1.0555... Val Loss: 1.2103 acc:0.6392968893051147\n",
      "Epoch: 47/100... Step: 9390... Loss: 1.0838... Val Loss: 1.2024 acc:0.6413280963897705\n",
      "Epoch: 47/100... Step: 9400... Loss: 1.1168... Val Loss: 1.2084 acc:0.6403906345367432\n",
      "Epoch: 48/100... Step: 9410... Loss: 1.0559... Val Loss: 1.2110 acc:0.6396093964576721\n",
      "Epoch: 48/100... Step: 9420... Loss: 1.0778... Val Loss: 1.2012 acc:0.640625\n",
      "Epoch: 48/100... Step: 9430... Loss: 1.0286... Val Loss: 1.2053 acc:0.6389843821525574\n",
      "Epoch: 48/100... Step: 9440... Loss: 1.0367... Val Loss: 1.2103 acc:0.6377343535423279\n",
      "Epoch: 48/100... Step: 9450... Loss: 1.0506... Val Loss: 1.2085 acc:0.641796886920929\n",
      "Epoch: 48/100... Step: 9460... Loss: 1.0534... Val Loss: 1.2051 acc:0.6389062404632568\n",
      "Epoch: 48/100... Step: 9470... Loss: 1.0397... Val Loss: 1.2085 acc:0.6396093964576721\n",
      "Epoch: 48/100... Step: 9480... Loss: 1.0492... Val Loss: 1.2102 acc:0.6385937333106995\n",
      "Epoch: 48/100... Step: 9490... Loss: 1.0434... Val Loss: 1.2055 acc:0.6388280987739563\n",
      "Epoch: 48/100... Step: 9500... Loss: 1.0469... Val Loss: 1.2021 acc:0.6399219036102295\n",
      "Epoch: 48/100... Step: 9510... Loss: 1.0566... Val Loss: 1.2152 acc:0.6376562714576721\n",
      "Epoch: 48/100... Step: 9520... Loss: 1.0347... Val Loss: 1.2028 acc:0.6408593654632568\n",
      "Epoch: 48/100... Step: 9530... Loss: 1.0432... Val Loss: 1.2058 acc:0.640625\n",
      "Epoch: 48/100... Step: 9540... Loss: 1.0647... Val Loss: 1.2060 acc:0.6376562714576721\n",
      "Epoch: 48/100... Step: 9550... Loss: 1.0733... Val Loss: 1.2131 acc:0.641796886920929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48/100... Step: 9560... Loss: 1.0575... Val Loss: 1.2042 acc:0.6381250023841858\n",
      "Epoch: 48/100... Step: 9570... Loss: 1.0493... Val Loss: 1.2095 acc:0.6374218463897705\n",
      "Epoch: 48/100... Step: 9580... Loss: 1.0580... Val Loss: 1.2069 acc:0.6379687786102295\n",
      "Epoch: 48/100... Step: 9590... Loss: 1.0725... Val Loss: 1.2034 acc:0.6401562690734863\n",
      "Epoch: 48/100... Step: 9600... Loss: 1.1065... Val Loss: 1.2083 acc:0.6382031440734863\n",
      "Epoch: 49/100... Step: 9610... Loss: 1.0527... Val Loss: 1.2138 acc:0.6385156512260437\n",
      "Epoch: 49/100... Step: 9620... Loss: 1.0664... Val Loss: 1.2031 acc:0.6404687762260437\n",
      "Epoch: 49/100... Step: 9630... Loss: 1.0347... Val Loss: 1.2052 acc:0.6387500166893005\n",
      "Epoch: 49/100... Step: 9640... Loss: 1.0334... Val Loss: 1.2108 acc:0.6374218463897705\n",
      "Epoch: 49/100... Step: 9650... Loss: 1.0365... Val Loss: 1.2127 acc:0.6385937333106995\n",
      "Epoch: 49/100... Step: 9660... Loss: 1.0432... Val Loss: 1.2035 acc:0.6414843797683716\n",
      "Epoch: 49/100... Step: 9670... Loss: 1.0402... Val Loss: 1.2121 acc:0.6389843821525574\n",
      "Epoch: 49/100... Step: 9680... Loss: 1.0596... Val Loss: 1.2113 acc:0.6383593678474426\n",
      "Epoch: 49/100... Step: 9690... Loss: 1.0332... Val Loss: 1.2074 acc:0.6350781321525574\n",
      "Epoch: 49/100... Step: 9700... Loss: 1.0400... Val Loss: 1.2074 acc:0.6407031416893005\n",
      "Epoch: 49/100... Step: 9710... Loss: 1.0478... Val Loss: 1.2127 acc:0.6393749713897705\n",
      "Epoch: 49/100... Step: 9720... Loss: 1.0317... Val Loss: 1.2038 acc:0.6421874761581421\n",
      "Epoch: 49/100... Step: 9730... Loss: 1.0321... Val Loss: 1.2092 acc:0.6407812237739563\n",
      "Epoch: 49/100... Step: 9740... Loss: 1.0705... Val Loss: 1.2073 acc:0.6396874785423279\n",
      "Epoch: 49/100... Step: 9750... Loss: 1.0552... Val Loss: 1.2214 acc:0.6384375095367432\n",
      "Epoch: 49/100... Step: 9760... Loss: 1.0533... Val Loss: 1.2093 acc:0.6375781297683716\n",
      "Epoch: 49/100... Step: 9770... Loss: 1.0460... Val Loss: 1.2127 acc:0.6362500190734863\n",
      "Epoch: 49/100... Step: 9780... Loss: 1.0532... Val Loss: 1.2081 acc:0.6402343511581421\n",
      "Epoch: 49/100... Step: 9790... Loss: 1.0759... Val Loss: 1.2067 acc:0.6407812237739563\n",
      "Epoch: 49/100... Step: 9800... Loss: 1.1075... Val Loss: 1.2087 acc:0.6415625214576721\n",
      "Epoch: 50/100... Step: 9810... Loss: 1.0549... Val Loss: 1.2119 acc:0.6403906345367432\n",
      "Epoch: 50/100... Step: 9820... Loss: 1.0614... Val Loss: 1.2032 acc:0.6421093940734863\n",
      "Epoch: 50/100... Step: 9830... Loss: 1.0300... Val Loss: 1.2076 acc:0.6405468583106995\n",
      "Epoch: 50/100... Step: 9840... Loss: 1.0191... Val Loss: 1.2146 acc:0.6396093964576721\n",
      "Epoch: 50/100... Step: 9850... Loss: 1.0481... Val Loss: 1.2100 acc:0.6414843797683716\n",
      "Epoch: 50/100... Step: 9860... Loss: 1.0518... Val Loss: 1.2075 acc:0.6422656178474426\n",
      "Epoch: 50/100... Step: 9870... Loss: 1.0349... Val Loss: 1.2123 acc:0.6413280963897705\n",
      "Epoch: 50/100... Step: 9880... Loss: 1.0499... Val Loss: 1.2131 acc:0.638671875\n",
      "Epoch: 50/100... Step: 9890... Loss: 1.0413... Val Loss: 1.2082 acc:0.638671875\n",
      "Epoch: 50/100... Step: 9900... Loss: 1.0376... Val Loss: 1.2079 acc:0.6382812261581421\n",
      "Epoch: 50/100... Step: 9910... Loss: 1.0542... Val Loss: 1.2134 acc:0.638671875\n",
      "Epoch: 50/100... Step: 9920... Loss: 1.0324... Val Loss: 1.2036 acc:0.6397656202316284\n",
      "Epoch: 50/100... Step: 9930... Loss: 1.0351... Val Loss: 1.2073 acc:0.6373437643051147\n",
      "Epoch: 50/100... Step: 9940... Loss: 1.0567... Val Loss: 1.2107 acc:0.6376562714576721\n",
      "Epoch: 50/100... Step: 9950... Loss: 1.0551... Val Loss: 1.2166 acc:0.6388280987739563\n",
      "Epoch: 50/100... Step: 9960... Loss: 1.0555... Val Loss: 1.2058 acc:0.6399219036102295\n",
      "Epoch: 50/100... Step: 9970... Loss: 1.0525... Val Loss: 1.2154 acc:0.6365625262260437\n",
      "Epoch: 50/100... Step: 9980... Loss: 1.0585... Val Loss: 1.2097 acc:0.6360156536102295\n",
      "Epoch: 50/100... Step: 9990... Loss: 1.0807... Val Loss: 1.2119 acc:0.6388280987739563\n",
      "Epoch: 50/100... Step: 10000... Loss: 1.1201... Val Loss: 1.2124 acc:0.638671875\n",
      "Epoch: 51/100... Step: 10010... Loss: 1.0506... Val Loss: 1.2143 acc:0.6389062404632568\n",
      "Epoch: 51/100... Step: 10020... Loss: 1.0728... Val Loss: 1.2056 acc:0.6382812261581421\n",
      "Epoch: 51/100... Step: 10030... Loss: 1.0214... Val Loss: 1.2082 acc:0.6396093964576721\n",
      "Epoch: 51/100... Step: 10040... Loss: 1.0352... Val Loss: 1.2162 acc:0.6368749737739563\n",
      "Epoch: 51/100... Step: 10050... Loss: 1.0469... Val Loss: 1.2144 acc:0.6393749713897705\n",
      "Epoch: 51/100... Step: 10060... Loss: 1.0429... Val Loss: 1.2146 acc:0.6376562714576721\n",
      "Epoch: 51/100... Step: 10070... Loss: 1.0238... Val Loss: 1.2124 acc:0.6396874785423279\n",
      "Epoch: 51/100... Step: 10080... Loss: 1.0433... Val Loss: 1.2148 acc:0.6358593702316284\n",
      "Epoch: 51/100... Step: 10090... Loss: 1.0339... Val Loss: 1.2085 acc:0.6363281011581421\n",
      "Epoch: 51/100... Step: 10100... Loss: 1.0379... Val Loss: 1.2042 acc:0.6383593678474426\n",
      "Epoch: 51/100... Step: 10110... Loss: 1.0482... Val Loss: 1.2173 acc:0.639843761920929\n",
      "Epoch: 51/100... Step: 10120... Loss: 1.0315... Val Loss: 1.2053 acc:0.6392968893051147\n",
      "Epoch: 51/100... Step: 10130... Loss: 1.0221... Val Loss: 1.2074 acc:0.6373437643051147\n",
      "Epoch: 51/100... Step: 10140... Loss: 1.0607... Val Loss: 1.2127 acc:0.6407031416893005\n",
      "Epoch: 51/100... Step: 10150... Loss: 1.0528... Val Loss: 1.2182 acc:0.6396874785423279\n",
      "Epoch: 51/100... Step: 10160... Loss: 1.0493... Val Loss: 1.2050 acc:0.6384375095367432\n",
      "Epoch: 51/100... Step: 10170... Loss: 1.0460... Val Loss: 1.2175 acc:0.634765625\n",
      "Epoch: 51/100... Step: 10180... Loss: 1.0558... Val Loss: 1.2082 acc:0.6378124952316284\n",
      "Epoch: 51/100... Step: 10190... Loss: 1.0691... Val Loss: 1.2065 acc:0.6377343535423279\n",
      "Epoch: 51/100... Step: 10200... Loss: 1.1041... Val Loss: 1.2114 acc:0.6382031440734863\n",
      "Epoch: 52/100... Step: 10210... Loss: 1.0423... Val Loss: 1.2138 acc:0.6365625262260437\n",
      "Epoch: 52/100... Step: 10220... Loss: 1.0623... Val Loss: 1.2060 acc:0.6384375095367432\n",
      "Epoch: 52/100... Step: 10230... Loss: 1.0186... Val Loss: 1.2071 acc:0.6402343511581421\n",
      "Epoch: 52/100... Step: 10240... Loss: 1.0136... Val Loss: 1.2133 acc:0.6378124952316284\n",
      "Epoch: 52/100... Step: 10250... Loss: 1.0327... Val Loss: 1.2128 acc:0.6389062404632568\n",
      "Epoch: 52/100... Step: 10260... Loss: 1.0468... Val Loss: 1.2077 acc:0.6392968893051147\n",
      "Epoch: 52/100... Step: 10270... Loss: 1.0217... Val Loss: 1.2151 acc:0.6385937333106995\n",
      "Epoch: 52/100... Step: 10280... Loss: 1.0477... Val Loss: 1.2115 acc:0.6375781297683716\n",
      "Epoch: 52/100... Step: 10290... Loss: 1.0304... Val Loss: 1.2086 acc:0.6367968916893005\n",
      "Epoch: 52/100... Step: 10300... Loss: 1.0357... Val Loss: 1.2114 acc:0.6367968916893005\n",
      "Epoch: 52/100... Step: 10310... Loss: 1.0508... Val Loss: 1.2158 acc:0.6369531154632568\n",
      "Epoch: 52/100... Step: 10320... Loss: 1.0300... Val Loss: 1.2065 acc:0.640625\n",
      "Epoch: 52/100... Step: 10330... Loss: 1.0265... Val Loss: 1.2091 acc:0.6389843821525574\n",
      "Epoch: 52/100... Step: 10340... Loss: 1.0597... Val Loss: 1.2122 acc:0.6383593678474426\n",
      "Epoch: 52/100... Step: 10350... Loss: 1.0501... Val Loss: 1.2184 acc:0.6384375095367432\n",
      "Epoch: 52/100... Step: 10360... Loss: 1.0423... Val Loss: 1.2078 acc:0.6363281011581421\n",
      "Epoch: 52/100... Step: 10370... Loss: 1.0391... Val Loss: 1.2151 acc:0.6357031464576721\n",
      "Epoch: 52/100... Step: 10380... Loss: 1.0496... Val Loss: 1.2128 acc:0.6367968916893005\n",
      "Epoch: 52/100... Step: 10390... Loss: 1.0636... Val Loss: 1.2106 acc:0.6364843845367432\n",
      "Epoch: 52/100... Step: 10400... Loss: 1.1064... Val Loss: 1.2115 acc:0.6389062404632568\n",
      "Epoch: 53/100... Step: 10410... Loss: 1.0385... Val Loss: 1.2145 acc:0.638671875\n",
      "Epoch: 53/100... Step: 10420... Loss: 1.0644... Val Loss: 1.2063 acc:0.6382812261581421\n",
      "Epoch: 53/100... Step: 10430... Loss: 1.0256... Val Loss: 1.2074 acc:0.6414843797683716\n",
      "Epoch: 53/100... Step: 10440... Loss: 1.0249... Val Loss: 1.2161 acc:0.6364062428474426\n",
      "Epoch: 53/100... Step: 10450... Loss: 1.0463... Val Loss: 1.2142 acc:0.6396093964576721\n",
      "Epoch: 53/100... Step: 10460... Loss: 1.0424... Val Loss: 1.2089 acc:0.641406238079071\n",
      "Epoch: 53/100... Step: 10470... Loss: 1.0203... Val Loss: 1.2130 acc:0.6432031393051147\n",
      "Epoch: 53/100... Step: 10480... Loss: 1.0387... Val Loss: 1.2129 acc:0.6375781297683716\n",
      "Epoch: 53/100... Step: 10490... Loss: 1.0272... Val Loss: 1.2102 acc:0.639843761920929\n",
      "Epoch: 53/100... Step: 10500... Loss: 1.0320... Val Loss: 1.2077 acc:0.6399219036102295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53/100... Step: 10510... Loss: 1.0399... Val Loss: 1.2161 acc:0.6380468606948853\n",
      "Epoch: 53/100... Step: 10520... Loss: 1.0296... Val Loss: 1.2085 acc:0.6414843797683716\n",
      "Epoch: 53/100... Step: 10530... Loss: 1.0313... Val Loss: 1.2122 acc:0.6370312571525574\n",
      "Epoch: 53/100... Step: 10540... Loss: 1.0470... Val Loss: 1.2112 acc:0.6401562690734863\n",
      "Epoch: 53/100... Step: 10550... Loss: 1.0509... Val Loss: 1.2168 acc:0.6392187476158142\n",
      "Epoch: 53/100... Step: 10560... Loss: 1.0488... Val Loss: 1.2075 acc:0.6388280987739563\n",
      "Epoch: 53/100... Step: 10570... Loss: 1.0342... Val Loss: 1.2141 acc:0.637499988079071\n",
      "Epoch: 53/100... Step: 10580... Loss: 1.0397... Val Loss: 1.2111 acc:0.6385937333106995\n",
      "Epoch: 53/100... Step: 10590... Loss: 1.0621... Val Loss: 1.2077 acc:0.6391406059265137\n",
      "Epoch: 53/100... Step: 10600... Loss: 1.1009... Val Loss: 1.2140 acc:0.640625\n",
      "Epoch: 54/100... Step: 10610... Loss: 1.0416... Val Loss: 1.2166 acc:0.6399999856948853\n",
      "Epoch: 54/100... Step: 10620... Loss: 1.0569... Val Loss: 1.2079 acc:0.6409375071525574\n",
      "Epoch: 54/100... Step: 10630... Loss: 1.0206... Val Loss: 1.2106 acc:0.6404687762260437\n",
      "Epoch: 54/100... Step: 10640... Loss: 1.0266... Val Loss: 1.2182 acc:0.6365625262260437\n",
      "Epoch: 54/100... Step: 10650... Loss: 1.0386... Val Loss: 1.2104 acc:0.6404687762260437\n",
      "Epoch: 54/100... Step: 10660... Loss: 1.0399... Val Loss: 1.2085 acc:0.6404687762260437\n",
      "Epoch: 54/100... Step: 10670... Loss: 1.0291... Val Loss: 1.2150 acc:0.6426562666893005\n",
      "Epoch: 54/100... Step: 10680... Loss: 1.0377... Val Loss: 1.2142 acc:0.6407031416893005\n",
      "Epoch: 54/100... Step: 10690... Loss: 1.0187... Val Loss: 1.2108 acc:0.6382812261581421\n",
      "Epoch: 54/100... Step: 10700... Loss: 1.0321... Val Loss: 1.2117 acc:0.6421874761581421\n",
      "Epoch: 54/100... Step: 10710... Loss: 1.0425... Val Loss: 1.2186 acc:0.6424999833106995\n",
      "Epoch: 54/100... Step: 10720... Loss: 1.0231... Val Loss: 1.2108 acc:0.6399219036102295\n",
      "Epoch: 54/100... Step: 10730... Loss: 1.0364... Val Loss: 1.2106 acc:0.6396874785423279\n",
      "Epoch: 54/100... Step: 10740... Loss: 1.0556... Val Loss: 1.2125 acc:0.6409375071525574\n",
      "Epoch: 54/100... Step: 10750... Loss: 1.0460... Val Loss: 1.2159 acc:0.6388280987739563\n",
      "Epoch: 54/100... Step: 10760... Loss: 1.0424... Val Loss: 1.2055 acc:0.6354687213897705\n",
      "Epoch: 54/100... Step: 10770... Loss: 1.0351... Val Loss: 1.2172 acc:0.6369531154632568\n",
      "Epoch: 54/100... Step: 10780... Loss: 1.0460... Val Loss: 1.2102 acc:0.6396874785423279\n",
      "Epoch: 54/100... Step: 10790... Loss: 1.0583... Val Loss: 1.2084 acc:0.6416406035423279\n",
      "Epoch: 54/100... Step: 10800... Loss: 1.1049... Val Loss: 1.2142 acc:0.6389062404632568\n",
      "Epoch: 55/100... Step: 10810... Loss: 1.0468... Val Loss: 1.2148 acc:0.6396874785423279\n",
      "Epoch: 55/100... Step: 10820... Loss: 1.0582... Val Loss: 1.2052 acc:0.639843761920929\n",
      "Epoch: 55/100... Step: 10830... Loss: 1.0264... Val Loss: 1.2142 acc:0.6409375071525574\n",
      "Epoch: 55/100... Step: 10840... Loss: 1.0203... Val Loss: 1.2169 acc:0.637890636920929\n",
      "Epoch: 55/100... Step: 10850... Loss: 1.0416... Val Loss: 1.2139 acc:0.6414843797683716\n",
      "Epoch: 55/100... Step: 10860... Loss: 1.0438... Val Loss: 1.2105 acc:0.6410937309265137\n",
      "Epoch: 55/100... Step: 10870... Loss: 1.0223... Val Loss: 1.2152 acc:0.6410156488418579\n",
      "Epoch: 55/100... Step: 10880... Loss: 1.0383... Val Loss: 1.2131 acc:0.6380468606948853\n",
      "Epoch: 55/100... Step: 10890... Loss: 1.0242... Val Loss: 1.2084 acc:0.6395312547683716\n",
      "Epoch: 55/100... Step: 10900... Loss: 1.0279... Val Loss: 1.2095 acc:0.6378124952316284\n",
      "Epoch: 55/100... Step: 10910... Loss: 1.0409... Val Loss: 1.2145 acc:0.6404687762260437\n",
      "Epoch: 55/100... Step: 10920... Loss: 1.0211... Val Loss: 1.2110 acc:0.6403124928474426\n",
      "Epoch: 55/100... Step: 10930... Loss: 1.0290... Val Loss: 1.2126 acc:0.6395312547683716\n",
      "Epoch: 55/100... Step: 10940... Loss: 1.0429... Val Loss: 1.2135 acc:0.6368749737739563\n",
      "Epoch: 55/100... Step: 10950... Loss: 1.0440... Val Loss: 1.2206 acc:0.6404687762260437\n",
      "Epoch: 55/100... Step: 10960... Loss: 1.0425... Val Loss: 1.2101 acc:0.6393749713897705\n",
      "Epoch: 55/100... Step: 10970... Loss: 1.0412... Val Loss: 1.2173 acc:0.6379687786102295\n",
      "Epoch: 55/100... Step: 10980... Loss: 1.0419... Val Loss: 1.2110 acc:0.6403906345367432\n",
      "Epoch: 55/100... Step: 10990... Loss: 1.0530... Val Loss: 1.2093 acc:0.6426562666893005\n",
      "Epoch: 55/100... Step: 11000... Loss: 1.0853... Val Loss: 1.2146 acc:0.6396093964576721\n",
      "Epoch: 56/100... Step: 11010... Loss: 1.0388... Val Loss: 1.2154 acc:0.6397656202316284\n",
      "Epoch: 56/100... Step: 11020... Loss: 1.0530... Val Loss: 1.2094 acc:0.639843761920929\n",
      "Epoch: 56/100... Step: 11030... Loss: 1.0110... Val Loss: 1.2114 acc:0.6418750286102295\n",
      "Epoch: 56/100... Step: 11040... Loss: 1.0093... Val Loss: 1.2171 acc:0.6401562690734863\n",
      "Epoch: 56/100... Step: 11050... Loss: 1.0386... Val Loss: 1.2194 acc:0.6423437595367432\n",
      "Epoch: 56/100... Step: 11060... Loss: 1.0309... Val Loss: 1.2095 acc:0.6422656178474426\n",
      "Epoch: 56/100... Step: 11070... Loss: 1.0177... Val Loss: 1.2207 acc:0.6421874761581421\n",
      "Epoch: 56/100... Step: 11080... Loss: 1.0330... Val Loss: 1.2152 acc:0.6403906345367432\n",
      "Epoch: 56/100... Step: 11090... Loss: 1.0203... Val Loss: 1.2120 acc:0.6399219036102295\n",
      "Epoch: 56/100... Step: 11100... Loss: 1.0260... Val Loss: 1.2102 acc:0.6403906345367432\n",
      "Epoch: 56/100... Step: 11110... Loss: 1.0337... Val Loss: 1.2214 acc:0.6385937333106995\n",
      "Epoch: 56/100... Step: 11120... Loss: 1.0064... Val Loss: 1.2143 acc:0.640625\n",
      "Epoch: 56/100... Step: 11130... Loss: 1.0205... Val Loss: 1.2175 acc:0.639453113079071\n",
      "Epoch: 56/100... Step: 11140... Loss: 1.0414... Val Loss: 1.2133 acc:0.6384375095367432\n",
      "Epoch: 56/100... Step: 11150... Loss: 1.0369... Val Loss: 1.2178 acc:0.6407031416893005\n",
      "Epoch: 56/100... Step: 11160... Loss: 1.0406... Val Loss: 1.2115 acc:0.6369531154632568\n",
      "Epoch: 56/100... Step: 11170... Loss: 1.0329... Val Loss: 1.2191 acc:0.6363281011581421\n",
      "Epoch: 56/100... Step: 11180... Loss: 1.0398... Val Loss: 1.2122 acc:0.6401562690734863\n",
      "Epoch: 56/100... Step: 11190... Loss: 1.0441... Val Loss: 1.2133 acc:0.6403906345367432\n",
      "Epoch: 56/100... Step: 11200... Loss: 1.0929... Val Loss: 1.2160 acc:0.6396093964576721\n",
      "Epoch: 57/100... Step: 11210... Loss: 1.0325... Val Loss: 1.2196 acc:0.639453113079071\n",
      "Epoch: 57/100... Step: 11220... Loss: 1.0565... Val Loss: 1.2103 acc:0.6412500143051147\n",
      "Epoch: 57/100... Step: 11230... Loss: 1.0107... Val Loss: 1.2168 acc:0.6418750286102295\n",
      "Epoch: 57/100... Step: 11240... Loss: 1.0167... Val Loss: 1.2194 acc:0.6410937309265137\n",
      "Epoch: 57/100... Step: 11250... Loss: 1.0302... Val Loss: 1.2158 acc:0.6417187452316284\n",
      "Epoch: 57/100... Step: 11260... Loss: 1.0290... Val Loss: 1.2130 acc:0.6412500143051147\n",
      "Epoch: 57/100... Step: 11270... Loss: 1.0053... Val Loss: 1.2163 acc:0.6435156464576721\n",
      "Epoch: 57/100... Step: 11280... Loss: 1.0387... Val Loss: 1.2139 acc:0.6410937309265137\n",
      "Epoch: 57/100... Step: 11290... Loss: 1.0162... Val Loss: 1.2101 acc:0.6390625238418579\n",
      "Epoch: 57/100... Step: 11300... Loss: 1.0205... Val Loss: 1.2138 acc:0.641406238079071\n",
      "Epoch: 57/100... Step: 11310... Loss: 1.0328... Val Loss: 1.2207 acc:0.637890636920929\n",
      "Epoch: 57/100... Step: 11320... Loss: 1.0176... Val Loss: 1.2129 acc:0.6417187452316284\n",
      "Epoch: 57/100... Step: 11330... Loss: 1.0259... Val Loss: 1.2135 acc:0.6405468583106995\n",
      "Epoch: 57/100... Step: 11340... Loss: 1.0397... Val Loss: 1.2155 acc:0.6412500143051147\n",
      "Epoch: 57/100... Step: 11350... Loss: 1.0429... Val Loss: 1.2197 acc:0.639453113079071\n",
      "Epoch: 57/100... Step: 11360... Loss: 1.0337... Val Loss: 1.2127 acc:0.6371093988418579\n",
      "Epoch: 57/100... Step: 11370... Loss: 1.0327... Val Loss: 1.2189 acc:0.6382812261581421\n",
      "Epoch: 57/100... Step: 11380... Loss: 1.0451... Val Loss: 1.2154 acc:0.6408593654632568\n",
      "Epoch: 57/100... Step: 11390... Loss: 1.0671... Val Loss: 1.2149 acc:0.6396093964576721\n",
      "Epoch: 57/100... Step: 11400... Loss: 1.0959... Val Loss: 1.2180 acc:0.6392968893051147\n",
      "Epoch: 58/100... Step: 11410... Loss: 1.0358... Val Loss: 1.2186 acc:0.638671875\n",
      "Epoch: 58/100... Step: 11420... Loss: 1.0492... Val Loss: 1.2098 acc:0.6401562690734863\n",
      "Epoch: 58/100... Step: 11430... Loss: 1.0082... Val Loss: 1.2188 acc:0.6382031440734863\n",
      "Epoch: 58/100... Step: 11440... Loss: 1.0065... Val Loss: 1.2184 acc:0.6396093964576721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58/100... Step: 11450... Loss: 1.0290... Val Loss: 1.2161 acc:0.6414843797683716\n",
      "Epoch: 58/100... Step: 11460... Loss: 1.0263... Val Loss: 1.2174 acc:0.6403906345367432\n",
      "Epoch: 58/100... Step: 11470... Loss: 1.0173... Val Loss: 1.2199 acc:0.639843761920929\n",
      "Epoch: 58/100... Step: 11480... Loss: 1.0218... Val Loss: 1.2190 acc:0.639843761920929\n",
      "Epoch: 58/100... Step: 11490... Loss: 1.0135... Val Loss: 1.2145 acc:0.6417187452316284\n",
      "Epoch: 58/100... Step: 11500... Loss: 1.0093... Val Loss: 1.2154 acc:0.6392187476158142\n",
      "Epoch: 58/100... Step: 11510... Loss: 1.0286... Val Loss: 1.2203 acc:0.6391406059265137\n",
      "Epoch: 58/100... Step: 11520... Loss: 1.0136... Val Loss: 1.2164 acc:0.6419531106948853\n",
      "Epoch: 58/100... Step: 11530... Loss: 1.0164... Val Loss: 1.2182 acc:0.6382031440734863\n",
      "Epoch: 58/100... Step: 11540... Loss: 1.0341... Val Loss: 1.2153 acc:0.6380468606948853\n",
      "Epoch: 58/100... Step: 11550... Loss: 1.0415... Val Loss: 1.2199 acc:0.6393749713897705\n",
      "Epoch: 58/100... Step: 11560... Loss: 1.0394... Val Loss: 1.2131 acc:0.6403906345367432\n",
      "Epoch: 58/100... Step: 11570... Loss: 1.0253... Val Loss: 1.2231 acc:0.6385156512260437\n",
      "Epoch: 58/100... Step: 11580... Loss: 1.0274... Val Loss: 1.2170 acc:0.6373437643051147\n",
      "Epoch: 58/100... Step: 11590... Loss: 1.0385... Val Loss: 1.2163 acc:0.638671875\n",
      "Epoch: 58/100... Step: 11600... Loss: 1.0826... Val Loss: 1.2201 acc:0.6392968893051147\n",
      "Epoch: 59/100... Step: 11610... Loss: 1.0343... Val Loss: 1.2194 acc:0.6371093988418579\n",
      "Epoch: 59/100... Step: 11620... Loss: 1.0446... Val Loss: 1.2115 acc:0.641406238079071\n",
      "Epoch: 59/100... Step: 11630... Loss: 1.0065... Val Loss: 1.2210 acc:0.6389062404632568\n",
      "Epoch: 59/100... Step: 11640... Loss: 1.0222... Val Loss: 1.2243 acc:0.6414843797683716\n",
      "Epoch: 59/100... Step: 11650... Loss: 1.0266... Val Loss: 1.2156 acc:0.6420312523841858\n",
      "Epoch: 59/100... Step: 11660... Loss: 1.0357... Val Loss: 1.2166 acc:0.639453113079071\n",
      "Epoch: 59/100... Step: 11670... Loss: 1.0139... Val Loss: 1.2217 acc:0.6407812237739563\n",
      "Epoch: 59/100... Step: 11680... Loss: 1.0273... Val Loss: 1.2209 acc:0.6382031440734863\n",
      "Epoch: 59/100... Step: 11690... Loss: 1.0085... Val Loss: 1.2170 acc:0.6402343511581421\n",
      "Epoch: 59/100... Step: 11700... Loss: 1.0088... Val Loss: 1.2177 acc:0.640625\n",
      "Epoch: 59/100... Step: 11710... Loss: 1.0353... Val Loss: 1.2267 acc:0.6381250023841858\n",
      "Epoch: 59/100... Step: 11720... Loss: 1.0068... Val Loss: 1.2184 acc:0.6381250023841858\n",
      "Epoch: 59/100... Step: 11730... Loss: 1.0081... Val Loss: 1.2159 acc:0.6402343511581421\n",
      "Epoch: 59/100... Step: 11740... Loss: 1.0320... Val Loss: 1.2156 acc:0.6418750286102295\n",
      "Epoch: 59/100... Step: 11750... Loss: 1.0325... Val Loss: 1.2212 acc:0.6401562690734863\n",
      "Epoch: 59/100... Step: 11760... Loss: 1.0272... Val Loss: 1.2164 acc:0.6384375095367432\n",
      "Epoch: 59/100... Step: 11770... Loss: 1.0212... Val Loss: 1.2240 acc:0.635937511920929\n",
      "Epoch: 59/100... Step: 11780... Loss: 1.0319... Val Loss: 1.2202 acc:0.6374218463897705\n",
      "Epoch: 59/100... Step: 11790... Loss: 1.0477... Val Loss: 1.2161 acc:0.6399219036102295\n",
      "Epoch: 59/100... Step: 11800... Loss: 1.0812... Val Loss: 1.2226 acc:0.6371093988418579\n",
      "Epoch: 60/100... Step: 11810... Loss: 1.0260... Val Loss: 1.2203 acc:0.6384375095367432\n",
      "Epoch: 60/100... Step: 11820... Loss: 1.0357... Val Loss: 1.2135 acc:0.640625\n",
      "Epoch: 60/100... Step: 11830... Loss: 1.0078... Val Loss: 1.2212 acc:0.6382812261581421\n",
      "Epoch: 60/100... Step: 11840... Loss: 0.9982... Val Loss: 1.2193 acc:0.6405468583106995\n",
      "Epoch: 60/100... Step: 11850... Loss: 1.0263... Val Loss: 1.2179 acc:0.6382812261581421\n",
      "Epoch: 60/100... Step: 11860... Loss: 1.0221... Val Loss: 1.2173 acc:0.6411718726158142\n",
      "Epoch: 60/100... Step: 11870... Loss: 1.0149... Val Loss: 1.2193 acc:0.6366406083106995\n",
      "Epoch: 60/100... Step: 11880... Loss: 1.0276... Val Loss: 1.2196 acc:0.6368749737739563\n",
      "Epoch: 60/100... Step: 11890... Loss: 1.0090... Val Loss: 1.2162 acc:0.6373437643051147\n",
      "Epoch: 60/100... Step: 11900... Loss: 1.0066... Val Loss: 1.2194 acc:0.6371093988418579\n",
      "Epoch: 60/100... Step: 11910... Loss: 1.0300... Val Loss: 1.2251 acc:0.6377343535423279\n",
      "Epoch: 60/100... Step: 11920... Loss: 1.0056... Val Loss: 1.2207 acc:0.6405468583106995\n",
      "Epoch: 60/100... Step: 11930... Loss: 1.0200... Val Loss: 1.2182 acc:0.6410156488418579\n",
      "Epoch: 60/100... Step: 11940... Loss: 1.0328... Val Loss: 1.2173 acc:0.641406238079071\n",
      "Epoch: 60/100... Step: 11950... Loss: 1.0250... Val Loss: 1.2217 acc:0.639843761920929\n",
      "Epoch: 60/100... Step: 11960... Loss: 1.0332... Val Loss: 1.2133 acc:0.6383593678474426\n",
      "Epoch: 60/100... Step: 11970... Loss: 1.0255... Val Loss: 1.2248 acc:0.6341406106948853\n",
      "Epoch: 60/100... Step: 11980... Loss: 1.0262... Val Loss: 1.2191 acc:0.6383593678474426\n",
      "Epoch: 60/100... Step: 11990... Loss: 1.0539... Val Loss: 1.2163 acc:0.6389062404632568\n",
      "Epoch: 60/100... Step: 12000... Loss: 1.0773... Val Loss: 1.2231 acc:0.6395312547683716\n",
      "Epoch: 61/100... Step: 12010... Loss: 1.0277... Val Loss: 1.2179 acc:0.6375781297683716\n",
      "Epoch: 61/100... Step: 12020... Loss: 1.0373... Val Loss: 1.2124 acc:0.6407031416893005\n",
      "Epoch: 61/100... Step: 12030... Loss: 1.0047... Val Loss: 1.2199 acc:0.6382031440734863\n",
      "Epoch: 61/100... Step: 12040... Loss: 1.0008... Val Loss: 1.2193 acc:0.6396874785423279\n",
      "Epoch: 61/100... Step: 12050... Loss: 1.0162... Val Loss: 1.2203 acc:0.6373437643051147\n",
      "Epoch: 61/100... Step: 12060... Loss: 1.0325... Val Loss: 1.2179 acc:0.6399219036102295\n",
      "Epoch: 61/100... Step: 12070... Loss: 1.0068... Val Loss: 1.2206 acc:0.6387500166893005\n",
      "Epoch: 61/100... Step: 12080... Loss: 1.0228... Val Loss: 1.2210 acc:0.6375781297683716\n",
      "Epoch: 61/100... Step: 12090... Loss: 1.0091... Val Loss: 1.2173 acc:0.6387500166893005\n",
      "Epoch: 61/100... Step: 12100... Loss: 1.0202... Val Loss: 1.2203 acc:0.6379687786102295\n",
      "Epoch: 61/100... Step: 12110... Loss: 1.0269... Val Loss: 1.2275 acc:0.6396874785423279\n",
      "Epoch: 61/100... Step: 12120... Loss: 1.0107... Val Loss: 1.2220 acc:0.6383593678474426\n",
      "Epoch: 61/100... Step: 12130... Loss: 1.0036... Val Loss: 1.2174 acc:0.6415625214576721\n",
      "Epoch: 61/100... Step: 12140... Loss: 1.0321... Val Loss: 1.2172 acc:0.6403906345367432\n",
      "Epoch: 61/100... Step: 12150... Loss: 1.0253... Val Loss: 1.2191 acc:0.6399999856948853\n",
      "Epoch: 61/100... Step: 12160... Loss: 1.0319... Val Loss: 1.2164 acc:0.6379687786102295\n",
      "Epoch: 61/100... Step: 12170... Loss: 1.0312... Val Loss: 1.2254 acc:0.6370312571525574\n",
      "Epoch: 61/100... Step: 12180... Loss: 1.0268... Val Loss: 1.2187 acc:0.6384375095367432\n",
      "Epoch: 61/100... Step: 12190... Loss: 1.0446... Val Loss: 1.2186 acc:0.639453113079071\n",
      "Epoch: 61/100... Step: 12200... Loss: 1.0831... Val Loss: 1.2206 acc:0.6404687762260437\n",
      "Epoch: 62/100... Step: 12210... Loss: 1.0221... Val Loss: 1.2228 acc:0.6387500166893005\n",
      "Epoch: 62/100... Step: 12220... Loss: 1.0360... Val Loss: 1.2185 acc:0.639453113079071\n",
      "Epoch: 62/100... Step: 12230... Loss: 1.0089... Val Loss: 1.2250 acc:0.6397656202316284\n",
      "Epoch: 62/100... Step: 12240... Loss: 1.0007... Val Loss: 1.2245 acc:0.6392187476158142\n",
      "Epoch: 62/100... Step: 12250... Loss: 1.0061... Val Loss: 1.2209 acc:0.6382031440734863\n",
      "Epoch: 62/100... Step: 12260... Loss: 1.0219... Val Loss: 1.2211 acc:0.6383593678474426\n",
      "Epoch: 62/100... Step: 12270... Loss: 1.0052... Val Loss: 1.2246 acc:0.641406238079071\n",
      "Epoch: 62/100... Step: 12280... Loss: 1.0196... Val Loss: 1.2266 acc:0.6393749713897705\n",
      "Epoch: 62/100... Step: 12290... Loss: 1.0022... Val Loss: 1.2206 acc:0.6414843797683716\n",
      "Epoch: 62/100... Step: 12300... Loss: 1.0004... Val Loss: 1.2190 acc:0.6389843821525574\n",
      "Epoch: 62/100... Step: 12310... Loss: 1.0237... Val Loss: 1.2278 acc:0.639843761920929\n",
      "Epoch: 62/100... Step: 12320... Loss: 1.0070... Val Loss: 1.2203 acc:0.6403124928474426\n",
      "Epoch: 62/100... Step: 12330... Loss: 1.0089... Val Loss: 1.2197 acc:0.6408593654632568\n",
      "Epoch: 62/100... Step: 12340... Loss: 1.0337... Val Loss: 1.2185 acc:0.6404687762260437\n",
      "Epoch: 62/100... Step: 12350... Loss: 1.0214... Val Loss: 1.2217 acc:0.6402343511581421\n",
      "Epoch: 62/100... Step: 12360... Loss: 1.0281... Val Loss: 1.2191 acc:0.6389062404632568\n",
      "Epoch: 62/100... Step: 12370... Loss: 1.0128... Val Loss: 1.2243 acc:0.6391406059265137\n",
      "Epoch: 62/100... Step: 12380... Loss: 1.0117... Val Loss: 1.2198 acc:0.6409375071525574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62/100... Step: 12390... Loss: 1.0420... Val Loss: 1.2212 acc:0.638671875\n",
      "Epoch: 62/100... Step: 12400... Loss: 1.0718... Val Loss: 1.2230 acc:0.6378124952316284\n",
      "Epoch: 63/100... Step: 12410... Loss: 1.0213... Val Loss: 1.2218 acc:0.639453113079071\n",
      "Epoch: 63/100... Step: 12420... Loss: 1.0362... Val Loss: 1.2197 acc:0.6389062404632568\n",
      "Epoch: 63/100... Step: 12430... Loss: 1.0012... Val Loss: 1.2269 acc:0.6382031440734863\n",
      "Epoch: 63/100... Step: 12440... Loss: 0.9980... Val Loss: 1.2253 acc:0.6407812237739563\n",
      "Epoch: 63/100... Step: 12450... Loss: 1.0108... Val Loss: 1.2224 acc:0.638671875\n",
      "Epoch: 63/100... Step: 12460... Loss: 1.0180... Val Loss: 1.2236 acc:0.6380468606948853\n",
      "Epoch: 63/100... Step: 12470... Loss: 1.0054... Val Loss: 1.2263 acc:0.6408593654632568\n",
      "Epoch: 63/100... Step: 12480... Loss: 1.0229... Val Loss: 1.2244 acc:0.639843761920929\n",
      "Epoch: 63/100... Step: 12490... Loss: 0.9990... Val Loss: 1.2199 acc:0.6418750286102295\n",
      "Epoch: 63/100... Step: 12500... Loss: 1.0062... Val Loss: 1.2176 acc:0.6396093964576721\n",
      "Epoch: 63/100... Step: 12510... Loss: 1.0185... Val Loss: 1.2290 acc:0.6396874785423279\n",
      "Epoch: 63/100... Step: 12520... Loss: 1.0040... Val Loss: 1.2222 acc:0.6402343511581421\n",
      "Epoch: 63/100... Step: 12530... Loss: 1.0025... Val Loss: 1.2227 acc:0.6418750286102295\n",
      "Epoch: 63/100... Step: 12540... Loss: 1.0224... Val Loss: 1.2180 acc:0.6423437595367432\n",
      "Epoch: 63/100... Step: 12550... Loss: 1.0294... Val Loss: 1.2253 acc:0.6403124928474426\n",
      "Epoch: 63/100... Step: 12560... Loss: 1.0254... Val Loss: 1.2202 acc:0.6416406035423279\n",
      "Epoch: 63/100... Step: 12570... Loss: 1.0214... Val Loss: 1.2250 acc:0.6371874809265137\n",
      "Epoch: 63/100... Step: 12580... Loss: 1.0262... Val Loss: 1.2211 acc:0.6373437643051147\n",
      "Epoch: 63/100... Step: 12590... Loss: 1.0352... Val Loss: 1.2193 acc:0.6415625214576721\n",
      "Epoch: 63/100... Step: 12600... Loss: 1.0699... Val Loss: 1.2258 acc:0.6416406035423279\n",
      "Epoch: 64/100... Step: 12610... Loss: 1.0169... Val Loss: 1.2229 acc:0.6410156488418579\n",
      "Epoch: 64/100... Step: 12620... Loss: 1.0289... Val Loss: 1.2239 acc:0.6430468559265137\n",
      "Epoch: 64/100... Step: 12630... Loss: 0.9960... Val Loss: 1.2303 acc:0.639843761920929\n",
      "Epoch: 64/100... Step: 12640... Loss: 0.9967... Val Loss: 1.2236 acc:0.6402343511581421\n",
      "Epoch: 64/100... Step: 12650... Loss: 1.0152... Val Loss: 1.2225 acc:0.6414843797683716\n",
      "Epoch: 64/100... Step: 12660... Loss: 1.0106... Val Loss: 1.2243 acc:0.641406238079071\n",
      "Epoch: 64/100... Step: 12670... Loss: 1.0088... Val Loss: 1.2294 acc:0.6389062404632568\n",
      "Epoch: 64/100... Step: 12680... Loss: 1.0175... Val Loss: 1.2279 acc:0.6389062404632568\n",
      "Epoch: 64/100... Step: 12690... Loss: 1.0024... Val Loss: 1.2229 acc:0.6378124952316284\n",
      "Epoch: 64/100... Step: 12700... Loss: 1.0029... Val Loss: 1.2200 acc:0.6393749713897705\n",
      "Epoch: 64/100... Step: 12710... Loss: 1.0284... Val Loss: 1.2323 acc:0.6399999856948853\n",
      "Epoch: 64/100... Step: 12720... Loss: 0.9973... Val Loss: 1.2247 acc:0.6407031416893005\n",
      "Epoch: 64/100... Step: 12730... Loss: 1.0109... Val Loss: 1.2211 acc:0.6404687762260437\n",
      "Epoch: 64/100... Step: 12740... Loss: 1.0334... Val Loss: 1.2205 acc:0.641406238079071\n",
      "Epoch: 64/100... Step: 12750... Loss: 1.0242... Val Loss: 1.2249 acc:0.6405468583106995\n",
      "Epoch: 64/100... Step: 12760... Loss: 1.0158... Val Loss: 1.2231 acc:0.6364843845367432\n",
      "Epoch: 64/100... Step: 12770... Loss: 1.0114... Val Loss: 1.2263 acc:0.6389843821525574\n",
      "Epoch: 64/100... Step: 12780... Loss: 1.0240... Val Loss: 1.2243 acc:0.6389843821525574\n",
      "Epoch: 64/100... Step: 12790... Loss: 1.0379... Val Loss: 1.2228 acc:0.6400781273841858\n",
      "Epoch: 64/100... Step: 12800... Loss: 1.0710... Val Loss: 1.2271 acc:0.6371874809265137\n",
      "Epoch: 65/100... Step: 12810... Loss: 1.0177... Val Loss: 1.2263 acc:0.639843761920929\n",
      "Epoch: 65/100... Step: 12820... Loss: 1.0382... Val Loss: 1.2228 acc:0.6431249976158142\n",
      "Epoch: 65/100... Step: 12830... Loss: 0.9946... Val Loss: 1.2301 acc:0.6395312547683716\n",
      "Epoch: 65/100... Step: 12840... Loss: 0.9988... Val Loss: 1.2243 acc:0.6408593654632568\n",
      "Epoch: 65/100... Step: 12850... Loss: 1.0050... Val Loss: 1.2201 acc:0.6401562690734863\n",
      "Epoch: 65/100... Step: 12860... Loss: 1.0186... Val Loss: 1.2250 acc:0.6408593654632568\n",
      "Epoch: 65/100... Step: 12870... Loss: 0.9990... Val Loss: 1.2284 acc:0.6392968893051147\n",
      "Epoch: 65/100... Step: 12880... Loss: 1.0148... Val Loss: 1.2305 acc:0.6403124928474426\n",
      "Epoch: 65/100... Step: 12890... Loss: 1.0023... Val Loss: 1.2235 acc:0.6413280963897705\n",
      "Epoch: 65/100... Step: 12900... Loss: 1.0082... Val Loss: 1.2221 acc:0.6382031440734863\n",
      "Epoch: 65/100... Step: 12910... Loss: 1.0197... Val Loss: 1.2336 acc:0.6420312523841858\n",
      "Epoch: 65/100... Step: 12920... Loss: 0.9958... Val Loss: 1.2218 acc:0.6393749713897705\n",
      "Epoch: 65/100... Step: 12930... Loss: 1.0048... Val Loss: 1.2230 acc:0.6385156512260437\n",
      "Epoch: 65/100... Step: 12940... Loss: 1.0194... Val Loss: 1.2224 acc:0.639843761920929\n",
      "Epoch: 65/100... Step: 12950... Loss: 1.0226... Val Loss: 1.2242 acc:0.6389062404632568\n",
      "Epoch: 65/100... Step: 12960... Loss: 1.0184... Val Loss: 1.2250 acc:0.633593738079071\n",
      "Epoch: 65/100... Step: 12970... Loss: 1.0180... Val Loss: 1.2286 acc:0.6385156512260437\n",
      "Epoch: 65/100... Step: 12980... Loss: 1.0160... Val Loss: 1.2291 acc:0.6344531178474426\n",
      "Epoch: 65/100... Step: 12990... Loss: 1.0400... Val Loss: 1.2215 acc:0.6376562714576721\n",
      "Epoch: 65/100... Step: 13000... Loss: 1.0675... Val Loss: 1.2301 acc:0.6364062428474426\n",
      "Epoch: 66/100... Step: 13010... Loss: 1.0182... Val Loss: 1.2297 acc:0.6379687786102295\n",
      "Epoch: 66/100... Step: 13020... Loss: 1.0226... Val Loss: 1.2201 acc:0.640625\n",
      "Epoch: 66/100... Step: 13030... Loss: 0.9914... Val Loss: 1.2298 acc:0.6373437643051147\n",
      "Epoch: 66/100... Step: 13040... Loss: 0.9932... Val Loss: 1.2251 acc:0.6403906345367432\n",
      "Epoch: 66/100... Step: 13050... Loss: 1.0148... Val Loss: 1.2248 acc:0.6380468606948853\n",
      "Epoch: 66/100... Step: 13060... Loss: 1.0175... Val Loss: 1.2211 acc:0.6400781273841858\n",
      "Epoch: 66/100... Step: 13070... Loss: 1.0049... Val Loss: 1.2286 acc:0.6392968893051147\n",
      "Epoch: 66/100... Step: 13080... Loss: 1.0187... Val Loss: 1.2300 acc:0.6391406059265137\n",
      "Epoch: 66/100... Step: 13090... Loss: 0.9998... Val Loss: 1.2205 acc:0.639843761920929\n",
      "Epoch: 66/100... Step: 13100... Loss: 1.0069... Val Loss: 1.2261 acc:0.6422656178474426\n",
      "Epoch: 66/100... Step: 13110... Loss: 1.0176... Val Loss: 1.2316 acc:0.6382812261581421\n",
      "Epoch: 66/100... Step: 13120... Loss: 0.9993... Val Loss: 1.2255 acc:0.639843761920929\n",
      "Epoch: 66/100... Step: 13130... Loss: 0.9967... Val Loss: 1.2276 acc:0.6396874785423279\n",
      "Epoch: 66/100... Step: 13140... Loss: 1.0203... Val Loss: 1.2226 acc:0.6403906345367432\n",
      "Epoch: 66/100... Step: 13150... Loss: 1.0206... Val Loss: 1.2280 acc:0.6390625238418579\n",
      "Epoch: 66/100... Step: 13160... Loss: 1.0093... Val Loss: 1.2255 acc:0.635546863079071\n",
      "Epoch: 66/100... Step: 13170... Loss: 1.0123... Val Loss: 1.2337 acc:0.6348437666893005\n",
      "Epoch: 66/100... Step: 13180... Loss: 1.0237... Val Loss: 1.2321 acc:0.6367968916893005\n",
      "Epoch: 66/100... Step: 13190... Loss: 1.0427... Val Loss: 1.2239 acc:0.639453113079071\n",
      "Epoch: 66/100... Step: 13200... Loss: 1.0679... Val Loss: 1.2286 acc:0.639453113079071\n",
      "Epoch: 67/100... Step: 13210... Loss: 1.0194... Val Loss: 1.2265 acc:0.6371874809265137\n",
      "Epoch: 67/100... Step: 13220... Loss: 1.0320... Val Loss: 1.2234 acc:0.6402343511581421\n",
      "Epoch: 67/100... Step: 13230... Loss: 0.9927... Val Loss: 1.2321 acc:0.6354687213897705\n",
      "Epoch: 67/100... Step: 13240... Loss: 0.9908... Val Loss: 1.2249 acc:0.6410156488418579\n",
      "Epoch: 67/100... Step: 13250... Loss: 1.0079... Val Loss: 1.2297 acc:0.6389843821525574\n",
      "Epoch: 67/100... Step: 13260... Loss: 1.0166... Val Loss: 1.2244 acc:0.6385937333106995\n",
      "Epoch: 67/100... Step: 13270... Loss: 1.0058... Val Loss: 1.2291 acc:0.6400781273841858\n",
      "Epoch: 67/100... Step: 13280... Loss: 1.0162... Val Loss: 1.2312 acc:0.6396874785423279\n",
      "Epoch: 67/100... Step: 13290... Loss: 1.0038... Val Loss: 1.2221 acc:0.6376562714576721\n",
      "Epoch: 67/100... Step: 13300... Loss: 0.9974... Val Loss: 1.2239 acc:0.6378124952316284\n",
      "Epoch: 67/100... Step: 13310... Loss: 1.0062... Val Loss: 1.2311 acc:0.6364843845367432\n",
      "Epoch: 67/100... Step: 13320... Loss: 0.9885... Val Loss: 1.2251 acc:0.6390625238418579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67/100... Step: 13330... Loss: 0.9985... Val Loss: 1.2286 acc:0.6369531154632568\n",
      "Epoch: 67/100... Step: 13340... Loss: 1.0210... Val Loss: 1.2203 acc:0.6401562690734863\n",
      "Epoch: 67/100... Step: 13350... Loss: 1.0217... Val Loss: 1.2307 acc:0.6380468606948853\n",
      "Epoch: 67/100... Step: 13360... Loss: 1.0168... Val Loss: 1.2225 acc:0.6385937333106995\n",
      "Epoch: 67/100... Step: 13370... Loss: 1.0052... Val Loss: 1.2289 acc:0.6378124952316284\n",
      "Epoch: 67/100... Step: 13380... Loss: 1.0061... Val Loss: 1.2276 acc:0.6357812285423279\n",
      "Epoch: 67/100... Step: 13390... Loss: 1.0288... Val Loss: 1.2260 acc:0.6370312571525574\n",
      "Epoch: 67/100... Step: 13400... Loss: 1.0692... Val Loss: 1.2258 acc:0.6376562714576721\n",
      "Epoch: 68/100... Step: 13410... Loss: 1.0112... Val Loss: 1.2248 acc:0.6389062404632568\n",
      "Epoch: 68/100... Step: 13420... Loss: 1.0230... Val Loss: 1.2240 acc:0.6407031416893005\n",
      "Epoch: 68/100... Step: 13430... Loss: 0.9907... Val Loss: 1.2295 acc:0.637499988079071\n",
      "Epoch: 68/100... Step: 13440... Loss: 0.9816... Val Loss: 1.2247 acc:0.6404687762260437\n",
      "Epoch: 68/100... Step: 13450... Loss: 1.0005... Val Loss: 1.2266 acc:0.6383593678474426\n",
      "Epoch: 68/100... Step: 13460... Loss: 1.0006... Val Loss: 1.2241 acc:0.6412500143051147\n",
      "Epoch: 68/100... Step: 13470... Loss: 0.9941... Val Loss: 1.2297 acc:0.6392968893051147\n",
      "Epoch: 68/100... Step: 13480... Loss: 1.0111... Val Loss: 1.2307 acc:0.6399219036102295\n",
      "Epoch: 68/100... Step: 13490... Loss: 0.9969... Val Loss: 1.2214 acc:0.6382812261581421\n",
      "Epoch: 68/100... Step: 13500... Loss: 1.0114... Val Loss: 1.2301 acc:0.6389843821525574\n",
      "Epoch: 68/100... Step: 13510... Loss: 1.0242... Val Loss: 1.2349 acc:0.6382031440734863\n",
      "Epoch: 68/100... Step: 13520... Loss: 0.9960... Val Loss: 1.2286 acc:0.6365625262260437\n",
      "Epoch: 68/100... Step: 13530... Loss: 0.9994... Val Loss: 1.2269 acc:0.6400781273841858\n",
      "Epoch: 68/100... Step: 13540... Loss: 1.0203... Val Loss: 1.2246 acc:0.6408593654632568\n",
      "Epoch: 68/100... Step: 13550... Loss: 1.0128... Val Loss: 1.2289 acc:0.6399999856948853\n",
      "Epoch: 68/100... Step: 13560... Loss: 1.0147... Val Loss: 1.2275 acc:0.6389062404632568\n",
      "Epoch: 68/100... Step: 13570... Loss: 1.0074... Val Loss: 1.2327 acc:0.6342187523841858\n",
      "Epoch: 68/100... Step: 13580... Loss: 1.0030... Val Loss: 1.2310 acc:0.637499988079071\n",
      "Epoch: 68/100... Step: 13590... Loss: 1.0349... Val Loss: 1.2271 acc:0.6389062404632568\n",
      "Epoch: 68/100... Step: 13600... Loss: 1.0652... Val Loss: 1.2295 acc:0.6389843821525574\n",
      "Epoch: 69/100... Step: 13610... Loss: 1.0167... Val Loss: 1.2278 acc:0.6393749713897705\n",
      "Epoch: 69/100... Step: 13620... Loss: 1.0219... Val Loss: 1.2264 acc:0.6403906345367432\n",
      "Epoch: 69/100... Step: 13630... Loss: 0.9947... Val Loss: 1.2324 acc:0.6399219036102295\n",
      "Epoch: 69/100... Step: 13640... Loss: 0.9855... Val Loss: 1.2296 acc:0.639453113079071\n",
      "Epoch: 69/100... Step: 13650... Loss: 1.0020... Val Loss: 1.2287 acc:0.6402343511581421\n",
      "Epoch: 69/100... Step: 13660... Loss: 1.0090... Val Loss: 1.2322 acc:0.641406238079071\n",
      "Epoch: 69/100... Step: 13670... Loss: 0.9920... Val Loss: 1.2326 acc:0.6403124928474426\n",
      "Epoch: 69/100... Step: 13680... Loss: 1.0146... Val Loss: 1.2317 acc:0.639843761920929\n",
      "Epoch: 69/100... Step: 13690... Loss: 0.9921... Val Loss: 1.2260 acc:0.6385937333106995\n",
      "Epoch: 69/100... Step: 13700... Loss: 1.0006... Val Loss: 1.2331 acc:0.6392968893051147\n",
      "Epoch: 69/100... Step: 13710... Loss: 1.0078... Val Loss: 1.2319 acc:0.6407031416893005\n",
      "Epoch: 69/100... Step: 13720... Loss: 0.9851... Val Loss: 1.2280 acc:0.6364843845367432\n",
      "Epoch: 69/100... Step: 13730... Loss: 0.9959... Val Loss: 1.2258 acc:0.6399999856948853\n",
      "Epoch: 69/100... Step: 13740... Loss: 1.0253... Val Loss: 1.2233 acc:0.6382031440734863\n",
      "Epoch: 69/100... Step: 13750... Loss: 1.0074... Val Loss: 1.2363 acc:0.635937511920929\n",
      "Epoch: 69/100... Step: 13760... Loss: 1.0139... Val Loss: 1.2276 acc:0.6365625262260437\n",
      "Epoch: 69/100... Step: 13770... Loss: 1.0143... Val Loss: 1.2304 acc:0.6366406083106995\n",
      "Epoch: 69/100... Step: 13780... Loss: 1.0043... Val Loss: 1.2283 acc:0.6379687786102295\n",
      "Epoch: 69/100... Step: 13790... Loss: 1.0251... Val Loss: 1.2262 acc:0.6390625238418579\n",
      "Epoch: 69/100... Step: 13800... Loss: 1.0573... Val Loss: 1.2300 acc:0.6366406083106995\n",
      "Epoch: 70/100... Step: 13810... Loss: 1.0160... Val Loss: 1.2293 acc:0.6366406083106995\n",
      "Epoch: 70/100... Step: 13820... Loss: 1.0258... Val Loss: 1.2251 acc:0.6382031440734863\n",
      "Epoch: 70/100... Step: 13830... Loss: 0.9777... Val Loss: 1.2311 acc:0.6340625286102295\n",
      "Epoch: 70/100... Step: 13840... Loss: 0.9960... Val Loss: 1.2249 acc:0.6389062404632568\n",
      "Epoch: 70/100... Step: 13850... Loss: 1.0053... Val Loss: 1.2268 acc:0.6389843821525574\n",
      "Epoch: 70/100... Step: 13860... Loss: 1.0149... Val Loss: 1.2290 acc:0.638671875\n",
      "Epoch: 70/100... Step: 13870... Loss: 0.9927... Val Loss: 1.2304 acc:0.6365625262260437\n",
      "Epoch: 70/100... Step: 13880... Loss: 1.0082... Val Loss: 1.2339 acc:0.6380468606948853\n",
      "Epoch: 70/100... Step: 13890... Loss: 0.9924... Val Loss: 1.2241 acc:0.6382031440734863\n",
      "Epoch: 70/100... Step: 13900... Loss: 0.9944... Val Loss: 1.2381 acc:0.638671875\n",
      "Epoch: 70/100... Step: 13910... Loss: 1.0111... Val Loss: 1.2321 acc:0.6399219036102295\n",
      "Epoch: 70/100... Step: 13920... Loss: 0.9862... Val Loss: 1.2313 acc:0.6390625238418579\n",
      "Epoch: 70/100... Step: 13930... Loss: 0.9847... Val Loss: 1.2278 acc:0.6393749713897705\n",
      "Epoch: 70/100... Step: 13940... Loss: 1.0165... Val Loss: 1.2261 acc:0.6396874785423279\n",
      "Epoch: 70/100... Step: 13950... Loss: 1.0097... Val Loss: 1.2341 acc:0.6382812261581421\n",
      "Epoch: 70/100... Step: 13960... Loss: 1.0061... Val Loss: 1.2301 acc:0.637499988079071\n",
      "Epoch: 70/100... Step: 13970... Loss: 1.0115... Val Loss: 1.2339 acc:0.6383593678474426\n",
      "Epoch: 70/100... Step: 13980... Loss: 1.0143... Val Loss: 1.2321 acc:0.6360937356948853\n",
      "Epoch: 70/100... Step: 13990... Loss: 1.0149... Val Loss: 1.2295 acc:0.6385156512260437\n",
      "Epoch: 70/100... Step: 14000... Loss: 1.0573... Val Loss: 1.2300 acc:0.6357812285423279\n",
      "Epoch: 71/100... Step: 14010... Loss: 1.0091... Val Loss: 1.2325 acc:0.634765625\n",
      "Epoch: 71/100... Step: 14020... Loss: 1.0222... Val Loss: 1.2302 acc:0.6377343535423279\n",
      "Epoch: 71/100... Step: 14030... Loss: 0.9894... Val Loss: 1.2383 acc:0.6362500190734863\n",
      "Epoch: 71/100... Step: 14040... Loss: 0.9876... Val Loss: 1.2290 acc:0.635546863079071\n",
      "Epoch: 71/100... Step: 14050... Loss: 1.0002... Val Loss: 1.2318 acc:0.6385937333106995\n",
      "Epoch: 71/100... Step: 14060... Loss: 1.0124... Val Loss: 1.2316 acc:0.6361718773841858\n",
      "Epoch: 71/100... Step: 14070... Loss: 0.9811... Val Loss: 1.2339 acc:0.637890636920929\n",
      "Epoch: 71/100... Step: 14080... Loss: 1.0145... Val Loss: 1.2312 acc:0.6379687786102295\n",
      "Epoch: 71/100... Step: 14090... Loss: 0.9924... Val Loss: 1.2256 acc:0.6387500166893005\n",
      "Epoch: 71/100... Step: 14100... Loss: 0.9902... Val Loss: 1.2361 acc:0.6397656202316284\n",
      "Epoch: 71/100... Step: 14110... Loss: 1.0093... Val Loss: 1.2302 acc:0.6403124928474426\n",
      "Epoch: 71/100... Step: 14120... Loss: 0.9876... Val Loss: 1.2324 acc:0.6364843845367432\n",
      "Epoch: 71/100... Step: 14130... Loss: 0.9883... Val Loss: 1.2223 acc:0.6383593678474426\n",
      "Epoch: 71/100... Step: 14140... Loss: 1.0104... Val Loss: 1.2263 acc:0.6369531154632568\n",
      "Epoch: 71/100... Step: 14150... Loss: 1.0113... Val Loss: 1.2313 acc:0.6393749713897705\n",
      "Epoch: 71/100... Step: 14160... Loss: 1.0144... Val Loss: 1.2310 acc:0.6353124976158142\n",
      "Epoch: 71/100... Step: 14170... Loss: 1.0004... Val Loss: 1.2341 acc:0.6367968916893005\n",
      "Epoch: 71/100... Step: 14180... Loss: 1.0008... Val Loss: 1.2323 acc:0.6343749761581421\n",
      "Epoch: 71/100... Step: 14190... Loss: 1.0303... Val Loss: 1.2269 acc:0.6376562714576721\n",
      "Epoch: 71/100... Step: 14200... Loss: 1.0631... Val Loss: 1.2294 acc:0.6364062428474426\n",
      "Epoch: 72/100... Step: 14210... Loss: 1.0083... Val Loss: 1.2336 acc:0.6356250047683716\n",
      "Epoch: 72/100... Step: 14220... Loss: 1.0183... Val Loss: 1.2293 acc:0.6366406083106995\n",
      "Epoch: 72/100... Step: 14230... Loss: 0.9831... Val Loss: 1.2397 acc:0.6370312571525574\n",
      "Epoch: 72/100... Step: 14240... Loss: 0.9905... Val Loss: 1.2276 acc:0.6381250023841858\n",
      "Epoch: 72/100... Step: 14250... Loss: 0.9956... Val Loss: 1.2335 acc:0.6382031440734863\n",
      "Epoch: 72/100... Step: 14260... Loss: 1.0090... Val Loss: 1.2290 acc:0.6408593654632568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72/100... Step: 14270... Loss: 0.9872... Val Loss: 1.2323 acc:0.6395312547683716\n",
      "Epoch: 72/100... Step: 14280... Loss: 1.0060... Val Loss: 1.2325 acc:0.6392187476158142\n",
      "Epoch: 72/100... Step: 14290... Loss: 0.9951... Val Loss: 1.2258 acc:0.6365625262260437\n",
      "Epoch: 72/100... Step: 14300... Loss: 0.9886... Val Loss: 1.2419 acc:0.6378124952316284\n",
      "Epoch: 72/100... Step: 14310... Loss: 1.0030... Val Loss: 1.2342 acc:0.6411718726158142\n",
      "Epoch: 72/100... Step: 14320... Loss: 0.9833... Val Loss: 1.2325 acc:0.6384375095367432\n",
      "Epoch: 72/100... Step: 14330... Loss: 0.9945... Val Loss: 1.2267 acc:0.6385937333106995\n",
      "Epoch: 72/100... Step: 14340... Loss: 1.0186... Val Loss: 1.2269 acc:0.6403906345367432\n",
      "Epoch: 72/100... Step: 14350... Loss: 1.0079... Val Loss: 1.2350 acc:0.6384375095367432\n",
      "Epoch: 72/100... Step: 14360... Loss: 1.0073... Val Loss: 1.2308 acc:0.6360156536102295\n",
      "Epoch: 72/100... Step: 14370... Loss: 0.9949... Val Loss: 1.2361 acc:0.6353906393051147\n",
      "Epoch: 72/100... Step: 14380... Loss: 1.0066... Val Loss: 1.2314 acc:0.6366406083106995\n",
      "Epoch: 72/100... Step: 14390... Loss: 1.0249... Val Loss: 1.2304 acc:0.6379687786102295\n",
      "Epoch: 72/100... Step: 14400... Loss: 1.0614... Val Loss: 1.2288 acc:0.6389843821525574\n",
      "Epoch: 73/100... Step: 14410... Loss: 1.0045... Val Loss: 1.2362 acc:0.6380468606948853\n",
      "Epoch: 73/100... Step: 14420... Loss: 1.0215... Val Loss: 1.2291 acc:0.6410156488418579\n",
      "Epoch: 73/100... Step: 14430... Loss: 0.9780... Val Loss: 1.2317 acc:0.639453113079071\n",
      "Epoch: 73/100... Step: 14440... Loss: 0.9866... Val Loss: 1.2311 acc:0.6384375095367432\n",
      "Epoch: 73/100... Step: 14450... Loss: 1.0003... Val Loss: 1.2332 acc:0.6368749737739563\n",
      "Epoch: 73/100... Step: 14460... Loss: 0.9944... Val Loss: 1.2333 acc:0.6372656226158142\n",
      "Epoch: 73/100... Step: 14470... Loss: 0.9911... Val Loss: 1.2325 acc:0.6374218463897705\n",
      "Epoch: 73/100... Step: 14480... Loss: 0.9943... Val Loss: 1.2321 acc:0.6399999856948853\n",
      "Epoch: 73/100... Step: 14490... Loss: 0.9914... Val Loss: 1.2271 acc:0.635546863079071\n",
      "Epoch: 73/100... Step: 14500... Loss: 0.9859... Val Loss: 1.2404 acc:0.6379687786102295\n",
      "Epoch: 73/100... Step: 14510... Loss: 1.0049... Val Loss: 1.2348 acc:0.6376562714576721\n",
      "Epoch: 73/100... Step: 14520... Loss: 0.9763... Val Loss: 1.2350 acc:0.6356250047683716\n",
      "Epoch: 73/100... Step: 14530... Loss: 0.9888... Val Loss: 1.2293 acc:0.6399999856948853\n",
      "Epoch: 73/100... Step: 14540... Loss: 1.0055... Val Loss: 1.2306 acc:0.6357812285423279\n",
      "Epoch: 73/100... Step: 14550... Loss: 0.9940... Val Loss: 1.2337 acc:0.6371093988418579\n",
      "Epoch: 73/100... Step: 14560... Loss: 1.0110... Val Loss: 1.2337 acc:0.6348437666893005\n",
      "Epoch: 73/100... Step: 14570... Loss: 1.0047... Val Loss: 1.2368 acc:0.635937511920929\n",
      "Epoch: 73/100... Step: 14580... Loss: 1.0012... Val Loss: 1.2326 acc:0.6374218463897705\n",
      "Epoch: 73/100... Step: 14590... Loss: 1.0221... Val Loss: 1.2296 acc:0.6368749737739563\n",
      "Epoch: 73/100... Step: 14600... Loss: 1.0595... Val Loss: 1.2330 acc:0.6365625262260437\n",
      "Epoch: 74/100... Step: 14610... Loss: 1.0033... Val Loss: 1.2335 acc:0.6380468606948853\n",
      "Epoch: 74/100... Step: 14620... Loss: 1.0194... Val Loss: 1.2312 acc:0.6385156512260437\n",
      "Epoch: 74/100... Step: 14630... Loss: 0.9785... Val Loss: 1.2320 acc:0.6370312571525574\n",
      "Epoch: 74/100... Step: 14640... Loss: 0.9871... Val Loss: 1.2336 acc:0.6382031440734863\n",
      "Epoch: 74/100... Step: 14650... Loss: 0.9932... Val Loss: 1.2326 acc:0.6381250023841858\n",
      "Epoch: 74/100... Step: 14660... Loss: 0.9986... Val Loss: 1.2331 acc:0.6349218487739563\n",
      "Epoch: 74/100... Step: 14670... Loss: 0.9759... Val Loss: 1.2341 acc:0.6361718773841858\n",
      "Epoch: 74/100... Step: 14680... Loss: 1.0040... Val Loss: 1.2312 acc:0.6369531154632568\n",
      "Epoch: 74/100... Step: 14690... Loss: 0.9871... Val Loss: 1.2284 acc:0.6367968916893005\n",
      "Epoch: 74/100... Step: 14700... Loss: 0.9905... Val Loss: 1.2441 acc:0.6371874809265137\n",
      "Epoch: 74/100... Step: 14710... Loss: 1.0090... Val Loss: 1.2343 acc:0.63671875\n",
      "Epoch: 74/100... Step: 14720... Loss: 0.9791... Val Loss: 1.2305 acc:0.6378124952316284\n",
      "Epoch: 74/100... Step: 14730... Loss: 0.9770... Val Loss: 1.2313 acc:0.6397656202316284\n",
      "Epoch: 74/100... Step: 14740... Loss: 1.0150... Val Loss: 1.2281 acc:0.6389843821525574\n",
      "Epoch: 74/100... Step: 14750... Loss: 0.9997... Val Loss: 1.2382 acc:0.6375781297683716\n",
      "Epoch: 74/100... Step: 14760... Loss: 1.0107... Val Loss: 1.2324 acc:0.6370312571525574\n",
      "Epoch: 74/100... Step: 14770... Loss: 0.9876... Val Loss: 1.2357 acc:0.6373437643051147\n",
      "Epoch: 74/100... Step: 14780... Loss: 1.0069... Val Loss: 1.2310 acc:0.6389062404632568\n",
      "Epoch: 74/100... Step: 14790... Loss: 1.0179... Val Loss: 1.2303 acc:0.639453113079071\n",
      "Epoch: 74/100... Step: 14800... Loss: 1.0525... Val Loss: 1.2311 acc:0.635937511920929\n",
      "Epoch: 75/100... Step: 14810... Loss: 1.0008... Val Loss: 1.2338 acc:0.637890636920929\n",
      "Epoch: 75/100... Step: 14820... Loss: 1.0088... Val Loss: 1.2316 acc:0.6385156512260437\n",
      "Epoch: 75/100... Step: 14830... Loss: 0.9786... Val Loss: 1.2351 acc:0.63671875\n",
      "Epoch: 75/100... Step: 14840... Loss: 0.9838... Val Loss: 1.2314 acc:0.6393749713897705\n",
      "Epoch: 75/100... Step: 14850... Loss: 0.9906... Val Loss: 1.2338 acc:0.6360156536102295\n",
      "Epoch: 75/100... Step: 14860... Loss: 1.0082... Val Loss: 1.2350 acc:0.6371093988418579\n",
      "Epoch: 75/100... Step: 14870... Loss: 0.9794... Val Loss: 1.2387 acc:0.639453113079071\n",
      "Epoch: 75/100... Step: 14880... Loss: 1.0011... Val Loss: 1.2364 acc:0.6377343535423279\n",
      "Epoch: 75/100... Step: 14890... Loss: 0.9930... Val Loss: 1.2309 acc:0.6382812261581421\n",
      "Epoch: 75/100... Step: 14900... Loss: 0.9863... Val Loss: 1.2458 acc:0.6360937356948853\n",
      "Epoch: 75/100... Step: 14910... Loss: 1.0006... Val Loss: 1.2348 acc:0.6372656226158142\n",
      "Epoch: 75/100... Step: 14920... Loss: 0.9742... Val Loss: 1.2382 acc:0.6374218463897705\n",
      "Epoch: 75/100... Step: 14930... Loss: 0.9812... Val Loss: 1.2269 acc:0.6380468606948853\n",
      "Epoch: 75/100... Step: 14940... Loss: 1.0088... Val Loss: 1.2322 acc:0.6377343535423279\n",
      "Epoch: 75/100... Step: 14950... Loss: 1.0039... Val Loss: 1.2395 acc:0.6365625262260437\n",
      "Epoch: 75/100... Step: 14960... Loss: 1.0031... Val Loss: 1.2355 acc:0.634765625\n",
      "Epoch: 75/100... Step: 14970... Loss: 0.9990... Val Loss: 1.2361 acc:0.6343749761581421\n",
      "Epoch: 75/100... Step: 14980... Loss: 0.9978... Val Loss: 1.2329 acc:0.6371874809265137\n",
      "Epoch: 75/100... Step: 14990... Loss: 1.0192... Val Loss: 1.2348 acc:0.6349218487739563\n",
      "Epoch: 75/100... Step: 15000... Loss: 1.0552... Val Loss: 1.2332 acc:0.6341406106948853\n",
      "Epoch: 76/100... Step: 15010... Loss: 0.9998... Val Loss: 1.2397 acc:0.6365625262260437\n",
      "Epoch: 76/100... Step: 15020... Loss: 1.0225... Val Loss: 1.2352 acc:0.6424219012260437\n",
      "Epoch: 76/100... Step: 15030... Loss: 0.9649... Val Loss: 1.2360 acc:0.6364062428474426\n",
      "Epoch: 76/100... Step: 15040... Loss: 0.9864... Val Loss: 1.2319 acc:0.6375781297683716\n",
      "Epoch: 76/100... Step: 15050... Loss: 0.9926... Val Loss: 1.2360 acc:0.6378124952316284\n",
      "Epoch: 76/100... Step: 15060... Loss: 0.9928... Val Loss: 1.2350 acc:0.6403124928474426\n",
      "Epoch: 76/100... Step: 15070... Loss: 0.9782... Val Loss: 1.2364 acc:0.6410937309265137\n",
      "Epoch: 76/100... Step: 15080... Loss: 0.9986... Val Loss: 1.2347 acc:0.6407812237739563\n",
      "Epoch: 76/100... Step: 15090... Loss: 0.9894... Val Loss: 1.2319 acc:0.639843761920929\n",
      "Epoch: 76/100... Step: 15100... Loss: 0.9829... Val Loss: 1.2460 acc:0.6378124952316284\n",
      "Epoch: 76/100... Step: 15110... Loss: 1.0066... Val Loss: 1.2352 acc:0.6378124952316284\n",
      "Epoch: 76/100... Step: 15120... Loss: 0.9841... Val Loss: 1.2379 acc:0.6397656202316284\n",
      "Epoch: 76/100... Step: 15130... Loss: 0.9813... Val Loss: 1.2319 acc:0.6399219036102295\n",
      "Epoch: 76/100... Step: 15140... Loss: 1.0037... Val Loss: 1.2321 acc:0.6393749713897705\n",
      "Epoch: 76/100... Step: 15150... Loss: 1.0104... Val Loss: 1.2390 acc:0.6368749737739563\n",
      "Epoch: 76/100... Step: 15160... Loss: 0.9994... Val Loss: 1.2331 acc:0.6375781297683716\n",
      "Epoch: 76/100... Step: 15170... Loss: 1.0015... Val Loss: 1.2394 acc:0.6349218487739563\n",
      "Epoch: 76/100... Step: 15180... Loss: 0.9948... Val Loss: 1.2325 acc:0.6365625262260437\n",
      "Epoch: 76/100... Step: 15190... Loss: 1.0150... Val Loss: 1.2371 acc:0.6366406083106995\n",
      "Epoch: 76/100... Step: 15200... Loss: 1.0632... Val Loss: 1.2322 acc:0.6352343559265137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77/100... Step: 15210... Loss: 1.0012... Val Loss: 1.2398 acc:0.6358593702316284\n",
      "Epoch: 77/100... Step: 15220... Loss: 1.0125... Val Loss: 1.2398 acc:0.6369531154632568\n",
      "Epoch: 77/100... Step: 15230... Loss: 0.9728... Val Loss: 1.2333 acc:0.639843761920929\n",
      "Epoch: 77/100... Step: 15240... Loss: 0.9731... Val Loss: 1.2358 acc:0.6393749713897705\n",
      "Epoch: 77/100... Step: 15250... Loss: 0.9898... Val Loss: 1.2374 acc:0.6372656226158142\n",
      "Epoch: 77/100... Step: 15260... Loss: 0.9914... Val Loss: 1.2368 acc:0.6374218463897705\n",
      "Epoch: 77/100... Step: 15270... Loss: 0.9817... Val Loss: 1.2392 acc:0.6373437643051147\n",
      "Epoch: 77/100... Step: 15280... Loss: 0.9950... Val Loss: 1.2366 acc:0.6392187476158142\n",
      "Epoch: 77/100... Step: 15290... Loss: 0.9767... Val Loss: 1.2360 acc:0.6371874809265137\n",
      "Epoch: 77/100... Step: 15300... Loss: 0.9850... Val Loss: 1.2494 acc:0.6378124952316284\n",
      "Epoch: 77/100... Step: 15310... Loss: 0.9947... Val Loss: 1.2374 acc:0.6371874809265137\n",
      "Epoch: 77/100... Step: 15320... Loss: 0.9818... Val Loss: 1.2416 acc:0.6357031464576721\n",
      "Epoch: 77/100... Step: 15330... Loss: 0.9829... Val Loss: 1.2330 acc:0.63671875\n",
      "Epoch: 77/100... Step: 15340... Loss: 0.9964... Val Loss: 1.2371 acc:0.6376562714576721\n",
      "Epoch: 77/100... Step: 15350... Loss: 0.9992... Val Loss: 1.2440 acc:0.6364062428474426\n",
      "Epoch: 77/100... Step: 15360... Loss: 1.0057... Val Loss: 1.2351 acc:0.6389843821525574\n",
      "Epoch: 77/100... Step: 15370... Loss: 0.9926... Val Loss: 1.2402 acc:0.6353124976158142\n",
      "Epoch: 77/100... Step: 15380... Loss: 0.9956... Val Loss: 1.2409 acc:0.6358593702316284\n",
      "Epoch: 77/100... Step: 15390... Loss: 1.0080... Val Loss: 1.2356 acc:0.6374218463897705\n",
      "Epoch: 77/100... Step: 15400... Loss: 1.0565... Val Loss: 1.2321 acc:0.6382031440734863\n",
      "Epoch: 78/100... Step: 15410... Loss: 0.9879... Val Loss: 1.2392 acc:0.6371093988418579\n",
      "Epoch: 78/100... Step: 15420... Loss: 1.0214... Val Loss: 1.2375 acc:0.6361718773841858\n",
      "Epoch: 78/100... Step: 15430... Loss: 0.9765... Val Loss: 1.2375 acc:0.6401562690734863\n",
      "Epoch: 78/100... Step: 15440... Loss: 0.9852... Val Loss: 1.2383 acc:0.6366406083106995\n",
      "Epoch: 78/100... Step: 15450... Loss: 0.9832... Val Loss: 1.2387 acc:0.638671875\n",
      "Epoch: 78/100... Step: 15460... Loss: 0.9871... Val Loss: 1.2359 acc:0.641406238079071\n",
      "Epoch: 78/100... Step: 15470... Loss: 0.9720... Val Loss: 1.2403 acc:0.6384375095367432\n",
      "Epoch: 78/100... Step: 15480... Loss: 0.9951... Val Loss: 1.2349 acc:0.6367968916893005\n",
      "Epoch: 78/100... Step: 15490... Loss: 0.9740... Val Loss: 1.2353 acc:0.6383593678474426\n",
      "Epoch: 78/100... Step: 15500... Loss: 0.9824... Val Loss: 1.2479 acc:0.6364843845367432\n",
      "Epoch: 78/100... Step: 15510... Loss: 1.0052... Val Loss: 1.2403 acc:0.637890636920929\n",
      "Epoch: 78/100... Step: 15520... Loss: 0.9759... Val Loss: 1.2394 acc:0.6373437643051147\n",
      "Epoch: 78/100... Step: 15530... Loss: 0.9809... Val Loss: 1.2336 acc:0.6388280987739563\n",
      "Epoch: 78/100... Step: 15540... Loss: 1.0039... Val Loss: 1.2386 acc:0.6404687762260437\n",
      "Epoch: 78/100... Step: 15550... Loss: 0.9893... Val Loss: 1.2423 acc:0.6372656226158142\n",
      "Epoch: 78/100... Step: 15560... Loss: 1.0005... Val Loss: 1.2336 acc:0.6399219036102295\n",
      "Epoch: 78/100... Step: 15570... Loss: 0.9925... Val Loss: 1.2391 acc:0.6357812285423279\n",
      "Epoch: 78/100... Step: 15580... Loss: 0.9935... Val Loss: 1.2362 acc:0.6383593678474426\n",
      "Epoch: 78/100... Step: 15590... Loss: 1.0110... Val Loss: 1.2397 acc:0.6382812261581421\n",
      "Epoch: 78/100... Step: 15600... Loss: 1.0491... Val Loss: 1.2371 acc:0.6365625262260437\n",
      "Epoch: 79/100... Step: 15610... Loss: 0.9949... Val Loss: 1.2412 acc:0.6374218463897705\n",
      "Epoch: 79/100... Step: 15620... Loss: 1.0013... Val Loss: 1.2343 acc:0.6412500143051147\n",
      "Epoch: 79/100... Step: 15630... Loss: 0.9732... Val Loss: 1.2349 acc:0.6396093964576721\n",
      "Epoch: 79/100... Step: 15640... Loss: 0.9743... Val Loss: 1.2370 acc:0.6379687786102295\n",
      "Epoch: 79/100... Step: 15650... Loss: 0.9832... Val Loss: 1.2394 acc:0.6373437643051147\n",
      "Epoch: 79/100... Step: 15660... Loss: 0.9959... Val Loss: 1.2364 acc:0.6374218463897705\n",
      "Epoch: 79/100... Step: 15670... Loss: 0.9726... Val Loss: 1.2396 acc:0.6366406083106995\n",
      "Epoch: 79/100... Step: 15680... Loss: 0.9925... Val Loss: 1.2363 acc:0.6389062404632568\n",
      "Epoch: 79/100... Step: 15690... Loss: 0.9754... Val Loss: 1.2379 acc:0.6372656226158142\n",
      "Epoch: 79/100... Step: 15700... Loss: 0.9706... Val Loss: 1.2511 acc:0.633984386920929\n",
      "Epoch: 79/100... Step: 15710... Loss: 0.9918... Val Loss: 1.2361 acc:0.6372656226158142\n",
      "Epoch: 79/100... Step: 15720... Loss: 0.9800... Val Loss: 1.2445 acc:0.6360937356948853\n",
      "Epoch: 79/100... Step: 15730... Loss: 0.9700... Val Loss: 1.2324 acc:0.6360156536102295\n",
      "Epoch: 79/100... Step: 15740... Loss: 1.0063... Val Loss: 1.2382 acc:0.6384375095367432\n",
      "Epoch: 79/100... Step: 15750... Loss: 1.0018... Val Loss: 1.2454 acc:0.6336718797683716\n",
      "Epoch: 79/100... Step: 15760... Loss: 1.0027... Val Loss: 1.2354 acc:0.6357031464576721\n",
      "Epoch: 79/100... Step: 15770... Loss: 0.9872... Val Loss: 1.2414 acc:0.6324999928474426\n",
      "Epoch: 79/100... Step: 15780... Loss: 0.9876... Val Loss: 1.2373 acc:0.6373437643051147\n",
      "Epoch: 79/100... Step: 15790... Loss: 1.0154... Val Loss: 1.2433 acc:0.6371874809265137\n",
      "Epoch: 79/100... Step: 15800... Loss: 1.0478... Val Loss: 1.2371 acc:0.6364843845367432\n",
      "Epoch: 80/100... Step: 15810... Loss: 0.9960... Val Loss: 1.2456 acc:0.631640613079071\n",
      "Epoch: 80/100... Step: 15820... Loss: 1.0024... Val Loss: 1.2382 acc:0.6353906393051147\n",
      "Epoch: 80/100... Step: 15830... Loss: 0.9692... Val Loss: 1.2387 acc:0.6380468606948853\n",
      "Epoch: 80/100... Step: 15840... Loss: 0.9783... Val Loss: 1.2363 acc:0.6381250023841858\n",
      "Epoch: 80/100... Step: 15850... Loss: 0.9844... Val Loss: 1.2407 acc:0.6385156512260437\n",
      "Epoch: 80/100... Step: 15860... Loss: 0.9889... Val Loss: 1.2394 acc:0.6389062404632568\n",
      "Epoch: 80/100... Step: 15870... Loss: 0.9702... Val Loss: 1.2404 acc:0.6365625262260437\n",
      "Epoch: 80/100... Step: 15880... Loss: 0.9894... Val Loss: 1.2363 acc:0.6383593678474426\n",
      "Epoch: 80/100... Step: 15890... Loss: 0.9853... Val Loss: 1.2371 acc:0.6392187476158142\n",
      "Epoch: 80/100... Step: 15900... Loss: 0.9809... Val Loss: 1.2604 acc:0.6385156512260437\n",
      "Epoch: 80/100... Step: 15910... Loss: 0.9997... Val Loss: 1.2373 acc:0.6395312547683716\n",
      "Epoch: 80/100... Step: 15920... Loss: 0.9735... Val Loss: 1.2448 acc:0.6358593702316284\n",
      "Epoch: 80/100... Step: 15930... Loss: 0.9731... Val Loss: 1.2349 acc:0.641406238079071\n",
      "Epoch: 80/100... Step: 15940... Loss: 0.9981... Val Loss: 1.2362 acc:0.6387500166893005\n",
      "Epoch: 80/100... Step: 15950... Loss: 0.9929... Val Loss: 1.2397 acc:0.6361718773841858\n",
      "Epoch: 80/100... Step: 15960... Loss: 0.9949... Val Loss: 1.2372 acc:0.6366406083106995\n",
      "Epoch: 80/100... Step: 15970... Loss: 0.9879... Val Loss: 1.2425 acc:0.6333593726158142\n",
      "Epoch: 80/100... Step: 15980... Loss: 0.9907... Val Loss: 1.2428 acc:0.635546863079071\n",
      "Epoch: 80/100... Step: 15990... Loss: 1.0038... Val Loss: 1.2395 acc:0.6377343535423279\n",
      "Epoch: 80/100... Step: 16000... Loss: 1.0376... Val Loss: 1.2390 acc:0.6369531154632568\n",
      "Epoch: 81/100... Step: 16010... Loss: 0.9938... Val Loss: 1.2416 acc:0.6369531154632568\n",
      "Epoch: 81/100... Step: 16020... Loss: 1.0000... Val Loss: 1.2364 acc:0.638671875\n",
      "Epoch: 81/100... Step: 16030... Loss: 0.9550... Val Loss: 1.2418 acc:0.6375781297683716\n",
      "Epoch: 81/100... Step: 16040... Loss: 0.9730... Val Loss: 1.2387 acc:0.6381250023841858\n",
      "Epoch: 81/100... Step: 16050... Loss: 0.9793... Val Loss: 1.2423 acc:0.6374218463897705\n",
      "Epoch: 81/100... Step: 16060... Loss: 0.9775... Val Loss: 1.2436 acc:0.6363281011581421\n",
      "Epoch: 81/100... Step: 16070... Loss: 0.9762... Val Loss: 1.2442 acc:0.6374218463897705\n",
      "Epoch: 81/100... Step: 16080... Loss: 0.9884... Val Loss: 1.2423 acc:0.6403906345367432\n",
      "Epoch: 81/100... Step: 16090... Loss: 0.9764... Val Loss: 1.2384 acc:0.6390625238418579\n",
      "Epoch: 81/100... Step: 16100... Loss: 0.9739... Val Loss: 1.2575 acc:0.6396093964576721\n",
      "Epoch: 81/100... Step: 16110... Loss: 0.9965... Val Loss: 1.2377 acc:0.6409375071525574\n",
      "Epoch: 81/100... Step: 16120... Loss: 0.9648... Val Loss: 1.2502 acc:0.6378124952316284\n",
      "Epoch: 81/100... Step: 16130... Loss: 0.9788... Val Loss: 1.2401 acc:0.6410937309265137\n",
      "Epoch: 81/100... Step: 16140... Loss: 0.9922... Val Loss: 1.2425 acc:0.6379687786102295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81/100... Step: 16150... Loss: 0.9935... Val Loss: 1.2415 acc:0.6346874833106995\n",
      "Epoch: 81/100... Step: 16160... Loss: 0.9910... Val Loss: 1.2383 acc:0.6385156512260437\n",
      "Epoch: 81/100... Step: 16170... Loss: 0.9792... Val Loss: 1.2447 acc:0.6374218463897705\n",
      "Epoch: 81/100... Step: 16180... Loss: 0.9885... Val Loss: 1.2441 acc:0.6377343535423279\n",
      "Epoch: 81/100... Step: 16190... Loss: 0.9991... Val Loss: 1.2456 acc:0.6376562714576721\n",
      "Epoch: 81/100... Step: 16200... Loss: 1.0453... Val Loss: 1.2366 acc:0.6374218463897705\n",
      "Epoch: 82/100... Step: 16210... Loss: 0.9943... Val Loss: 1.2442 acc:0.637890636920929\n",
      "Epoch: 82/100... Step: 16220... Loss: 0.9959... Val Loss: 1.2401 acc:0.6399999856948853\n",
      "Epoch: 82/100... Step: 16230... Loss: 0.9667... Val Loss: 1.2421 acc:0.639453113079071\n",
      "Epoch: 82/100... Step: 16240... Loss: 0.9838... Val Loss: 1.2395 acc:0.6391406059265137\n",
      "Epoch: 82/100... Step: 16250... Loss: 0.9796... Val Loss: 1.2466 acc:0.6349218487739563\n",
      "Epoch: 82/100... Step: 16260... Loss: 0.9879... Val Loss: 1.2427 acc:0.6361718773841858\n",
      "Epoch: 82/100... Step: 16270... Loss: 0.9684... Val Loss: 1.2437 acc:0.6378124952316284\n",
      "Epoch: 82/100... Step: 16280... Loss: 0.9843... Val Loss: 1.2391 acc:0.6378124952316284\n",
      "Epoch: 82/100... Step: 16290... Loss: 0.9805... Val Loss: 1.2404 acc:0.637499988079071\n",
      "Epoch: 82/100... Step: 16300... Loss: 0.9783... Val Loss: 1.2588 acc:0.6361718773841858\n",
      "Epoch: 82/100... Step: 16310... Loss: 0.9898... Val Loss: 1.2381 acc:0.6404687762260437\n",
      "Epoch: 82/100... Step: 16320... Loss: 0.9707... Val Loss: 1.2525 acc:0.6366406083106995\n",
      "Epoch: 82/100... Step: 16330... Loss: 0.9767... Val Loss: 1.2407 acc:0.6393749713897705\n",
      "Epoch: 82/100... Step: 16340... Loss: 0.9924... Val Loss: 1.2414 acc:0.6380468606948853\n",
      "Epoch: 82/100... Step: 16350... Loss: 0.9952... Val Loss: 1.2420 acc:0.637890636920929\n",
      "Epoch: 82/100... Step: 16360... Loss: 0.9962... Val Loss: 1.2406 acc:0.6370312571525574\n",
      "Epoch: 82/100... Step: 16370... Loss: 0.9870... Val Loss: 1.2506 acc:0.635546863079071\n",
      "Epoch: 82/100... Step: 16380... Loss: 0.9800... Val Loss: 1.2395 acc:0.6372656226158142\n",
      "Epoch: 82/100... Step: 16390... Loss: 1.0029... Val Loss: 1.2458 acc:0.635937511920929\n",
      "Epoch: 82/100... Step: 16400... Loss: 1.0364... Val Loss: 1.2389 acc:0.6369531154632568\n",
      "Epoch: 83/100... Step: 16410... Loss: 0.9825... Val Loss: 1.2460 acc:0.6378124952316284\n",
      "Epoch: 83/100... Step: 16420... Loss: 1.0062... Val Loss: 1.2425 acc:0.6378124952316284\n",
      "Epoch: 83/100... Step: 16430... Loss: 0.9654... Val Loss: 1.2427 acc:0.6390625238418579\n",
      "Epoch: 83/100... Step: 16440... Loss: 0.9656... Val Loss: 1.2388 acc:0.639453113079071\n",
      "Epoch: 83/100... Step: 16450... Loss: 0.9836... Val Loss: 1.2418 acc:0.6377343535423279\n",
      "Epoch: 83/100... Step: 16460... Loss: 0.9789... Val Loss: 1.2432 acc:0.6377343535423279\n",
      "Epoch: 83/100... Step: 16470... Loss: 0.9664... Val Loss: 1.2450 acc:0.637499988079071\n",
      "Epoch: 83/100... Step: 16480... Loss: 0.9874... Val Loss: 1.2459 acc:0.6358593702316284\n",
      "Epoch: 83/100... Step: 16490... Loss: 0.9721... Val Loss: 1.2442 acc:0.6360937356948853\n",
      "Epoch: 83/100... Step: 16500... Loss: 0.9785... Val Loss: 1.2623 acc:0.6371093988418579\n",
      "Epoch: 83/100... Step: 16510... Loss: 0.9939... Val Loss: 1.2386 acc:0.6378124952316284\n",
      "Epoch: 83/100... Step: 16520... Loss: 0.9629... Val Loss: 1.2526 acc:0.6361718773841858\n",
      "Epoch: 83/100... Step: 16530... Loss: 0.9766... Val Loss: 1.2448 acc:0.6379687786102295\n",
      "Epoch: 83/100... Step: 16540... Loss: 0.9884... Val Loss: 1.2462 acc:0.6417187452316284\n",
      "Epoch: 83/100... Step: 16550... Loss: 0.9921... Val Loss: 1.2435 acc:0.6376562714576721\n",
      "Epoch: 83/100... Step: 16560... Loss: 0.9843... Val Loss: 1.2410 acc:0.6372656226158142\n",
      "Epoch: 83/100... Step: 16570... Loss: 0.9856... Val Loss: 1.2538 acc:0.6360156536102295\n",
      "Epoch: 83/100... Step: 16580... Loss: 0.9861... Val Loss: 1.2432 acc:0.6377343535423279\n",
      "Epoch: 83/100... Step: 16590... Loss: 1.0047... Val Loss: 1.2482 acc:0.6376562714576721\n",
      "Epoch: 83/100... Step: 16600... Loss: 1.0399... Val Loss: 1.2413 acc:0.6380468606948853\n",
      "Epoch: 84/100... Step: 16610... Loss: 0.9849... Val Loss: 1.2479 acc:0.6346094012260437\n",
      "Epoch: 84/100... Step: 16620... Loss: 0.9962... Val Loss: 1.2436 acc:0.6392187476158142\n",
      "Epoch: 84/100... Step: 16630... Loss: 0.9687... Val Loss: 1.2429 acc:0.6382812261581421\n",
      "Epoch: 84/100... Step: 16640... Loss: 0.9666... Val Loss: 1.2446 acc:0.6389062404632568\n",
      "Epoch: 84/100... Step: 16650... Loss: 0.9758... Val Loss: 1.2449 acc:0.639453113079071\n",
      "Epoch: 84/100... Step: 16660... Loss: 0.9850... Val Loss: 1.2467 acc:0.6360156536102295\n",
      "Epoch: 84/100... Step: 16670... Loss: 0.9694... Val Loss: 1.2498 acc:0.6371874809265137\n",
      "Epoch: 84/100... Step: 16680... Loss: 0.9861... Val Loss: 1.2490 acc:0.6378124952316284\n",
      "Epoch: 84/100... Step: 16690... Loss: 0.9749... Val Loss: 1.2502 acc:0.6329687237739563\n",
      "Epoch: 84/100... Step: 16700... Loss: 0.9744... Val Loss: 1.2643 acc:0.6396874785423279\n",
      "Epoch: 84/100... Step: 16710... Loss: 1.0023... Val Loss: 1.2369 acc:0.6353906393051147\n",
      "Epoch: 84/100... Step: 16720... Loss: 0.9703... Val Loss: 1.2527 acc:0.6339062452316284\n",
      "Epoch: 84/100... Step: 16730... Loss: 0.9717... Val Loss: 1.2416 acc:0.637499988079071\n",
      "Epoch: 84/100... Step: 16740... Loss: 0.9906... Val Loss: 1.2466 acc:0.6384375095367432\n",
      "Epoch: 84/100... Step: 16750... Loss: 0.9873... Val Loss: 1.2487 acc:0.6368749737739563\n",
      "Epoch: 84/100... Step: 16760... Loss: 0.9885... Val Loss: 1.2413 acc:0.6349999904632568\n",
      "Epoch: 84/100... Step: 16770... Loss: 0.9794... Val Loss: 1.2545 acc:0.6348437666893005\n",
      "Epoch: 84/100... Step: 16780... Loss: 0.9785... Val Loss: 1.2481 acc:0.6381250023841858\n",
      "Epoch: 84/100... Step: 16790... Loss: 1.0112... Val Loss: 1.2544 acc:0.6357031464576721\n",
      "Epoch: 84/100... Step: 16800... Loss: 1.0416... Val Loss: 1.2429 acc:0.6368749737739563\n",
      "Epoch: 85/100... Step: 16810... Loss: 0.9857... Val Loss: 1.2506 acc:0.6370312571525574\n",
      "Epoch: 85/100... Step: 16820... Loss: 1.0019... Val Loss: 1.2440 acc:0.6372656226158142\n",
      "Epoch: 85/100... Step: 16830... Loss: 0.9614... Val Loss: 1.2456 acc:0.6353124976158142\n",
      "Epoch: 85/100... Step: 16840... Loss: 0.9790... Val Loss: 1.2452 acc:0.6378124952316284\n",
      "Epoch: 85/100... Step: 16850... Loss: 0.9793... Val Loss: 1.2495 acc:0.6378124952316284\n",
      "Epoch: 85/100... Step: 16860... Loss: 0.9843... Val Loss: 1.2476 acc:0.6348437666893005\n",
      "Epoch: 85/100... Step: 16870... Loss: 0.9632... Val Loss: 1.2490 acc:0.6384375095367432\n",
      "Epoch: 85/100... Step: 16880... Loss: 0.9831... Val Loss: 1.2443 acc:0.6367968916893005\n",
      "Epoch: 85/100... Step: 16890... Loss: 0.9681... Val Loss: 1.2487 acc:0.6364062428474426\n",
      "Epoch: 85/100... Step: 16900... Loss: 0.9689... Val Loss: 1.2574 acc:0.6388280987739563\n",
      "Epoch: 85/100... Step: 16910... Loss: 0.9899... Val Loss: 1.2414 acc:0.6378124952316284\n",
      "Epoch: 85/100... Step: 16920... Loss: 0.9591... Val Loss: 1.2542 acc:0.6366406083106995\n",
      "Epoch: 85/100... Step: 16930... Loss: 0.9748... Val Loss: 1.2448 acc:0.6391406059265137\n",
      "Epoch: 85/100... Step: 16940... Loss: 0.9952... Val Loss: 1.2510 acc:0.63671875\n",
      "Epoch: 85/100... Step: 16950... Loss: 0.9957... Val Loss: 1.2411 acc:0.6382812261581421\n",
      "Epoch: 85/100... Step: 16960... Loss: 0.9966... Val Loss: 1.2458 acc:0.6377343535423279\n",
      "Epoch: 85/100... Step: 16970... Loss: 0.9815... Val Loss: 1.2514 acc:0.6375781297683716\n",
      "Epoch: 85/100... Step: 16980... Loss: 0.9749... Val Loss: 1.2463 acc:0.6385937333106995\n",
      "Epoch: 85/100... Step: 16990... Loss: 1.0096... Val Loss: 1.2527 acc:0.6343749761581421\n",
      "Epoch: 85/100... Step: 17000... Loss: 1.0363... Val Loss: 1.2425 acc:0.6366406083106995\n",
      "Epoch: 86/100... Step: 17010... Loss: 0.9928... Val Loss: 1.2536 acc:0.6370312571525574\n",
      "Epoch: 86/100... Step: 17020... Loss: 0.9875... Val Loss: 1.2486 acc:0.6360156536102295\n",
      "Epoch: 86/100... Step: 17030... Loss: 0.9605... Val Loss: 1.2481 acc:0.635546863079071\n",
      "Epoch: 86/100... Step: 17040... Loss: 0.9586... Val Loss: 1.2450 acc:0.6378124952316284\n",
      "Epoch: 86/100... Step: 17050... Loss: 0.9695... Val Loss: 1.2494 acc:0.6374218463897705\n",
      "Epoch: 86/100... Step: 17060... Loss: 0.9836... Val Loss: 1.2466 acc:0.6390625238418579\n",
      "Epoch: 86/100... Step: 17070... Loss: 0.9685... Val Loss: 1.2463 acc:0.6364062428474426\n",
      "Epoch: 86/100... Step: 17080... Loss: 0.9857... Val Loss: 1.2476 acc:0.6384375095367432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86/100... Step: 17090... Loss: 0.9681... Val Loss: 1.2529 acc:0.6364843845367432\n",
      "Epoch: 86/100... Step: 17100... Loss: 0.9744... Val Loss: 1.2668 acc:0.6356250047683716\n",
      "Epoch: 86/100... Step: 17110... Loss: 0.9965... Val Loss: 1.2435 acc:0.6381250023841858\n",
      "Epoch: 86/100... Step: 17120... Loss: 0.9625... Val Loss: 1.2612 acc:0.6339062452316284\n",
      "Epoch: 86/100... Step: 17130... Loss: 0.9661... Val Loss: 1.2499 acc:0.6382031440734863\n",
      "Epoch: 86/100... Step: 17140... Loss: 0.9863... Val Loss: 1.2499 acc:0.6370312571525574\n",
      "Epoch: 86/100... Step: 17150... Loss: 0.9940... Val Loss: 1.2441 acc:0.6371093988418579\n",
      "Epoch: 86/100... Step: 17160... Loss: 0.9901... Val Loss: 1.2477 acc:0.6362500190734863\n",
      "Epoch: 86/100... Step: 17170... Loss: 0.9789... Val Loss: 1.2538 acc:0.635937511920929\n",
      "Epoch: 86/100... Step: 17180... Loss: 0.9717... Val Loss: 1.2501 acc:0.6357031464576721\n",
      "Epoch: 86/100... Step: 17190... Loss: 0.9994... Val Loss: 1.2547 acc:0.6396093964576721\n",
      "Epoch: 86/100... Step: 17200... Loss: 1.0450... Val Loss: 1.2451 acc:0.6391406059265137\n",
      "Epoch: 87/100... Step: 17210... Loss: 0.9817... Val Loss: 1.2585 acc:0.634765625\n",
      "Epoch: 87/100... Step: 17220... Loss: 0.9865... Val Loss: 1.2517 acc:0.639843761920929\n",
      "Epoch: 87/100... Step: 17230... Loss: 0.9638... Val Loss: 1.2469 acc:0.6385156512260437\n",
      "Epoch: 87/100... Step: 17240... Loss: 0.9672... Val Loss: 1.2468 acc:0.6377343535423279\n",
      "Epoch: 87/100... Step: 17250... Loss: 0.9782... Val Loss: 1.2526 acc:0.6371874809265137\n",
      "Epoch: 87/100... Step: 17260... Loss: 0.9845... Val Loss: 1.2514 acc:0.6392968893051147\n",
      "Epoch: 87/100... Step: 17270... Loss: 0.9637... Val Loss: 1.2481 acc:0.6391406059265137\n",
      "Epoch: 87/100... Step: 17280... Loss: 0.9771... Val Loss: 1.2504 acc:0.6357812285423279\n",
      "Epoch: 87/100... Step: 17290... Loss: 0.9584... Val Loss: 1.2537 acc:0.6336718797683716\n",
      "Epoch: 87/100... Step: 17300... Loss: 0.9717... Val Loss: 1.2609 acc:0.6349218487739563\n",
      "Epoch: 87/100... Step: 17310... Loss: 0.9878... Val Loss: 1.2422 acc:0.637499988079071\n",
      "Epoch: 87/100... Step: 17320... Loss: 0.9555... Val Loss: 1.2544 acc:0.6379687786102295\n",
      "Epoch: 87/100... Step: 17330... Loss: 0.9752... Val Loss: 1.2470 acc:0.6364062428474426\n",
      "Epoch: 87/100... Step: 17340... Loss: 0.9980... Val Loss: 1.2501 acc:0.6383593678474426\n",
      "Epoch: 87/100... Step: 17350... Loss: 0.9874... Val Loss: 1.2419 acc:0.6364062428474426\n",
      "Epoch: 87/100... Step: 17360... Loss: 0.9848... Val Loss: 1.2512 acc:0.6353124976158142\n",
      "Epoch: 87/100... Step: 17370... Loss: 0.9801... Val Loss: 1.2546 acc:0.6342968940734863\n",
      "Epoch: 87/100... Step: 17380... Loss: 0.9723... Val Loss: 1.2502 acc:0.6334375143051147\n",
      "Epoch: 87/100... Step: 17390... Loss: 1.0032... Val Loss: 1.2545 acc:0.6356250047683716\n",
      "Epoch: 87/100... Step: 17400... Loss: 1.0368... Val Loss: 1.2440 acc:0.6384375095367432\n",
      "Epoch: 88/100... Step: 17410... Loss: 0.9782... Val Loss: 1.2531 acc:0.6372656226158142\n",
      "Epoch: 88/100... Step: 17420... Loss: 0.9988... Val Loss: 1.2532 acc:0.635937511920929\n",
      "Epoch: 88/100... Step: 17430... Loss: 0.9591... Val Loss: 1.2469 acc:0.6373437643051147\n",
      "Epoch: 88/100... Step: 17440... Loss: 0.9641... Val Loss: 1.2484 acc:0.6385156512260437\n",
      "Epoch: 88/100... Step: 17450... Loss: 0.9715... Val Loss: 1.2559 acc:0.6366406083106995\n",
      "Epoch: 88/100... Step: 17460... Loss: 0.9780... Val Loss: 1.2553 acc:0.635937511920929\n",
      "Epoch: 88/100... Step: 17470... Loss: 0.9580... Val Loss: 1.2504 acc:0.6373437643051147\n",
      "Epoch: 88/100... Step: 17480... Loss: 0.9814... Val Loss: 1.2505 acc:0.6381250023841858\n",
      "Epoch: 88/100... Step: 17490... Loss: 0.9658... Val Loss: 1.2523 acc:0.6349999904632568\n",
      "Epoch: 88/100... Step: 17500... Loss: 0.9679... Val Loss: 1.2606 acc:0.6353906393051147\n",
      "Epoch: 88/100... Step: 17510... Loss: 0.9892... Val Loss: 1.2428 acc:0.6337500214576721\n",
      "Epoch: 88/100... Step: 17520... Loss: 0.9602... Val Loss: 1.2542 acc:0.6382031440734863\n",
      "Epoch: 88/100... Step: 17530... Loss: 0.9649... Val Loss: 1.2506 acc:0.6387500166893005\n",
      "Epoch: 88/100... Step: 17540... Loss: 0.9934... Val Loss: 1.2498 acc:0.6349999904632568\n",
      "Epoch: 88/100... Step: 17550... Loss: 0.9828... Val Loss: 1.2481 acc:0.6317187547683716\n",
      "Epoch: 88/100... Step: 17560... Loss: 0.9875... Val Loss: 1.2506 acc:0.6349218487739563\n",
      "Epoch: 88/100... Step: 17570... Loss: 0.9754... Val Loss: 1.2524 acc:0.6346094012260437\n",
      "Epoch: 88/100... Step: 17580... Loss: 0.9808... Val Loss: 1.2481 acc:0.6363281011581421\n",
      "Epoch: 88/100... Step: 17590... Loss: 1.0008... Val Loss: 1.2566 acc:0.634765625\n",
      "Epoch: 88/100... Step: 17600... Loss: 1.0430... Val Loss: 1.2535 acc:0.6325781345367432\n",
      "Epoch: 89/100... Step: 17610... Loss: 0.9788... Val Loss: 1.2589 acc:0.6338281035423279\n",
      "Epoch: 89/100... Step: 17620... Loss: 0.9892... Val Loss: 1.2517 acc:0.6358593702316284\n",
      "Epoch: 89/100... Step: 17630... Loss: 0.9618... Val Loss: 1.2465 acc:0.6349999904632568\n",
      "Epoch: 89/100... Step: 17640... Loss: 0.9672... Val Loss: 1.2464 acc:0.6352343559265137\n",
      "Epoch: 89/100... Step: 17650... Loss: 0.9765... Val Loss: 1.2503 acc:0.6368749737739563\n",
      "Epoch: 89/100... Step: 17660... Loss: 0.9763... Val Loss: 1.2581 acc:0.6321874856948853\n",
      "Epoch: 89/100... Step: 17670... Loss: 0.9645... Val Loss: 1.2512 acc:0.6360937356948853\n",
      "Epoch: 89/100... Step: 17680... Loss: 0.9827... Val Loss: 1.2545 acc:0.633984386920929\n",
      "Epoch: 89/100... Step: 17690... Loss: 0.9599... Val Loss: 1.2557 acc:0.6344531178474426\n",
      "Epoch: 89/100... Step: 17700... Loss: 0.9632... Val Loss: 1.2636 acc:0.6371874809265137\n",
      "Epoch: 89/100... Step: 17710... Loss: 0.9842... Val Loss: 1.2438 acc:0.6389843821525574\n",
      "Epoch: 89/100... Step: 17720... Loss: 0.9583... Val Loss: 1.2570 acc:0.6356250047683716\n",
      "Epoch: 89/100... Step: 17730... Loss: 0.9577... Val Loss: 1.2535 acc:0.6372656226158142\n",
      "Epoch: 89/100... Step: 17740... Loss: 0.9931... Val Loss: 1.2546 acc:0.6363281011581421\n",
      "Epoch: 89/100... Step: 17750... Loss: 0.9857... Val Loss: 1.2466 acc:0.6363281011581421\n",
      "Epoch: 89/100... Step: 17760... Loss: 0.9941... Val Loss: 1.2523 acc:0.6346094012260437\n",
      "Epoch: 89/100... Step: 17770... Loss: 0.9805... Val Loss: 1.2601 acc:0.6365625262260437\n",
      "Epoch: 89/100... Step: 17780... Loss: 0.9730... Val Loss: 1.2567 acc:0.634765625\n",
      "Epoch: 89/100... Step: 17790... Loss: 0.9957... Val Loss: 1.2562 acc:0.6351562738418579\n",
      "Epoch: 89/100... Step: 17800... Loss: 1.0349... Val Loss: 1.2504 acc:0.6365625262260437\n",
      "Epoch: 90/100... Step: 17810... Loss: 0.9825... Val Loss: 1.2619 acc:0.6322656273841858\n",
      "Epoch: 90/100... Step: 17820... Loss: 0.9897... Val Loss: 1.2572 acc:0.633984386920929\n",
      "Epoch: 90/100... Step: 17830... Loss: 0.9590... Val Loss: 1.2471 acc:0.6362500190734863\n",
      "Epoch: 90/100... Step: 17840... Loss: 0.9611... Val Loss: 1.2522 acc:0.6374218463897705\n",
      "Epoch: 90/100... Step: 17850... Loss: 0.9725... Val Loss: 1.2518 acc:0.6372656226158142\n",
      "Epoch: 90/100... Step: 17860... Loss: 0.9693... Val Loss: 1.2559 acc:0.634765625\n",
      "Epoch: 90/100... Step: 17870... Loss: 0.9627... Val Loss: 1.2549 acc:0.6385156512260437\n",
      "Epoch: 90/100... Step: 17880... Loss: 0.9858... Val Loss: 1.2555 acc:0.63671875\n",
      "Epoch: 90/100... Step: 17890... Loss: 0.9609... Val Loss: 1.2573 acc:0.6362500190734863\n",
      "Epoch: 90/100... Step: 17900... Loss: 0.9600... Val Loss: 1.2641 acc:0.637499988079071\n",
      "Epoch: 90/100... Step: 17910... Loss: 0.9830... Val Loss: 1.2437 acc:0.639453113079071\n",
      "Epoch: 90/100... Step: 17920... Loss: 0.9604... Val Loss: 1.2612 acc:0.635546863079071\n",
      "Epoch: 90/100... Step: 17930... Loss: 0.9684... Val Loss: 1.2541 acc:0.6357031464576721\n",
      "Epoch: 90/100... Step: 17940... Loss: 0.9896... Val Loss: 1.2527 acc:0.640625\n",
      "Epoch: 90/100... Step: 17950... Loss: 0.9757... Val Loss: 1.2464 acc:0.6377343535423279\n",
      "Epoch: 90/100... Step: 17960... Loss: 0.9823... Val Loss: 1.2520 acc:0.6360156536102295\n",
      "Epoch: 90/100... Step: 17970... Loss: 0.9743... Val Loss: 1.2590 acc:0.6346874833106995\n",
      "Epoch: 90/100... Step: 17980... Loss: 0.9744... Val Loss: 1.2549 acc:0.6366406083106995\n",
      "Epoch: 90/100... Step: 17990... Loss: 0.9935... Val Loss: 1.2580 acc:0.63671875\n",
      "Epoch: 90/100... Step: 18000... Loss: 1.0291... Val Loss: 1.2454 acc:0.6390625238418579\n",
      "Epoch: 91/100... Step: 18010... Loss: 0.9700... Val Loss: 1.2570 acc:0.6356250047683716\n",
      "Epoch: 91/100... Step: 18020... Loss: 0.9832... Val Loss: 1.2533 acc:0.6375781297683716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91/100... Step: 18030... Loss: 0.9655... Val Loss: 1.2483 acc:0.6396093964576721\n",
      "Epoch: 91/100... Step: 18040... Loss: 0.9675... Val Loss: 1.2492 acc:0.6389062404632568\n",
      "Epoch: 91/100... Step: 18050... Loss: 0.9736... Val Loss: 1.2574 acc:0.6366406083106995\n",
      "Epoch: 91/100... Step: 18060... Loss: 0.9727... Val Loss: 1.2539 acc:0.6375781297683716\n",
      "Epoch: 91/100... Step: 18070... Loss: 0.9608... Val Loss: 1.2533 acc:0.6370312571525574\n",
      "Epoch: 91/100... Step: 18080... Loss: 0.9768... Val Loss: 1.2509 acc:0.6368749737739563\n",
      "Epoch: 91/100... Step: 18090... Loss: 0.9541... Val Loss: 1.2542 acc:0.6357812285423279\n",
      "Epoch: 91/100... Step: 18100... Loss: 0.9676... Val Loss: 1.2643 acc:0.6349218487739563\n",
      "Epoch: 91/100... Step: 18110... Loss: 0.9894... Val Loss: 1.2450 acc:0.6354687213897705\n",
      "Epoch: 91/100... Step: 18120... Loss: 0.9615... Val Loss: 1.2645 acc:0.6379687786102295\n",
      "Epoch: 91/100... Step: 18130... Loss: 0.9717... Val Loss: 1.2520 acc:0.6370312571525574\n",
      "Epoch: 91/100... Step: 18140... Loss: 0.9809... Val Loss: 1.2548 acc:0.6396874785423279\n",
      "Epoch: 91/100... Step: 18150... Loss: 0.9821... Val Loss: 1.2476 acc:0.6385937333106995\n",
      "Epoch: 91/100... Step: 18160... Loss: 0.9835... Val Loss: 1.2591 acc:0.6348437666893005\n",
      "Epoch: 91/100... Step: 18170... Loss: 0.9744... Val Loss: 1.2543 acc:0.6318749785423279\n",
      "Epoch: 91/100... Step: 18180... Loss: 0.9626... Val Loss: 1.2568 acc:0.6373437643051147\n",
      "Epoch: 91/100... Step: 18190... Loss: 0.9947... Val Loss: 1.2576 acc:0.6354687213897705\n",
      "Epoch: 91/100... Step: 18200... Loss: 1.0353... Val Loss: 1.2517 acc:0.6357812285423279\n",
      "Epoch: 92/100... Step: 18210... Loss: 0.9771... Val Loss: 1.2593 acc:0.6396093964576721\n",
      "Epoch: 92/100... Step: 18220... Loss: 0.9855... Val Loss: 1.2531 acc:0.6366406083106995\n",
      "Epoch: 92/100... Step: 18230... Loss: 0.9673... Val Loss: 1.2509 acc:0.6360937356948853\n",
      "Epoch: 92/100... Step: 18240... Loss: 0.9593... Val Loss: 1.2526 acc:0.637499988079071\n",
      "Epoch: 92/100... Step: 18250... Loss: 0.9677... Val Loss: 1.2579 acc:0.6360156536102295\n",
      "Epoch: 92/100... Step: 18260... Loss: 0.9711... Val Loss: 1.2548 acc:0.6385156512260437\n",
      "Epoch: 92/100... Step: 18270... Loss: 0.9552... Val Loss: 1.2540 acc:0.6366406083106995\n",
      "Epoch: 92/100... Step: 18280... Loss: 0.9706... Val Loss: 1.2584 acc:0.6365625262260437\n",
      "Epoch: 92/100... Step: 18290... Loss: 0.9613... Val Loss: 1.2585 acc:0.6349999904632568\n",
      "Epoch: 92/100... Step: 18300... Loss: 0.9657... Val Loss: 1.2663 acc:0.6357031464576721\n",
      "Epoch: 92/100... Step: 18310... Loss: 0.9849... Val Loss: 1.2461 acc:0.6380468606948853\n",
      "Epoch: 92/100... Step: 18320... Loss: 0.9506... Val Loss: 1.2626 acc:0.6366406083106995\n",
      "Epoch: 92/100... Step: 18330... Loss: 0.9625... Val Loss: 1.2585 acc:0.6345312595367432\n",
      "Epoch: 92/100... Step: 18340... Loss: 0.9829... Val Loss: 1.2555 acc:0.6360937356948853\n",
      "Epoch: 92/100... Step: 18350... Loss: 0.9819... Val Loss: 1.2485 acc:0.6357031464576721\n",
      "Epoch: 92/100... Step: 18360... Loss: 0.9816... Val Loss: 1.2563 acc:0.6346874833106995\n",
      "Epoch: 92/100... Step: 18370... Loss: 0.9659... Val Loss: 1.2573 acc:0.6377343535423279\n",
      "Epoch: 92/100... Step: 18380... Loss: 0.9739... Val Loss: 1.2567 acc:0.6345312595367432\n",
      "Epoch: 92/100... Step: 18390... Loss: 0.9897... Val Loss: 1.2579 acc:0.6362500190734863\n",
      "Epoch: 92/100... Step: 18400... Loss: 1.0252... Val Loss: 1.2515 acc:0.6364062428474426\n",
      "Epoch: 93/100... Step: 18410... Loss: 0.9732... Val Loss: 1.2588 acc:0.6349999904632568\n",
      "Epoch: 93/100... Step: 18420... Loss: 0.9794... Val Loss: 1.2562 acc:0.6370312571525574\n",
      "Epoch: 93/100... Step: 18430... Loss: 0.9460... Val Loss: 1.2481 acc:0.638671875\n",
      "Epoch: 93/100... Step: 18440... Loss: 0.9604... Val Loss: 1.2515 acc:0.6380468606948853\n",
      "Epoch: 93/100... Step: 18450... Loss: 0.9690... Val Loss: 1.2603 acc:0.6357031464576721\n",
      "Epoch: 93/100... Step: 18460... Loss: 0.9753... Val Loss: 1.2571 acc:0.635546863079071\n",
      "Epoch: 93/100... Step: 18470... Loss: 0.9579... Val Loss: 1.2561 acc:0.6380468606948853\n",
      "Epoch: 93/100... Step: 18480... Loss: 0.9722... Val Loss: 1.2572 acc:0.637499988079071\n",
      "Epoch: 93/100... Step: 18490... Loss: 0.9574... Val Loss: 1.2588 acc:0.6349999904632568\n",
      "Epoch: 93/100... Step: 18500... Loss: 0.9596... Val Loss: 1.2664 acc:0.63671875\n",
      "Epoch: 93/100... Step: 18510... Loss: 0.9771... Val Loss: 1.2483 acc:0.6383593678474426\n",
      "Epoch: 93/100... Step: 18520... Loss: 0.9609... Val Loss: 1.2650 acc:0.6349218487739563\n",
      "Epoch: 93/100... Step: 18530... Loss: 0.9625... Val Loss: 1.2569 acc:0.6377343535423279\n",
      "Epoch: 93/100... Step: 18540... Loss: 0.9784... Val Loss: 1.2565 acc:0.6392187476158142\n",
      "Epoch: 93/100... Step: 18550... Loss: 0.9755... Val Loss: 1.2508 acc:0.6393749713897705\n",
      "Epoch: 93/100... Step: 18560... Loss: 0.9796... Val Loss: 1.2618 acc:0.63671875\n",
      "Epoch: 93/100... Step: 18570... Loss: 0.9731... Val Loss: 1.2605 acc:0.637499988079071\n",
      "Epoch: 93/100... Step: 18580... Loss: 0.9813... Val Loss: 1.2586 acc:0.6382031440734863\n",
      "Epoch: 93/100... Step: 18590... Loss: 0.9882... Val Loss: 1.2572 acc:0.6360156536102295\n",
      "Epoch: 93/100... Step: 18600... Loss: 1.0292... Val Loss: 1.2520 acc:0.6371874809265137\n",
      "Epoch: 94/100... Step: 18610... Loss: 0.9678... Val Loss: 1.2601 acc:0.6384375095367432\n",
      "Epoch: 94/100... Step: 18620... Loss: 0.9827... Val Loss: 1.2583 acc:0.6371093988418579\n",
      "Epoch: 94/100... Step: 18630... Loss: 0.9484... Val Loss: 1.2512 acc:0.6358593702316284\n",
      "Epoch: 94/100... Step: 18640... Loss: 0.9549... Val Loss: 1.2519 acc:0.63671875\n",
      "Epoch: 94/100... Step: 18650... Loss: 0.9659... Val Loss: 1.2617 acc:0.6397656202316284\n",
      "Epoch: 94/100... Step: 18660... Loss: 0.9637... Val Loss: 1.2578 acc:0.6374218463897705\n",
      "Epoch: 94/100... Step: 18670... Loss: 0.9563... Val Loss: 1.2577 acc:0.6360937356948853\n",
      "Epoch: 94/100... Step: 18680... Loss: 0.9751... Val Loss: 1.2592 acc:0.6356250047683716\n",
      "Epoch: 94/100... Step: 18690... Loss: 0.9590... Val Loss: 1.2654 acc:0.6339062452316284\n",
      "Epoch: 94/100... Step: 18700... Loss: 0.9673... Val Loss: 1.2744 acc:0.635546863079071\n",
      "Epoch: 94/100... Step: 18710... Loss: 0.9888... Val Loss: 1.2479 acc:0.637890636920929\n",
      "Epoch: 94/100... Step: 18720... Loss: 0.9550... Val Loss: 1.2658 acc:0.6371093988418579\n",
      "Epoch: 94/100... Step: 18730... Loss: 0.9605... Val Loss: 1.2556 acc:0.6357031464576721\n",
      "Epoch: 94/100... Step: 18740... Loss: 0.9848... Val Loss: 1.2563 acc:0.6360156536102295\n",
      "Epoch: 94/100... Step: 18750... Loss: 0.9694... Val Loss: 1.2507 acc:0.6376562714576721\n",
      "Epoch: 94/100... Step: 18760... Loss: 0.9814... Val Loss: 1.2632 acc:0.6344531178474426\n",
      "Epoch: 94/100... Step: 18770... Loss: 0.9728... Val Loss: 1.2611 acc:0.6342968940734863\n",
      "Epoch: 94/100... Step: 18780... Loss: 0.9769... Val Loss: 1.2580 acc:0.6361718773841858\n",
      "Epoch: 94/100... Step: 18790... Loss: 0.9945... Val Loss: 1.2614 acc:0.635546863079071\n",
      "Epoch: 94/100... Step: 18800... Loss: 1.0242... Val Loss: 1.2554 acc:0.635937511920929\n",
      "Epoch: 95/100... Step: 18810... Loss: 0.9733... Val Loss: 1.2657 acc:0.6385156512260437\n",
      "Epoch: 95/100... Step: 18820... Loss: 0.9845... Val Loss: 1.2575 acc:0.6373437643051147\n",
      "Epoch: 95/100... Step: 18830... Loss: 0.9498... Val Loss: 1.2507 acc:0.6371874809265137\n",
      "Epoch: 95/100... Step: 18840... Loss: 0.9624... Val Loss: 1.2580 acc:0.6389062404632568\n",
      "Epoch: 95/100... Step: 18850... Loss: 0.9668... Val Loss: 1.2600 acc:0.6360156536102295\n",
      "Epoch: 95/100... Step: 18860... Loss: 0.9726... Val Loss: 1.2615 acc:0.6356250047683716\n",
      "Epoch: 95/100... Step: 18870... Loss: 0.9561... Val Loss: 1.2577 acc:0.6385937333106995\n",
      "Epoch: 95/100... Step: 18880... Loss: 0.9720... Val Loss: 1.2545 acc:0.6345312595367432\n",
      "Epoch: 95/100... Step: 18890... Loss: 0.9568... Val Loss: 1.2625 acc:0.6348437666893005\n",
      "Epoch: 95/100... Step: 18900... Loss: 0.9580... Val Loss: 1.2721 acc:0.6337500214576721\n",
      "Epoch: 95/100... Step: 18910... Loss: 0.9876... Val Loss: 1.2493 acc:0.6354687213897705\n",
      "Epoch: 95/100... Step: 18920... Loss: 0.9478... Val Loss: 1.2655 acc:0.6357812285423279\n",
      "Epoch: 95/100... Step: 18930... Loss: 0.9557... Val Loss: 1.2573 acc:0.635546863079071\n",
      "Epoch: 95/100... Step: 18940... Loss: 0.9857... Val Loss: 1.2603 acc:0.6351562738418579\n",
      "Epoch: 95/100... Step: 18950... Loss: 0.9716... Val Loss: 1.2569 acc:0.6372656226158142\n",
      "Epoch: 95/100... Step: 18960... Loss: 0.9733... Val Loss: 1.2662 acc:0.6353906393051147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95/100... Step: 18970... Loss: 0.9643... Val Loss: 1.2654 acc:0.633984386920929\n",
      "Epoch: 95/100... Step: 18980... Loss: 0.9701... Val Loss: 1.2584 acc:0.6357812285423279\n",
      "Epoch: 95/100... Step: 18990... Loss: 0.9832... Val Loss: 1.2566 acc:0.6390625238418579\n",
      "Epoch: 95/100... Step: 19000... Loss: 1.0244... Val Loss: 1.2533 acc:0.6387500166893005\n",
      "Epoch: 96/100... Step: 19010... Loss: 0.9702... Val Loss: 1.2652 acc:0.6371874809265137\n",
      "Epoch: 96/100... Step: 19020... Loss: 0.9863... Val Loss: 1.2568 acc:0.6353124976158142\n",
      "Epoch: 96/100... Step: 19030... Loss: 0.9490... Val Loss: 1.2527 acc:0.6360156536102295\n",
      "Epoch: 96/100... Step: 19040... Loss: 0.9599... Val Loss: 1.2580 acc:0.633984386920929\n",
      "Epoch: 96/100... Step: 19050... Loss: 0.9718... Val Loss: 1.2591 acc:0.6375781297683716\n",
      "Epoch: 96/100... Step: 19060... Loss: 0.9730... Val Loss: 1.2582 acc:0.6322656273841858\n",
      "Epoch: 96/100... Step: 19070... Loss: 0.9529... Val Loss: 1.2554 acc:0.6360156536102295\n",
      "Epoch: 96/100... Step: 19080... Loss: 0.9750... Val Loss: 1.2588 acc:0.6357812285423279\n",
      "Epoch: 96/100... Step: 19090... Loss: 0.9458... Val Loss: 1.2615 acc:0.6367968916893005\n",
      "Epoch: 96/100... Step: 19100... Loss: 0.9511... Val Loss: 1.2732 acc:0.6351562738418579\n",
      "Epoch: 96/100... Step: 19110... Loss: 0.9800... Val Loss: 1.2507 acc:0.6340625286102295\n",
      "Epoch: 96/100... Step: 19120... Loss: 0.9564... Val Loss: 1.2635 acc:0.6356250047683716\n",
      "Epoch: 96/100... Step: 19130... Loss: 0.9532... Val Loss: 1.2618 acc:0.6361718773841858\n",
      "Epoch: 96/100... Step: 19140... Loss: 0.9849... Val Loss: 1.2567 acc:0.6387500166893005\n",
      "Epoch: 96/100... Step: 19150... Loss: 0.9795... Val Loss: 1.2501 acc:0.6372656226158142\n",
      "Epoch: 96/100... Step: 19160... Loss: 0.9732... Val Loss: 1.2614 acc:0.6338281035423279\n",
      "Epoch: 96/100... Step: 19170... Loss: 0.9687... Val Loss: 1.2625 acc:0.6325781345367432\n",
      "Epoch: 96/100... Step: 19180... Loss: 0.9699... Val Loss: 1.2583 acc:0.6367968916893005\n",
      "Epoch: 96/100... Step: 19190... Loss: 0.9846... Val Loss: 1.2556 acc:0.6364843845367432\n",
      "Epoch: 96/100... Step: 19200... Loss: 1.0283... Val Loss: 1.2532 acc:0.6364062428474426\n",
      "Epoch: 97/100... Step: 19210... Loss: 0.9625... Val Loss: 1.2614 acc:0.6382031440734863\n",
      "Epoch: 97/100... Step: 19220... Loss: 0.9793... Val Loss: 1.2564 acc:0.639453113079071\n",
      "Epoch: 97/100... Step: 19230... Loss: 0.9526... Val Loss: 1.2532 acc:0.6365625262260437\n",
      "Epoch: 97/100... Step: 19240... Loss: 0.9625... Val Loss: 1.2606 acc:0.6365625262260437\n",
      "Epoch: 97/100... Step: 19250... Loss: 0.9666... Val Loss: 1.2639 acc:0.6362500190734863\n",
      "Epoch: 97/100... Step: 19260... Loss: 0.9773... Val Loss: 1.2591 acc:0.63671875\n",
      "Epoch: 97/100... Step: 19270... Loss: 0.9440... Val Loss: 1.2596 acc:0.6370312571525574\n",
      "Epoch: 97/100... Step: 19280... Loss: 0.9739... Val Loss: 1.2607 acc:0.6310937404632568\n",
      "Epoch: 97/100... Step: 19290... Loss: 0.9473... Val Loss: 1.2630 acc:0.6336718797683716\n",
      "Epoch: 97/100... Step: 19300... Loss: 0.9513... Val Loss: 1.2742 acc:0.6361718773841858\n",
      "Epoch: 97/100... Step: 19310... Loss: 0.9891... Val Loss: 1.2498 acc:0.6376562714576721\n",
      "Epoch: 97/100... Step: 19320... Loss: 0.9457... Val Loss: 1.2671 acc:0.6348437666893005\n",
      "Epoch: 97/100... Step: 19330... Loss: 0.9466... Val Loss: 1.2646 acc:0.6331250071525574\n",
      "Epoch: 97/100... Step: 19340... Loss: 0.9795... Val Loss: 1.2562 acc:0.6344531178474426\n",
      "Epoch: 97/100... Step: 19350... Loss: 0.9802... Val Loss: 1.2519 acc:0.6336718797683716\n",
      "Epoch: 97/100... Step: 19360... Loss: 0.9851... Val Loss: 1.2662 acc:0.6335155963897705\n",
      "Epoch: 97/100... Step: 19370... Loss: 0.9667... Val Loss: 1.2651 acc:0.6324218511581421\n",
      "Epoch: 97/100... Step: 19380... Loss: 0.9731... Val Loss: 1.2610 acc:0.633984386920929\n",
      "Epoch: 97/100... Step: 19390... Loss: 0.9846... Val Loss: 1.2533 acc:0.6366406083106995\n",
      "Epoch: 97/100... Step: 19400... Loss: 1.0226... Val Loss: 1.2523 acc:0.6362500190734863\n",
      "Epoch: 98/100... Step: 19410... Loss: 0.9696... Val Loss: 1.2622 acc:0.6361718773841858\n",
      "Epoch: 98/100... Step: 19420... Loss: 0.9918... Val Loss: 1.2566 acc:0.6349999904632568\n",
      "Epoch: 98/100... Step: 19430... Loss: 0.9391... Val Loss: 1.2566 acc:0.6362500190734863\n",
      "Epoch: 98/100... Step: 19440... Loss: 0.9482... Val Loss: 1.2594 acc:0.6369531154632568\n",
      "Epoch: 98/100... Step: 19450... Loss: 0.9601... Val Loss: 1.2671 acc:0.6344531178474426\n",
      "Epoch: 98/100... Step: 19460... Loss: 0.9622... Val Loss: 1.2595 acc:0.6348437666893005\n",
      "Epoch: 98/100... Step: 19470... Loss: 0.9464... Val Loss: 1.2581 acc:0.635937511920929\n",
      "Epoch: 98/100... Step: 19480... Loss: 0.9763... Val Loss: 1.2590 acc:0.6342968940734863\n",
      "Epoch: 98/100... Step: 19490... Loss: 0.9482... Val Loss: 1.2659 acc:0.6360156536102295\n",
      "Epoch: 98/100... Step: 19500... Loss: 0.9560... Val Loss: 1.2733 acc:0.63671875\n",
      "Epoch: 98/100... Step: 19510... Loss: 0.9810... Val Loss: 1.2505 acc:0.6370312571525574\n",
      "Epoch: 98/100... Step: 19520... Loss: 0.9463... Val Loss: 1.2652 acc:0.6365625262260437\n",
      "Epoch: 98/100... Step: 19530... Loss: 0.9546... Val Loss: 1.2638 acc:0.635937511920929\n",
      "Epoch: 98/100... Step: 19540... Loss: 0.9764... Val Loss: 1.2530 acc:0.63671875\n",
      "Epoch: 98/100... Step: 19550... Loss: 0.9720... Val Loss: 1.2560 acc:0.6328125\n",
      "Epoch: 98/100... Step: 19560... Loss: 0.9802... Val Loss: 1.2729 acc:0.6325781345367432\n",
      "Epoch: 98/100... Step: 19570... Loss: 0.9655... Val Loss: 1.2635 acc:0.6349218487739563\n",
      "Epoch: 98/100... Step: 19580... Loss: 0.9713... Val Loss: 1.2629 acc:0.6372656226158142\n",
      "Epoch: 98/100... Step: 19590... Loss: 0.9903... Val Loss: 1.2587 acc:0.6332812309265137\n",
      "Epoch: 98/100... Step: 19600... Loss: 1.0271... Val Loss: 1.2581 acc:0.6360156536102295\n",
      "Epoch: 99/100... Step: 19610... Loss: 0.9739... Val Loss: 1.2654 acc:0.6361718773841858\n",
      "Epoch: 99/100... Step: 19620... Loss: 0.9753... Val Loss: 1.2594 acc:0.6366406083106995\n",
      "Epoch: 99/100... Step: 19630... Loss: 0.9495... Val Loss: 1.2572 acc:0.6361718773841858\n",
      "Epoch: 99/100... Step: 19640... Loss: 0.9542... Val Loss: 1.2615 acc:0.6372656226158142\n",
      "Epoch: 99/100... Step: 19650... Loss: 0.9675... Val Loss: 1.2593 acc:0.6390625238418579\n",
      "Epoch: 99/100... Step: 19660... Loss: 0.9620... Val Loss: 1.2601 acc:0.6385937333106995\n",
      "Epoch: 99/100... Step: 19670... Loss: 0.9507... Val Loss: 1.2551 acc:0.6382031440734863\n",
      "Epoch: 99/100... Step: 19680... Loss: 0.9646... Val Loss: 1.2599 acc:0.6337500214576721\n",
      "Epoch: 99/100... Step: 19690... Loss: 0.9504... Val Loss: 1.2646 acc:0.6345312595367432\n",
      "Epoch: 99/100... Step: 19700... Loss: 0.9649... Val Loss: 1.2707 acc:0.63671875\n",
      "Epoch: 99/100... Step: 19710... Loss: 0.9802... Val Loss: 1.2536 acc:0.6362500190734863\n",
      "Epoch: 99/100... Step: 19720... Loss: 0.9450... Val Loss: 1.2651 acc:0.6375781297683716\n",
      "Epoch: 99/100... Step: 19730... Loss: 0.9544... Val Loss: 1.2663 acc:0.6363281011581421\n",
      "Epoch: 99/100... Step: 19740... Loss: 0.9734... Val Loss: 1.2579 acc:0.6371874809265137\n",
      "Epoch: 99/100... Step: 19750... Loss: 0.9775... Val Loss: 1.2592 acc:0.6365625262260437\n",
      "Epoch: 99/100... Step: 19760... Loss: 0.9803... Val Loss: 1.2702 acc:0.6362500190734863\n",
      "Epoch: 99/100... Step: 19770... Loss: 0.9613... Val Loss: 1.2594 acc:0.6349218487739563\n",
      "Epoch: 99/100... Step: 19780... Loss: 0.9635... Val Loss: 1.2628 acc:0.6366406083106995\n",
      "Epoch: 99/100... Step: 19790... Loss: 0.9865... Val Loss: 1.2618 acc:0.6331250071525574\n",
      "Epoch: 99/100... Step: 19800... Loss: 1.0189... Val Loss: 1.2557 acc:0.6343749761581421\n",
      "Epoch: 100/100... Step: 19810... Loss: 0.9592... Val Loss: 1.2654 acc:0.6346094012260437\n",
      "Epoch: 100/100... Step: 19820... Loss: 0.9820... Val Loss: 1.2577 acc:0.634765625\n",
      "Epoch: 100/100... Step: 19830... Loss: 0.9494... Val Loss: 1.2656 acc:0.6315624713897705\n",
      "Epoch: 100/100... Step: 19840... Loss: 0.9476... Val Loss: 1.2657 acc:0.6373437643051147\n",
      "Epoch: 100/100... Step: 19850... Loss: 0.9643... Val Loss: 1.2615 acc:0.6352343559265137\n",
      "Epoch: 100/100... Step: 19860... Loss: 0.9665... Val Loss: 1.2583 acc:0.6364062428474426\n",
      "Epoch: 100/100... Step: 19870... Loss: 0.9506... Val Loss: 1.2595 acc:0.6368749737739563\n",
      "Epoch: 100/100... Step: 19880... Loss: 0.9609... Val Loss: 1.2653 acc:0.6350781321525574\n",
      "Epoch: 100/100... Step: 19890... Loss: 0.9455... Val Loss: 1.2683 acc:0.6364843845367432\n",
      "Epoch: 100/100... Step: 19900... Loss: 0.9487... Val Loss: 1.2740 acc:0.6343749761581421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/100... Step: 19910... Loss: 0.9753... Val Loss: 1.2506 acc:0.6364062428474426\n",
      "Epoch: 100/100... Step: 19920... Loss: 0.9431... Val Loss: 1.2673 acc:0.637890636920929\n",
      "Epoch: 100/100... Step: 19930... Loss: 0.9500... Val Loss: 1.2677 acc:0.6368749737739563\n",
      "Epoch: 100/100... Step: 19940... Loss: 0.9750... Val Loss: 1.2570 acc:0.6357812285423279\n",
      "Epoch: 100/100... Step: 19950... Loss: 0.9730... Val Loss: 1.2585 acc:0.6350781321525574\n",
      "Epoch: 100/100... Step: 19960... Loss: 0.9699... Val Loss: 1.2684 acc:0.6334375143051147\n",
      "Epoch: 100/100... Step: 19970... Loss: 0.9628... Val Loss: 1.2629 acc:0.6343749761581421\n",
      "Epoch: 100/100... Step: 19980... Loss: 0.9632... Val Loss: 1.2630 acc:0.6352343559265137\n",
      "Epoch: 100/100... Step: 19990... Loss: 0.9842... Val Loss: 1.2679 acc:0.6343749761581421\n",
      "Epoch: 100/100... Step: 20000... Loss: 1.0271... Val Loss: 1.2583 acc:0.6363281011581421\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 100 # start small if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "08e46033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T07:30:15.396446Z",
     "start_time": "2021-06-02T07:30:15.383779Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e0b33aba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T07:30:17.104982Z",
     "start_time": "2021-06-02T07:30:17.096352Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "abfb449c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T07:31:16.475305Z",
     "start_time": "2021-06-02T07:31:16.463187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple \n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1, prime='appl', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7b63f208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T07:32:49.564698Z",
     "start_time": "2021-06-02T07:32:49.523998Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(net, 'output/model/twolayLSTM.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6eaa1e57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T07:34:28.534269Z",
     "start_time": "2021-06-02T07:34:28.517007Z"
    }
   },
   "outputs": [],
   "source": [
    "model=torch.load('output/model/twolayLSTM.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3e81411e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T07:34:37.929445Z",
     "start_time": "2021-06-02T07:34:37.915681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apples\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, 1, prime='appl', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf2a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dataglove] *",
   "language": "python",
   "name": "conda-env-dataglove-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
