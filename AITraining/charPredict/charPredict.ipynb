{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "007f81e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:03:05.486083Z",
     "start_time": "2021-10-15T13:02:58.093174Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6baf5af7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:03:08.286547Z",
     "start_time": "2021-10-15T13:03:08.272667Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/home/iot/jupyter/root_dir/liudongdong/dataset/charprediction/val.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41ede937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:03:10.836144Z",
     "start_time": "2021-10-15T13:03:10.830218Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808bf6b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:03:12.996229Z",
     "start_time": "2021-10-15T13:03:12.985781Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    ## TODO: Get the number of batches we can make\n",
    "    n_batches = (len(arr))//(batch_size*seq_length)\n",
    "    \n",
    "    ## TODO: Keep only enough characters to make full batches\n",
    "    arr = arr[:(n_batches*batch_size*seq_length)]\n",
    "    \n",
    "    ## TODO: Reshape into batch_size rows\n",
    "    size=(batch_size,-1)\n",
    "    arr = arr.reshape(size)  #(batch, columns)  后续数据直接在 columns 遍历\n",
    "    \n",
    "    ## TODO: Iterate over the batches using a window of size seq_length\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "          y[:, :-1],y[:, -1]=x[:,1:], arr[:,n+seq_length]\n",
    "        except IndexError:\n",
    "          y[:, :-1],y[:, -1]=x[:,1:], arr[:,0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a84b56f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:03:15.171518Z",
     "start_time": "2021-10-15T13:03:15.132586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acdf79b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:03:16.994504Z",
     "start_time": "2021-10-15T13:03:16.978902Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the layers of the model\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)   #注意这里\n",
    "        \n",
    "        self.dropout=nn.Dropout(drop_prob)\n",
    "\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        #x=rearrange(x,'b s d-> s b d')\n",
    "        r_output,hidden=self.lstm(x,hidden)\n",
    "        #r_output=rearrange(r_output,'s b d-> b s d')\n",
    "        out=self.dropout(r_output)\n",
    "        #print(\"self.dropout\",out.shape)\n",
    "        out=out.contiguous().view(-1,self.n_hidden)\n",
    "        #print(\"self.contiguous\",out.shape)\n",
    "        out=self.fc(out)\n",
    "# self.dropout torch.Size([128, 100, 512])\n",
    "# self.contiguous torch.Size([12800, 512])\n",
    "# output, torch.Size([12800, 94])\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c8bafc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:03:20.949723Z",
     "start_time": "2021-10-15T13:03:20.907021Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(72, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=72, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## TODO: set you model hyperparameters\n",
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00a3f981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:03:25.914066Z",
     "start_time": "2021-10-15T13:03:25.892459Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1e914",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-09-14T07:30:01.881Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "seq_length = 20\n",
    "n_epochs = 100 # start small if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08e46033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:04:30.411379Z",
     "start_time": "2021-10-15T13:04:30.399005Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0b33aba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:04:33.478125Z",
     "start_time": "2021-10-15T13:04:33.469821Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abfb449c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:05:28.669732Z",
     "start_time": "2021-10-15T13:05:28.657634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applit\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1, prime='appl', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7b63f208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T07:32:49.564698Z",
     "start_time": "2021-06-02T07:32:49.523998Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(net, 'output/model/twolayLSTM.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eaa1e57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T10:03:04.771402Z",
     "start_time": "2021-06-04T10:03:02.630206Z"
    }
   },
   "outputs": [],
   "source": [
    "model=torch.load('output/model/twolayLSTM.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3e81411e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T07:34:37.929445Z",
     "start_time": "2021-06-02T07:34:37.915681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apples\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, 1, prime='appl', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8faf2a60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T10:04:08.998500Z",
     "start_time": "2021-06-04T10:04:08.952738Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'twolayLSTM_params.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70445a89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:11:06.782358Z",
     "start_time": "2021-10-15T13:11:06.759514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right \n",
      "right w\n",
      "right wo\n",
      "right won\n",
      "right wond\n",
      "right wonde\n",
      "right wonder\n",
      "right wonderf\n",
      "right wonderfu\n",
      "right wonderful\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'erful'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-15208a19aa9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minital_content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mpossible_chars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mpossible_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'erful'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "  # \"https://github.com/pradeepradyumna/SampleData/blob/master/sampledata.txt\")\n",
    "\n",
    "data = \"apple banana orange black yellow good bad right wrong blue green white think thought wonderful\"\n",
    "\n",
    "\n",
    "# Markov Chains Algorithm\n",
    "\n",
    "def generatetable(data, k):\n",
    "    T = {}\n",
    "    for i in range(len(data)-k):\n",
    "        x = data[i:i+k]\n",
    "        y = data[i+k]\n",
    "\n",
    "        if T.get(x) is None:\n",
    "            T[x] = {}\n",
    "            T[x][y] = 1\n",
    "        else:\n",
    "            if T[x].get(y) is None:\n",
    "                T[x][y] = 1\n",
    "            else:\n",
    "                T[x][y] += 1\n",
    "    return T\n",
    "\n",
    "\n",
    "k = 5\n",
    "inital_content = \"right\"\n",
    "\n",
    "T = generatetable(data.lower(), k)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    inp = inital_content[-k:]\n",
    "\n",
    "    possible_chars = list(T[inp].keys())\n",
    "    possible_values = list(T[inp].values())\n",
    "\n",
    "    sum_ = sum(T[inp].values())\n",
    "\n",
    "    probabs = np.array(possible_values)/sum_\n",
    "\n",
    "    next_char = np.random.choice(possible_chars, p=probabs)\n",
    "\n",
    "    inital_content += next_char\n",
    "\n",
    "    print(inital_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df50e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
