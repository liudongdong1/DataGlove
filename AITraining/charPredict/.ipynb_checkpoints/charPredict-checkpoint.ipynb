{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "007f81e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:03:05.486083Z",
     "start_time": "2021-10-15T13:02:58.093174Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6baf5af7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:03:08.286547Z",
     "start_time": "2021-10-15T13:03:08.272667Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/home/iot/jupyter/root_dir/liudongdong/dataset/charprediction/val.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41ede937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:03:10.836144Z",
     "start_time": "2021-10-15T13:03:10.830218Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808bf6b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:03:12.996229Z",
     "start_time": "2021-10-15T13:03:12.985781Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    ## TODO: Get the number of batches we can make\n",
    "    n_batches = (len(arr))//(batch_size*seq_length)\n",
    "    \n",
    "    ## TODO: Keep only enough characters to make full batches\n",
    "    arr = arr[:(n_batches*batch_size*seq_length)]\n",
    "    \n",
    "    ## TODO: Reshape into batch_size rows\n",
    "    size=(batch_size,-1)\n",
    "    arr = arr.reshape(size)  #(batch, columns)  后续数据直接在 columns 遍历\n",
    "    \n",
    "    ## TODO: Iterate over the batches using a window of size seq_length\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "          y[:, :-1],y[:, -1]=x[:,1:], arr[:,n+seq_length]\n",
    "        except IndexError:\n",
    "          y[:, :-1],y[:, -1]=x[:,1:], arr[:,0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a84b56f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:03:15.171518Z",
     "start_time": "2021-10-15T13:03:15.132586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acdf79b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:03:16.994504Z",
     "start_time": "2021-10-15T13:03:16.978902Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the layers of the model\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)   #注意这里\n",
    "        \n",
    "        self.dropout=nn.Dropout(drop_prob)\n",
    "\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        #x=rearrange(x,'b s d-> s b d')\n",
    "        r_output,hidden=self.lstm(x,hidden)\n",
    "        #r_output=rearrange(r_output,'s b d-> b s d')\n",
    "        out=self.dropout(r_output)\n",
    "        #print(\"self.dropout\",out.shape)\n",
    "        out=out.contiguous().view(-1,self.n_hidden)\n",
    "        #print(\"self.contiguous\",out.shape)\n",
    "        out=self.fc(out)\n",
    "# self.dropout torch.Size([128, 100, 512])\n",
    "# self.contiguous torch.Size([12800, 512])\n",
    "# output, torch.Size([12800, 94])\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c8bafc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:03:20.949723Z",
     "start_time": "2021-10-15T13:03:20.907021Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(72, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=72, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## TODO: set you model hyperparameters\n",
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00a3f981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:03:25.914066Z",
     "start_time": "2021-10-15T13:03:25.892459Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1da1e914",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:04:16.762146Z",
     "start_time": "2021-10-15T13:03:49.536994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100... Step: 10... Loss: 3.2202... Val Loss: 3.3182 acc:0.15625\n",
      "Epoch: 2/100... Step: 20... Loss: 3.1342... Val Loss: 3.2452 acc:0.16171875596046448\n",
      "Epoch: 2/100... Step: 30... Loss: 3.0884... Val Loss: 3.2127 acc:0.16249999403953552\n",
      "Epoch: 3/100... Step: 40... Loss: 3.0797... Val Loss: 3.1934 acc:0.16249999403953552\n",
      "Epoch: 4/100... Step: 50... Loss: 3.0875... Val Loss: 3.1895 acc:0.16249999403953552\n",
      "Epoch: 4/100... Step: 60... Loss: 3.0850... Val Loss: 3.1792 acc:0.16249999403953552\n",
      "Epoch: 5/100... Step: 70... Loss: 3.0348... Val Loss: 3.1751 acc:0.16249999403953552\n",
      "Epoch: 5/100... Step: 80... Loss: 3.0795... Val Loss: 3.1662 acc:0.16249999403953552\n",
      "Epoch: 6/100... Step: 90... Loss: 3.0793... Val Loss: 3.1515 acc:0.16249999403953552\n",
      "Epoch: 7/100... Step: 100... Loss: 3.0389... Val Loss: 3.1276 acc:0.16249999403953552\n",
      "Epoch: 7/100... Step: 110... Loss: 2.9710... Val Loss: 3.0895 acc:0.17734375596046448\n",
      "Epoch: 8/100... Step: 120... Loss: 2.8927... Val Loss: 3.0233 acc:0.18671874701976776\n",
      "Epoch: 9/100... Step: 130... Loss: 2.8227... Val Loss: 2.9773 acc:0.19921875\n",
      "Epoch: 9/100... Step: 140... Loss: 2.7719... Val Loss: 2.9179 acc:0.21015624701976776\n",
      "Epoch: 10/100... Step: 150... Loss: 2.6389... Val Loss: 2.8298 acc:0.22890624403953552\n",
      "Epoch: 10/100... Step: 160... Loss: 2.6771... Val Loss: 2.7712 acc:0.23828125\n",
      "Epoch: 11/100... Step: 170... Loss: 2.5675... Val Loss: 2.7217 acc:0.24609375\n",
      "Epoch: 12/100... Step: 180... Loss: 2.5187... Val Loss: 2.6863 acc:0.2523437440395355\n",
      "Epoch: 12/100... Step: 190... Loss: 2.4767... Val Loss: 2.6677 acc:0.24140624701976776\n",
      "Epoch: 13/100... Step: 200... Loss: 2.4273... Val Loss: 2.6484 acc:0.2578125\n",
      "Epoch: 14/100... Step: 210... Loss: 2.3677... Val Loss: 2.6300 acc:0.26484376192092896\n",
      "Epoch: 14/100... Step: 220... Loss: 2.3884... Val Loss: 2.6007 acc:0.26484376192092896\n",
      "Epoch: 15/100... Step: 230... Loss: 2.3047... Val Loss: 2.5861 acc:0.28437501192092896\n",
      "Epoch: 15/100... Step: 240... Loss: 2.4821... Val Loss: 2.5727 acc:0.28046876192092896\n",
      "Epoch: 16/100... Step: 250... Loss: 2.2541... Val Loss: 2.5461 acc:0.28593748807907104\n",
      "Epoch: 17/100... Step: 260... Loss: 2.2815... Val Loss: 2.5291 acc:0.28515625\n",
      "Epoch: 17/100... Step: 270... Loss: 2.2554... Val Loss: 2.5077 acc:0.28984373807907104\n",
      "Epoch: 18/100... Step: 280... Loss: 2.1789... Val Loss: 2.4867 acc:0.296875\n",
      "Epoch: 19/100... Step: 290... Loss: 2.1811... Val Loss: 2.4798 acc:0.29374998807907104\n",
      "Epoch: 19/100... Step: 300... Loss: 2.1943... Val Loss: 2.4611 acc:0.2953124940395355\n",
      "Epoch: 20/100... Step: 310... Loss: 2.1483... Val Loss: 2.4453 acc:0.30156248807907104\n",
      "Epoch: 20/100... Step: 320... Loss: 2.3433... Val Loss: 2.4374 acc:0.31328123807907104\n",
      "Epoch: 21/100... Step: 330... Loss: 2.0933... Val Loss: 2.4127 acc:0.31328123807907104\n",
      "Epoch: 22/100... Step: 340... Loss: 2.0929... Val Loss: 2.4152 acc:0.31171876192092896\n",
      "Epoch: 22/100... Step: 350... Loss: 2.0502... Val Loss: 2.3922 acc:0.33203125\n",
      "Epoch: 23/100... Step: 360... Loss: 2.0113... Val Loss: 2.3648 acc:0.33125001192092896\n",
      "Epoch: 24/100... Step: 370... Loss: 1.9909... Val Loss: 2.3591 acc:0.34296876192092896\n",
      "Epoch: 24/100... Step: 380... Loss: 2.0067... Val Loss: 2.3550 acc:0.34843748807907104\n",
      "Epoch: 25/100... Step: 390... Loss: 1.9469... Val Loss: 2.3613 acc:0.3453125059604645\n",
      "Epoch: 25/100... Step: 400... Loss: 2.1767... Val Loss: 2.3535 acc:0.35234373807907104\n",
      "Epoch: 26/100... Step: 410... Loss: 1.9158... Val Loss: 2.3302 acc:0.3570312559604645\n",
      "Epoch: 27/100... Step: 420... Loss: 1.9446... Val Loss: 2.3260 acc:0.36250001192092896\n",
      "Epoch: 27/100... Step: 430... Loss: 1.8627... Val Loss: 2.3121 acc:0.36015623807907104\n",
      "Epoch: 28/100... Step: 440... Loss: 1.8254... Val Loss: 2.2923 acc:0.36640626192092896\n",
      "Epoch: 29/100... Step: 450... Loss: 1.8201... Val Loss: 2.3090 acc:0.37187498807907104\n",
      "Epoch: 29/100... Step: 460... Loss: 1.8582... Val Loss: 2.2875 acc:0.3695312440395355\n",
      "Epoch: 30/100... Step: 470... Loss: 1.8158... Val Loss: 2.2851 acc:0.37421876192092896\n",
      "Epoch: 30/100... Step: 480... Loss: 1.9995... Val Loss: 2.2793 acc:0.375\n",
      "Epoch: 31/100... Step: 490... Loss: 1.7549... Val Loss: 2.2826 acc:0.375\n",
      "Epoch: 32/100... Step: 500... Loss: 1.7706... Val Loss: 2.2625 acc:0.3812499940395355\n",
      "Epoch: 32/100... Step: 510... Loss: 1.7243... Val Loss: 2.2667 acc:0.37890625\n",
      "Epoch: 33/100... Step: 520... Loss: 1.6610... Val Loss: 2.2409 acc:0.39140623807907104\n",
      "Epoch: 34/100... Step: 530... Loss: 1.6929... Val Loss: 2.2453 acc:0.3882812559604645\n",
      "Epoch: 34/100... Step: 540... Loss: 1.6967... Val Loss: 2.2522 acc:0.38749998807907104\n",
      "Epoch: 35/100... Step: 550... Loss: 1.6313... Val Loss: 2.2400 acc:0.39453125\n",
      "Epoch: 35/100... Step: 560... Loss: 1.8624... Val Loss: 2.2555 acc:0.37578123807907104\n",
      "Epoch: 36/100... Step: 570... Loss: 1.5741... Val Loss: 2.2407 acc:0.3968749940395355\n",
      "Epoch: 37/100... Step: 580... Loss: 1.6345... Val Loss: 2.2572 acc:0.38749998807907104\n",
      "Epoch: 37/100... Step: 590... Loss: 1.5676... Val Loss: 2.2552 acc:0.39453125\n",
      "Epoch: 38/100... Step: 600... Loss: 1.5112... Val Loss: 2.2363 acc:0.39765626192092896\n",
      "Epoch: 39/100... Step: 610... Loss: 1.5128... Val Loss: 2.2294 acc:0.3960937559604645\n",
      "Epoch: 39/100... Step: 620... Loss: 1.5096... Val Loss: 2.2334 acc:0.40312498807907104\n",
      "Epoch: 40/100... Step: 630... Loss: 1.4621... Val Loss: 2.2505 acc:0.40156251192092896\n",
      "Epoch: 40/100... Step: 640... Loss: 1.6553... Val Loss: 2.2386 acc:0.4007812440395355\n",
      "Epoch: 41/100... Step: 650... Loss: 1.4088... Val Loss: 2.2548 acc:0.40546876192092896\n",
      "Epoch: 42/100... Step: 660... Loss: 1.4649... Val Loss: 2.2430 acc:0.4124999940395355\n",
      "Epoch: 42/100... Step: 670... Loss: 1.3690... Val Loss: 2.2549 acc:0.4000000059604645\n",
      "Epoch: 43/100... Step: 680... Loss: 1.3430... Val Loss: 2.2419 acc:0.4085937440395355\n",
      "Epoch: 44/100... Step: 690... Loss: 1.3113... Val Loss: 2.2673 acc:0.4000000059604645\n",
      "Epoch: 44/100... Step: 700... Loss: 1.3372... Val Loss: 2.2379 acc:0.40312498807907104\n",
      "Epoch: 45/100... Step: 710... Loss: 1.2964... Val Loss: 2.2893 acc:0.4046874940395355\n",
      "Epoch: 45/100... Step: 720... Loss: 1.4829... Val Loss: 2.2436 acc:0.4164062440395355\n",
      "Epoch: 46/100... Step: 730... Loss: 1.2831... Val Loss: 2.2684 acc:0.40625\n",
      "Epoch: 47/100... Step: 740... Loss: 1.2688... Val Loss: 2.2587 acc:0.41328126192092896\n",
      "Epoch: 47/100... Step: 750... Loss: 1.2560... Val Loss: 2.2796 acc:0.41484373807907104\n",
      "Epoch: 48/100... Step: 760... Loss: 1.1893... Val Loss: 2.2661 acc:0.41718751192092896\n",
      "Epoch: 49/100... Step: 770... Loss: 1.1452... Val Loss: 2.2999 acc:0.41015625\n",
      "Epoch: 49/100... Step: 780... Loss: 1.1608... Val Loss: 2.3092 acc:0.4078125059604645\n",
      "Epoch: 50/100... Step: 790... Loss: 1.1161... Val Loss: 2.2813 acc:0.41796875\n",
      "Epoch: 50/100... Step: 800... Loss: 1.2757... Val Loss: 2.3032 acc:0.4156250059604645\n",
      "Epoch: 51/100... Step: 810... Loss: 1.0959... Val Loss: 2.2958 acc:0.40937501192092896\n",
      "Epoch: 52/100... Step: 820... Loss: 1.0843... Val Loss: 2.3077 acc:0.421875\n",
      "Epoch: 52/100... Step: 830... Loss: 1.0483... Val Loss: 2.3559 acc:0.4046874940395355\n",
      "Epoch: 53/100... Step: 840... Loss: 1.0237... Val Loss: 2.3429 acc:0.4156250059604645\n",
      "Epoch: 54/100... Step: 850... Loss: 0.9976... Val Loss: 2.3880 acc:0.40546876192092896\n",
      "Epoch: 54/100... Step: 860... Loss: 0.9994... Val Loss: 2.3665 acc:0.40156251192092896\n",
      "Epoch: 55/100... Step: 870... Loss: 0.9940... Val Loss: 2.3844 acc:0.4007812440395355\n",
      "Epoch: 55/100... Step: 880... Loss: 1.0797... Val Loss: 2.3977 acc:0.40312498807907104\n",
      "Epoch: 56/100... Step: 890... Loss: 0.9778... Val Loss: 2.3716 acc:0.4140625\n",
      "Epoch: 57/100... Step: 900... Loss: 0.9870... Val Loss: 2.4281 acc:0.41093748807907104\n",
      "Epoch: 57/100... Step: 910... Loss: 0.9238... Val Loss: 2.4298 acc:0.39531248807907104\n",
      "Epoch: 58/100... Step: 920... Loss: 0.8900... Val Loss: 2.4757 acc:0.40546876192092896\n",
      "Epoch: 59/100... Step: 930... Loss: 0.9012... Val Loss: 2.4516 acc:0.39765626192092896\n",
      "Epoch: 59/100... Step: 940... Loss: 0.8984... Val Loss: 2.4628 acc:0.4000000059604645\n",
      "Epoch: 60/100... Step: 950... Loss: 0.9110... Val Loss: 2.4330 acc:0.40312498807907104\n",
      "Epoch: 60/100... Step: 960... Loss: 0.9764... Val Loss: 2.4798 acc:0.39375001192092896\n",
      "Epoch: 61/100... Step: 970... Loss: 0.8714... Val Loss: 2.4713 acc:0.40625\n",
      "Epoch: 62/100... Step: 980... Loss: 0.8364... Val Loss: 2.5006 acc:0.40234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62/100... Step: 990... Loss: 0.8192... Val Loss: 2.5000 acc:0.40625\n",
      "Epoch: 63/100... Step: 1000... Loss: 0.7590... Val Loss: 2.5489 acc:0.4007812440395355\n",
      "Epoch: 64/100... Step: 1010... Loss: 0.7656... Val Loss: 2.5389 acc:0.39375001192092896\n",
      "Epoch: 64/100... Step: 1020... Loss: 0.7153... Val Loss: 2.5538 acc:0.3984375\n",
      "Epoch: 65/100... Step: 1030... Loss: 0.6857... Val Loss: 2.5357 acc:0.4000000059604645\n",
      "Epoch: 65/100... Step: 1040... Loss: 0.7433... Val Loss: 2.5755 acc:0.39765626192092896\n",
      "Epoch: 66/100... Step: 1050... Loss: 0.6993... Val Loss: 2.6134 acc:0.4000000059604645\n",
      "Epoch: 67/100... Step: 1060... Loss: 0.7064... Val Loss: 2.6111 acc:0.39531248807907104\n",
      "Epoch: 67/100... Step: 1070... Loss: 0.6605... Val Loss: 2.6666 acc:0.40312498807907104\n",
      "Epoch: 68/100... Step: 1080... Loss: 0.6347... Val Loss: 2.6507 acc:0.390625\n",
      "Epoch: 69/100... Step: 1090... Loss: 0.6103... Val Loss: 2.6503 acc:0.3984375\n",
      "Epoch: 69/100... Step: 1100... Loss: 0.6137... Val Loss: 2.6794 acc:0.39140623807907104\n",
      "Epoch: 70/100... Step: 1110... Loss: 0.6229... Val Loss: 2.6629 acc:0.3882812559604645\n",
      "Epoch: 70/100... Step: 1120... Loss: 0.6253... Val Loss: 2.6789 acc:0.39140623807907104\n",
      "Epoch: 71/100... Step: 1130... Loss: 0.6109... Val Loss: 2.7205 acc:0.38749998807907104\n",
      "Epoch: 72/100... Step: 1140... Loss: 0.5635... Val Loss: 2.7512 acc:0.38593751192092896\n",
      "Epoch: 72/100... Step: 1150... Loss: 0.5609... Val Loss: 2.7403 acc:0.39375001192092896\n",
      "Epoch: 73/100... Step: 1160... Loss: 0.5420... Val Loss: 2.7856 acc:0.3828125\n",
      "Epoch: 74/100... Step: 1170... Loss: 0.5230... Val Loss: 2.7467 acc:0.3921875059604645\n",
      "Epoch: 74/100... Step: 1180... Loss: 0.5330... Val Loss: 2.7727 acc:0.3882812559604645\n",
      "Epoch: 75/100... Step: 1190... Loss: 0.5299... Val Loss: 2.7618 acc:0.38671875\n",
      "Epoch: 75/100... Step: 1200... Loss: 0.5595... Val Loss: 2.8064 acc:0.39140623807907104\n",
      "Epoch: 76/100... Step: 1210... Loss: 0.4882... Val Loss: 2.7682 acc:0.39375001192092896\n",
      "Epoch: 77/100... Step: 1220... Loss: 0.4820... Val Loss: 2.8121 acc:0.3929687440395355\n",
      "Epoch: 77/100... Step: 1230... Loss: 0.4707... Val Loss: 2.7773 acc:0.3921875059604645\n",
      "Epoch: 78/100... Step: 1240... Loss: 0.4370... Val Loss: 2.8494 acc:0.39453125\n",
      "Epoch: 79/100... Step: 1250... Loss: 0.4389... Val Loss: 2.8315 acc:0.38984376192092896\n",
      "Epoch: 79/100... Step: 1260... Loss: 0.4633... Val Loss: 2.8875 acc:0.3851562440395355\n",
      "Epoch: 80/100... Step: 1270... Loss: 0.4212... Val Loss: 2.8771 acc:0.38749998807907104\n",
      "Epoch: 80/100... Step: 1280... Loss: 0.4440... Val Loss: 2.8773 acc:0.3843750059604645\n",
      "Epoch: 81/100... Step: 1290... Loss: 0.4180... Val Loss: 2.9026 acc:0.39531248807907104\n",
      "Epoch: 82/100... Step: 1300... Loss: 0.3725... Val Loss: 2.9262 acc:0.3929687440395355\n",
      "Epoch: 82/100... Step: 1310... Loss: 0.3781... Val Loss: 2.8984 acc:0.3921875059604645\n",
      "Epoch: 83/100... Step: 1320... Loss: 0.3592... Val Loss: 2.9528 acc:0.39140623807907104\n",
      "Epoch: 84/100... Step: 1330... Loss: 0.3594... Val Loss: 2.9327 acc:0.38671875\n",
      "Epoch: 84/100... Step: 1340... Loss: 0.3853... Val Loss: 3.0375 acc:0.3890624940395355\n",
      "Epoch: 85/100... Step: 1350... Loss: 0.3674... Val Loss: 2.9871 acc:0.39453125\n",
      "Epoch: 85/100... Step: 1360... Loss: 0.3906... Val Loss: 2.9879 acc:0.38749998807907104\n",
      "Epoch: 86/100... Step: 1370... Loss: 0.3436... Val Loss: 3.0119 acc:0.38671875\n",
      "Epoch: 87/100... Step: 1380... Loss: 0.3458... Val Loss: 3.0065 acc:0.3851562440395355\n",
      "Epoch: 87/100... Step: 1390... Loss: 0.3279... Val Loss: 3.0640 acc:0.38671875\n",
      "Epoch: 88/100... Step: 1400... Loss: 0.3197... Val Loss: 3.0124 acc:0.3828125\n",
      "Epoch: 89/100... Step: 1410... Loss: 0.3359... Val Loss: 3.0731 acc:0.38203126192092896\n",
      "Epoch: 89/100... Step: 1420... Loss: 0.3161... Val Loss: 3.0437 acc:0.3921875059604645\n",
      "Epoch: 90/100... Step: 1430... Loss: 0.3170... Val Loss: 3.0533 acc:0.3828125\n",
      "Epoch: 90/100... Step: 1440... Loss: 0.3449... Val Loss: 3.0601 acc:0.3890624940395355\n",
      "Epoch: 91/100... Step: 1450... Loss: 0.3304... Val Loss: 3.1166 acc:0.38593751192092896\n",
      "Epoch: 92/100... Step: 1460... Loss: 0.3106... Val Loss: 3.0969 acc:0.38359373807907104\n",
      "Epoch: 92/100... Step: 1470... Loss: 0.3100... Val Loss: 3.1077 acc:0.3890624940395355\n",
      "Epoch: 93/100... Step: 1480... Loss: 0.2876... Val Loss: 3.1037 acc:0.38203126192092896\n",
      "Epoch: 94/100... Step: 1490... Loss: 0.2807... Val Loss: 3.1310 acc:0.3812499940395355\n",
      "Epoch: 94/100... Step: 1500... Loss: 0.2883... Val Loss: 3.1318 acc:0.3843750059604645\n",
      "Epoch: 95/100... Step: 1510... Loss: 0.2886... Val Loss: 3.1162 acc:0.37890625\n",
      "Epoch: 95/100... Step: 1520... Loss: 0.2936... Val Loss: 3.1439 acc:0.38984376192092896\n",
      "Epoch: 96/100... Step: 1530... Loss: 0.2838... Val Loss: 3.1749 acc:0.38984376192092896\n",
      "Epoch: 97/100... Step: 1540... Loss: 0.2817... Val Loss: 3.1456 acc:0.3882812559604645\n",
      "Epoch: 97/100... Step: 1550... Loss: 0.2807... Val Loss: 3.1993 acc:0.3882812559604645\n",
      "Epoch: 98/100... Step: 1560... Loss: 0.2319... Val Loss: 3.1784 acc:0.38203126192092896\n",
      "Epoch: 99/100... Step: 1570... Loss: 0.2740... Val Loss: 3.2119 acc:0.38984376192092896\n",
      "Epoch: 99/100... Step: 1580... Loss: 0.2373... Val Loss: 3.2180 acc:0.3828125\n",
      "Epoch: 100/100... Step: 1590... Loss: 0.2456... Val Loss: 3.2155 acc:0.3812499940395355\n",
      "Epoch: 100/100... Step: 1600... Loss: 0.2682... Val Loss: 3.2445 acc:0.3804687559604645\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_length = 20\n",
    "n_epochs = 100 # start small if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08e46033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:04:30.411379Z",
     "start_time": "2021-10-15T13:04:30.399005Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0b33aba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:04:33.478125Z",
     "start_time": "2021-10-15T13:04:33.469821Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abfb449c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:05:28.669732Z",
     "start_time": "2021-10-15T13:05:28.657634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applit\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1, prime='appl', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7b63f208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T07:32:49.564698Z",
     "start_time": "2021-06-02T07:32:49.523998Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(net, 'output/model/twolayLSTM.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eaa1e57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T10:03:04.771402Z",
     "start_time": "2021-06-04T10:03:02.630206Z"
    }
   },
   "outputs": [],
   "source": [
    "model=torch.load('output/model/twolayLSTM.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3e81411e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T07:34:37.929445Z",
     "start_time": "2021-06-02T07:34:37.915681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apples\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, 1, prime='appl', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8faf2a60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T10:04:08.998500Z",
     "start_time": "2021-06-04T10:04:08.952738Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'twolayLSTM_params.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70445a89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:11:06.782358Z",
     "start_time": "2021-10-15T13:11:06.759514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right \n",
      "right w\n",
      "right wo\n",
      "right won\n",
      "right wond\n",
      "right wonde\n",
      "right wonder\n",
      "right wonderf\n",
      "right wonderfu\n",
      "right wonderful\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'erful'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-15208a19aa9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minital_content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mpossible_chars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mpossible_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'erful'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "  # \"https://github.com/pradeepradyumna/SampleData/blob/master/sampledata.txt\")\n",
    "\n",
    "data = \"apple banana orange black yellow good bad right wrong blue green white think thought wonderful\"\n",
    "\n",
    "\n",
    "# Markov Chains Algorithm\n",
    "\n",
    "def generatetable(data, k):\n",
    "    T = {}\n",
    "    for i in range(len(data)-k):\n",
    "        x = data[i:i+k]\n",
    "        y = data[i+k]\n",
    "\n",
    "        if T.get(x) is None:\n",
    "            T[x] = {}\n",
    "            T[x][y] = 1\n",
    "        else:\n",
    "            if T[x].get(y) is None:\n",
    "                T[x][y] = 1\n",
    "            else:\n",
    "                T[x][y] += 1\n",
    "    return T\n",
    "\n",
    "\n",
    "k = 5\n",
    "inital_content = \"right\"\n",
    "\n",
    "T = generatetable(data.lower(), k)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    inp = inital_content[-k:]\n",
    "\n",
    "    possible_chars = list(T[inp].keys())\n",
    "    possible_values = list(T[inp].values())\n",
    "\n",
    "    sum_ = sum(T[inp].values())\n",
    "\n",
    "    probabs = np.array(possible_values)/sum_\n",
    "\n",
    "    next_char = np.random.choice(possible_chars, p=probabs)\n",
    "\n",
    "    inital_content += next_char\n",
    "\n",
    "    print(inital_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df50e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dataglove] *",
   "language": "python",
   "name": "conda-env-dataglove-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
